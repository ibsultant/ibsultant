---
layout: single
title:  "Essence of Linear Algebra_#01"
categories: 
 - Linear Algebra
classes: wide
use_math: true

---

## Essence of Linear Algebra_1


<b>선형대수</b>는 다차원 공간을 이해하는 대에 있어 매우 도움이 되는 학문입니다. 이는 여러 차원이 데이터가 쓰이는 인공지능 분야에서 더욱 빛을 발휘하는데요, PCA의 차원 축소같이 고유값/고유벡터의 아이디어에서 시작한 decomposition 기법으로 전처리를 수행할 수 있습니다. 이미지와 같은 고차원 데이터도 내부적으로 결국 2차원 matrix multiplication을 통해 계산이 이루어지기 때문에 선형 변환의 아이디어가 활용됩니다. 그 외 rank-1 outer product의 아이디어가 decoder 연산에 활용되기도 하고 change of basis의 아이디어를 통해 딥러닝 아키텍처 내에서 layer가 이미지의 어느 부분을 이해하고 있는지 통찰력도 얻을 수 있습니다.

저 역시 위와 같은 데이터 분석의 각 지점마다 선형대수의 중요성을 여실히 느껴 다시 한 번 review를 하게 되었습니다. 다만 전공 서적을 전부 정리하는 것은 시간적 제약이 따르는지라 고민이 있었는데요, 대학원 수업 당시 교수님께서 아주 좋은 강의라고 소개를 하셨던 강의가 생각나 게시글로 작성해두면 좋겠다는 생각이 들어 이렇게 정리해봅니다. 

강의의 제목은 Youtube 3blue1brown 채널의 <b>_Linear Algebra_</b> 입니다. 기존 선형대수와 비교하여 본 강의의 특장점은 geometric 관점에서 선형대수를 새롭게 받아들일 수 있도록 다양한 시각적 자료를 제공하고 직관적인 설명을 해준다는 점입니다. 데이터 분석에 있어 우리가 다루는 대부분의 manifold는 고차원이며 3차원 존재인 필자 입장에서 이를 이해하기 위해 선형대수의 기하하적 직관을 배울 수 있다면 분석 결과를 해석하는 부분에서도 큰 도움이 될 것이라 믿습니다. 

Essence of Linear Algebra 강의에선 선형대수의 모든 부분을 다루진 않지만 해당 강의를 우선적으로 정리하고, 예전에 네이버 본사에서 직접 들은 강의를 토대로 일부 내용을 추가/보완해볼까 합니다. 목차는 다음과 같습니다.

<span style='color:#808080'>_모든 첨부 영상 및 이미지는 3Blue1Brown에 저작권이 있습니다_</span> 


#### _Contents_

---
01. Vectors

02. Linear combinations, span, and basis vectors

03. Linear transformations and matrices

04. Matrix multiplication as composition

05. Three-dimensional linear transformations

06. The determinant

07. Inverse matrices, column space and null space

08. Nonsquare matrices as transformations between dimensions

09. Dot porducts and duality

10. Cross products

11. Cross products in the light of linear transformations

12. Cramer's rule, explained geometrically

13. Change of basis

14. Eigenvectors and eigenvalues

15. A quick trick for computing eigenvalues

16. Abstract vector spaces

---

### 1. Vectors

벡터를 크게 세 학계의 관점에서 보면 다음과 같습니다. 

* 물리학자 : Arrows pointing in space. 만약 방향성과 길이가 같다면 어느 곳이던 벡터는 같습니다.
* 컴퓨터공학자 : Ordered list of numbers
* 수학자 : 합 / 곱 등 특정 연산 규칙에 부합된다면 어느 것이던 벡터가 될 수 있습니다.

해당 강의에선 원점(origin)에 있는 화살표(arrow)를 벡터로 제안합니다. 이 경우 vector의 숫자 쌍은 벡터의 화살표 끝이 도달하는 지점을 의미합니다. 모든 숫자 쌍은 벡터 하나이고, 벡터도 하나의 숫자 쌍에 대응됩니다. 

선형대수는 벡터의 합 연산 / 곱 연산이 중심이 됩니다. 각 연산은 다음과 같습니다.

<b>_1.1. 합 연산_</b>

두 벡터의 합 연산이란 벡터의 꼬리를 다른 벡터의 화살표에 잇고 나머지 꼬리와 화살표를 이어줄 때 생기는 새로운 벡터를 의미합니다. 저자는 벡터를 '방향성 / 움직임'이란 개념으로 이해하면 편하다고 코멘트하였습니다. 해당 연산의 경우, 분리된 좌표별로 이동을 하여 표현할 수 있습니다. 

<b>_1.2. 곱 연산_</b>

Scaling 입장에서 이해하면 편합니다. 직관적으로 '벡터가 얼마나 늘어났는지/줄어든 수준'으로 해석할 수 있으며 음의 방향인 경우는 위치를 뒤집고 양수배 해주면 됩니다. numerical 입장에선 각 좌표 요소에 scaling을 해준다고 볼 수 있습니다. 

위의 두 연산을 중심으로 선형대수가 진행됩니다. 벡터에 대한 위의 각 학계의 관점은 상호 쉽게 전환될 수 있습니다. 물리학자에게 ordered list of numbers 관점으로 벡터를 생각하면 수치적으로 좋게 표현할 수 있으며 컴퓨터공학자는 arrow pointing 입장에서 벡터를 생각하면 좋은 기하학적 해석을 가져다 주는 식입니다. 이처럼 서로 전환될 수 있는 관점들이 선형대수의 특징입니다. 저와 같은 데이터 분석가에겐 다양한 숫자 리스트를 가상의 공간에서 개념화할 수 있도록 하며 data pattern을 명확하게 해주고 연산의 정체에 대한 전지적 관점을 부여해주는데 도움을 주는 학문입니다.

### 2. Linear combinations, span, and basis vectors

Vector를 보는 새로운 관점으로, 각 좌표계를 unit vector에 해당하는 $\hat{i}, \hat{j}$의 scalar 합으로 생각해봅니다.  

예를들어 $\begin{bmatrix}3 \\ -2\end{bmatrix}=3\hat{i}+2\hat{j}$ 와 같이 특정 벡터에 대해 단순 스케일링 된 두 벡터의 합이라고 해석하는 것은 상당히 중요합니다.

이때의 $\hat{i},\ \hat{j}$를 <b>_basis vectors_</b> of the x-y coordinate system이라 합니다. 즉, 좌표값을 scalar로 봤을 때, basis는 scalar가 scaling 하는 대상이 됩니다. 이렇게 basis vector로 좌표계를 framing하면, 

* basis의 수가 곧 number of dimension이 됩니다
* 차원 내 모든 vector는 basis의 scalar scaling으로 표현할 수 있습니다 (두 scailed basis의 합)

이는 우리가 수치로 vector를 표현할 때, 암묵적으로 특정 basis를 선택하여 표기한 것임을 의미합니다.

<b>_2.1. Linear Combination_</b>

 특정 두 벡터에 대해 각 벡터를 scaling 후 더하는 것을 두 벡터의 <b>_linear combination_</b>이라 표현합니다. linear의 어원은 해당 두 벡터 사이에 한 벡터는 고정하고 나머지 벡터의 scalar 값을 바꾸면 해당 벡터의 끝은 하나의 직선을 이루는 성질에서 유추할 수 있습니다. 영상을 통해 좀 더 직관적으로 이해할 수 있습니다.


<center><video src="https://user-images.githubusercontent.com/33462847/214817036-3ae914e6-4bd3-485c-abc6-93e6942050e4.mp4" controls="controls" height="70%" width="70%"></video></center>



<b>_2.2. Span_</b>

linear combination 상태인 두 이차원 벡터의 scalar value를 바꾸면 나오는 세 가지 상황이 있습니다.

        1. 2차원 상의 모든 공간에 도달
        2. 동일 선 상에 위치
        3. 원점
        
이러한 두 벡터의 조합으로 나타낼 수 있는 모든 결과 벡터들의 집합을 <b>_Span_</b>이라 합니다.

<b>_2.3. Linearly Independent_</b>

이번엔 벡터를 물리학자의 관점인 an arrow pointing in space로 생각해보겠습니다. 벡터의 꼬리는 원점에 위치합니다. 이 경우 선과 면은 무수한 화살표의 set으로 생각할 수 있습니다. 이제 3차원 상에서 특정 두 벡터간 linear combination의 span을 생각해보면, 원점을 지나는 평면이 됩니다. 그럼 3차원 상에서 세 벡터의 linear combination의 span은 어떻게 될지 생각해봅시다. 세 가지 상황을 가정할 수 있습니다. 

        1. 이전 두 벡터의 span 안에 세 번째 벡터가 포함되어 있는 경우 : span은 변함없이 동일하게 됩니다
        2. 이전 두 벡터의 span 밖에 세 번째 벡터가 위치하는 경우
                2.1. 이전 두 벡터의 span이 plane 인 경우 : 3차원 공간 전체가 span이 됩니다
                2.2. 이전 두 벡터의 span이 line 인 경우 : 2차원 평면이 span이 됩니다

여기서 각 벡터 간의 linear combination 관계에 따라 종류를 구분할 수 있는데,

각 벡터가 span의 차원을 추가하는 경우(2.1. 케이스) <b>_linearly independent_</b>하다고 말합니다. 또한 벡터 중 하나가 다른 두 벡터의 linear combination으로 표현가능하면 / 특정 벡터를 제외해도 span이 동일하면(나머지 케이스) <b>_linearly dependent_</b>하다고 말합니다. 

<b>_이처럼 basis는 linearly independent한 vector의 집합으로, full space를 span합니다._</b>

### 3. Linear transformations and matrices

<b>_3.1. Linear Transformation_</b>


<center><video src="https://user-images.githubusercontent.com/33462847/214817440-36ab4193-86a1-4488-a3e1-ec709425306b.mp4" controls="controls" height="70%" width="70%"></video></center>


여기서 <b>transformation</b>은 function과 같은 말로 function 대신 사용함으로써 'movement'의 어감을 내포하게 됩니다. 영상에서 보다시피 input vector를 'moving'시켜 output vector로 만듭니다. 공간상의 모든 점(vector)를 다른 점으로 이동시키는 것으로, 그 정도가 linear 하기에 <b>linear</b> transformation 이라 부릅니다. 

Geometrically, linear 하다는 말의 의미는 변환 후에도 line 이어야하고 원점이 변하지 않음을 의미합니다. (Grid lines remain parallel and evenly spaced)

Numerically, basis 의 변환을 생각하고 나머지 벡터는 해당 basis 로 표현하면 됩니다. 



<center><video src="https://user-images.githubusercontent.com/33462847/214822106-398edc7d-9642-4f36-9dbe-aa82df68e139.mp4" controls="controls" height="70%" width="70%"></video></center>



Basis는 linear transformation 상에서도 linear combination을 유지합니다. 한 예를 들어보겠습니다. 

$$
\begin{align}
\vec{V}&=\begin{bmatrix}-1 \\ 2\end{bmatrix}=(-1)i+(2)j
\\ \nonumber
transformed\ \vec{V} &= (-1)(transformed\ i)+(2)(transformed\ j) 
\nonumber
\end{align}
$$

이와 같은 형태로 표현 가능합니다. 즉 변환 전 좌표계 입장에서 표현했을 때 

$transformed\ i=\begin{bmatrix}1 \\ -2\end{bmatrix},\quad transformed\ j=\begin{bmatrix}3 \\ 0\end{bmatrix}$

이면 단순히 대입하면 됩니다. 2-dimensional 상에서 해당 basis의 변환 좌표를 2X2로 다음과 같이 표현할 수 있습니다. 

$transformation\ matrix\ L = \begin{bmatrix}1&3 \\ -2&0\end{bmatrix}$

위의 내용을 기반으로 벡터 $\begin{bmatrix}5 \\ 3\end{bmatrix}$ 가 변환 후 어디로 이동할 지 알고싶다면 $5\begin{bmatrix}1 \\ -2\end{bmatrix}+3\begin{bmatrix}3 \\ 0\end{bmatrix}$ 에 대입하면 됩니다. 이를 정리하여 각 basis가 변환되는 것을 도식하면 다음과 같습니다. 

$$
\begin{bmatrix}a&b \\ c&d\end{bmatrix}=\begin{bmatrix}a \\ c\end{bmatrix}i+\begin{bmatrix}b \\ d\end{bmatrix}j
$$

이제 일반 vector $\begin{bmatrix}x \\ y\end{bmatrix}$ 가 transformation 되는 과정을 표현하면 

$$
x\begin{bmatrix}a \\ c\end{bmatrix}+y\begin{bmatrix}b \\ d\end{bmatrix}=\begin{bmatrix}ax+by \\ cx+dy\end{bmatrix}=\begin{bmatrix}a&b \\ c&d\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix}
$$

와 같이 정리할 수 있으며 이를 <b>_Matrix-Vector Multiplication_</b> 라고 표현합니다. 추가로 $\begin{bmatrix}2 & -2
\\
1 & -1
\end{bmatrix}$ 와 같이 두 벡터가 linearly dependent 하다면 해당 변환은 공간을 squish 시켜 모든 벡터를 해당 line에 놓여있게 합니다. 


<center><video src="https://user-images.githubusercontent.com/33462847/214822242-b96385f7-f33b-4fae-872d-b5451b8d7131.mp4" controls="controls" height="70%" width="70%"></video></center>


핵심은 matrix를 볼 때마다 <b>_"공간의 어떤 변형에 해당한다"_</b> 고 인식하는 것입니다.

> <b>_To be continue..._</b>
