---
layout: single
title:  "Deep Learning from Scratch_#07_(1/2)"
categories: 
 - Deep Learning_Basic
classes: wide
use_math: true

---

### &lt;밑바닥부터 시작하는 딥러닝&gt; 코드 및 내용을 정리해보았습니다.

 * Deep Learning from Scratch_07_(1/2) 

---

## 7. Convoluntional Neural Network_(1/2) 

#### _Contents_

---
1. Structure
    
2. Convolutaional Layer
    
    2.1. 완전연결 계층의 문제점<br>
    2.2. 합성곱 연산<br>
    2.3. Padding<br>
    2.4. Stride<br>
    2.5. 3차원 데이터의 합성곱 연산<br>
    2.6. 블록으로 생각하기<br>
    2.7. 배치 처리<br>

3. Pooling Layer

    3.1. 풀링 계층의 특징

4. Implement Convolutional / Pooling Layer

    4.1. 4차원 배열<br>
    4.2. im2row로 데이터 전개하기<br>
    4.3. 합성곱 계층 구현하기<br>
    4.4. 풀링 계층 구현하기<br>
    
5. Implement CNN

6. Visualize CNN

    6.1. 1번째 층의 가중치 시각화하기<br>
    6.2. 층 깊이에 따른 추출 정보 변화

7. Well-Known CNN

    7.1. LeNet<br>
    7.2. AlexNet

8. Overview


---


```python
from matplotlib.image import imread
from sympy.plotting import plot3d
from collections import OrderedDict
from mnist import load_mnist
from PIL import Image
from tqdm import tqdm


import matplotlib.pyplot as plt
import sympy as sy
import numpy as np
import os, sys
import pickle
```

이번 장에선 이미지 인식 분야에서 쓰이는 Convolutional Neural Network(CNN)에 대해 다루도록 하겠습니다.

Layer가 필연적으로 깊어지기 때문에 본격적인 딥러닝의 시작이라고 할 수 있습니다.

### 1. Structure

먼저 기존 신경망의 구조는 다음과 같습니다.


```python
img_01 = Image.open('./dataset/images/fig 7-1.png')
img_01
```




    
![png](/assets/images/DLScratch_07_01/img_01.png)
    



현재까지 봤던 신경망은 모든 뉴런끼리 연결되어 있었습니다. 이를 <b>_Fully Connected Layer_</b>라고 합니다. 기존의 fully connected layer는 <b>_affine layer_</b>로 표현하였습니다. 이후 activation function과 연결되고, 해당 구조가 반복되다가 최종 layer에선 softmax로 연결되는 구조였습니다. 

CNN 구조는 다음과 같습니다.


```python
img_02 = Image.open('./dataset/images/fig 7-2.png')
img_02
```




    
![png](/assets/images/DLScratch_07_01/img_02.png)
    



새롭게 <b>_convolution(conv) layer_</b>와 <b>_pooling layer_</b>가 추가됩니다. 기존 affine layer의 역할을 conv layer가 대신한다고 보면 됩니다. 몇 가지 또 다른 점은, pooling 계층이 위와 같이 생략되기도 하고 출력 계층에선 기존의 계층 조합을 쓸 수 있다는 점입니다. 마지막 계층에선 기존의 계층 조합을 동일하게 사용합니다. 

### 2. Convolution Layer

CNN에선 <b>padding, stride</b> 등의 새로운 용어가 등장하고 3차원 데이터가 신경망 내에서 전달되는 점이 새롭습니다. 이번 절에선 해당 내용들에 대해 차근차근 살펴보겠습니다.

<b>_2.1. 완전연결 계층의 문제점_</b>

 완전연결계층의 가장 큰 문제점은 <b>_데이터의 형상이 무시된다_</b>는 점입니다. 기존의 MNIST dataset의 경우 (1, 28, 28) 형상의 데이터를 (784)의 데이터로 변환하여 Affine layer의 입력으로 사용했습니다. 이는 많은 정보의 손실을 야기합니다. 공간적 정보는 책에서 언급하듯 인접한 픽셀 간의 상관관계, RGB channel 간의 상관관계, 3차원이어야만 의미를 갖는 데이터 등 다양한 정보가 있습니다. 

 <b>CNN은 이러한 데이터의 형상을 유지한 채로 신경망 내에서 데이터가 전달</b>되도록 할 수 있습니다. 이는 결국 이미지와 같이 형상에 다양한 정보가 담겨있는 데이터에 대한 문제를 푸는데 좋은 성능을 보입니다. 

 CNN에선 conv layer의 입출력 데이터를 input/output feature map이라 부릅니다. 앞으로 input/output data 대신 feature map이란 용어를 사용하겠습니다. 

 <b>_2.2. 합성곱 연산_</b>

 conv layer에선 convolution operation을 수행합니다. 


```python
img_03 = Image.open('./dataset/images/fig 7-4.png')
img_03
```




    
![png](/assets/images/DLScratch_07_01/img_03.png)
    



각각 <b>input feature map, filter(or kernel), output feature map</b>에 해당하는 위의 그림은 filter가 이미지를 좌측/상단에서 우측/하단 순으로 moving window가 진행되며 element-wise product(fused multiply-add, FMA)가 일어나는 과정을 묘사한 것입니다. 총 네 번의 연산이 일어났으며 과정은 다음과 같습니다.

$$
\begin{align}\begin{pmatrix}1&2&3\\0&1&2\\3&0&1\end{pmatrix}\circledast
\begin{pmatrix}2&0&1\\0&2&2\\1&0&2\end{pmatrix}&\rightarrow\begin{pmatrix}1*2&2*0&3*1\\0*0&1*2&2*2\\3*1&0*0&1*2\end{pmatrix}\nonumber
\\&\rightarrow\begin{pmatrix}2&0&3\\0&2&4\\3&0&2\end{pmatrix}\nonumber
\\&\rightarrow\ 2+0+3+0+2+4+3+0+2 = 15\nonumber
\end{align}
$$

Filter의 첫 moving window 연산만 도식화한 것입니다. 나머지도 이와 동일합니다. 

이처럼 기존 신경망의 weight 역할을 filter가 대신하고 bias의 역할은 filter operation 이후 다음과 같이 진행됩니다.


```python
img_04 = Image.open('./dataset/images/fig 7-5.png')
img_04
```




    
![png](/assets/images/DLScratch_07_01/img_04.png)
    



<b>_2.3. Padding_</b>

위의 연산에서 보듯 이미지 모서리에 있는 데이터는 filter의 moving window 상 겹치지 않게 되기에 filter operation에 덜 반영됩니다. 이를 방지하고자 input feature map 주변을 특정 값(0)으로 채워넣고 conv operation을 진행하는데 이를 <b>_Padding_</b>이라고 합니다.


```python
img_05 = Image.open('./dataset/images/fig 7-6.png')
img_05
```




    
![png](/assets/images/DLScratch_07_01/img_05.png)
    



패딩 값은 추가되는 주변 데이터 사이즈를 의미합니다. 한 겹, 두 겹의 개념으로 생각하면 편합니다. 여기서 input feature map / filter / padding size 모두 output feature map의 사이즈에 관여합니다. 후술할 Stride와 함께 그 계산식을 살펴보겠습니다.

<b>_2.4. Stride_</b>

conv operation 상 moving window 간격을 <b>_Stride_</b>라 합니다.


```python
img_06 = Image.open('./dataset/images/fig 7-7.png')
img_06
```




    
![png](/assets/images/DLScratch_07_01/img_06.png)
    



이제 input feature map / filter / padding / stride size 와 output feature map size 간의 관련성을 살펴보겠습니다.

* Input Feature map Size : ($\boldsymbol H$, $\boldsymbol W$)
* Filter Size : ($\boldsymbol {FH}$, $\boldsymbol {FW}$)
* Padding Size : $\boldsymbol P$ 
* Stride Size : $\boldsymbol S$
* Output Feature map Size : ($\boldsymbol {OH}$, $\boldsymbol {OW}$)

라고 할 때,

$$
\boldsymbol {OH} = \dfrac{\boldsymbol H+2\boldsymbol P-\boldsymbol {FH}}{\boldsymbol S}+1
\\
\\
\boldsymbol {OW} = \dfrac{\boldsymbol W+2\boldsymbol P-\boldsymbol {FW}}{\boldsymbol S}+1
$$

라는 공식으로 output feature map 크기를 계산할 수 있습니다. 식의 형태가 직관적이므로 외워서 계산하면 됩니다. 당연하지만, 계산 값은 자연수여야 합니다.

<b>_2.5. 3차원 데이터의 합성곱 연산_</b>

3차원 데이터의 conv operation 연산이 기존 2차원 데이터와 다른 점은 channel이 추가되어 각 channel에 대한 filter가 추가되고, 이를 종합해서 하나의 출력을 얻는다는 점입니다.


```python
img_07 = Image.open('./dataset/images/fig 7-9.png')
img_07
```




    
![png](/assets/images/DLScratch_07_01/img_07.png)
    



유의할 점은 feature map channel과 filter channel이 같아야하는 점입니다. 물론 각 filter의 크기도 동일해야 합니다. 

<b>2.6._블록으로 생각하기_</b>

<b>2.7._배치 처리_</b>

이제 이미지 뭉치와  filter 뭉치도 3차원 블록으로 고려하겠습니다. 차원을 늘려 일반화한 Conv operation의 흐름은 다음과 같습니다.


```python
img_08 = Image.open('./dataset/images/fig 7-13.png')
img_08
```




    
![png](/assets/images/DLScratch_07_01/img_08.png)
    



* N : batch size, C : channel, H : height, W : width 에 해당합니다.
* FN / FH / FW는  filter의 N/H/W를 의미합니다. OH / OW 는 output feature의 H / W 를 의미합니다.
* 3차원 filter 블록 하나와 input feature 간의 연산의 결과 값은 위와 같은 2차원 출력입니다. 
* 이에 filter 개수를 FN개로 늘리면 다음과 같이 FN개의 output feature map을 얻을 수 있습니다.
* 여기서 배치 처리를 하면 달라지는 것은 input / output feature에 batch size N 이 추가되는 것입니다.
* 전체 과정에는 역시 bias가 추가되므로 이를 반영한 그림은 위와 같게됩니다.
* 3차원 데이터이기 때문에 배치 처리를 하면 4차원 데이터가 신경망을 흐르게 됩니다.

### 3. Pooling Layer

<b>_Pooling_</b>은 가로, 세로 방향의 공간을 줄이는 연산입니다. 


```python
img_09 = Image.open('./dataset/images/fig 7-14.png')
img_09
```




    
![png](/assets/images/DLScratch_07_01/img_09.png)
    



위의 경우는 2 X 2 max pooling을 stride 2로 feature에 적용하는 것으로, 해당 범위 내 최댓값을 하나 뽑아 moving window 하며 출력합니다. Pooling은 (N X N) 윈도우에 stride N을 설정하는 것이 일반적입니다. 외에 avg pooling 도 있으며 범위 내 값의 평균을 출력합니다. 이미지 인식 분야에선 보통 max pooling을 이용합니다. 

<b>_3.1. 풀링 계층의 특징_</b>

* 학습해야 할 매개변수가 없다
* 채널 수가 변하지 않는다 : 채널마다 독립적으로 계산하기 때문에 2차원 filter와 달리 채널 수 변화가 없습니다.
* 입력의 변화에 영향을 적게 받는다 : 아래 그림과 같이, feature map이 달라도 pooling에서 보정이 되기 때문에 강건한 성능을 갖습니다.


```python
img_10 = Image.open('./dataset/images/fig 7-16.png')
img_10
```




    
![png](/assets/images/DLScratch_07_01/img_10.png)
    



### 4. Implement Convolutional / Pooling Layer

이제 배운것들을 구현해보도록 하겠습니다.

<b>_4.1. 4차원 배열_</b> : CNN은 4차원 배열이기 때문에 [N, C, H, W] 순으로 접근합니다.

<b>_4.2. im2row로 데이터 전개하기_</b>

Numpy를 이용해 고차원 배열의 곱 연산을 하는 것은 비효율 적입니다. 이에 image to row(;im2row) 함수를 이용해 이미지를 펼쳐서 차원을 축소하여 연산을 단순하게 합니다. 즉 4차원 input feature map과 4차원 filter image의 conv operation을 2차원 행렬곱 형태로 단순화하여 output feature map을 출력하는 것입니다. 아래와 같이 동일 N/C 상 feature map에서 filter가 적용되는 영역을 한 줄로 늘어놓고 filter도 한 줄로 변환하여 matrix dot product를 유도하고 이를 전 영역에서 수행합니다. 


```python
img_11 = Image.open('./dataset/images/fig 7-19.png')
img_11
```




    
![png](/assets/images/DLScratch_07_01/img_11.png)
    



앞서 보았듯, stride가 크지 않는 이상 filter window moving은 feature map을 겹치며 이동하기에 펼쳐 놓은 2차원 데이터가 커져 메모리 소모가 커지는 단점이 있습니다. 대신 컴퓨터는 행렬 곱을 빠르게 계산할 수 있는 구조기 떄문에 효율적으로 해당 연산을 수행할 수 있습니다. 추가로, 이 형태는 기존 Affine 계층과 구현이 유사하게 이루어집니다.

<b>_4.3. 합성곱 계층 구현하기_</b>

먼저 im2row 함수부터 구현하겠습니다. (교재에선 im2col로 명명하였는데, 이미지 값들이 row를 기준으로 반영되었다 생각하여 바꿔지었습니다)

직관적이지 않아 코드를 설명하자면, 

* 최종 output feature map의 인자 하나는 filter 연산 하나의 결과입니다. 이것과 배치 데이터 사이즈를 반영하여 펼쳐놓을 이미지의 row수를 정합니다. 
* 그러기 위한 output feature map의 out_h / out_w 를 계산합니다. 
* row는 결국 (channel_1, filter_123456789, channel_2, filter_987654321, ...) 등의 filter 채널 / 원소들이 반영된 col들과 matrix product가 이루어지게 됩니다. 이에 대응하는 이미지 정보가 들어가 있습니다.
* 해당 이미지 정보를 넣기 위한 반복문이 아래에 구현되어 reshape 후 출력됩니다.


```python
def im2row(input_data, filter_h, filter_w, stride=1, pad=0):
    N, C, H, W = input_data.shape
    out_h = (H + 2*pad - filter_h)//stride + 1  # 나눠지지 않는 케이스 고려
    out_w = (W + 2*pad - filter_w)//stride + 1  # 나눠지지 않는 케이스 고려

    # 4차원 array의 H, W 요소마다 왼/오른쪽 pad 삽입
    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')
    # 6차원 array 생성
    row = np.zeros((N, C, filter_h, filter_w, out_h, out_w)) 
    # x, y : 필터의 각 원소
    # y - y_max/x - x_max는 필터의 원소 (x, y)가 img를 stride 단위로 이동하며 가는 인덱스(발자취)
    for y in range(filter_h):
        y_max = y + stride*out_h
        for x in range(filter_w):
            x_max = x + stride*out_w
            # 각 filter 원소 y, x에 대해 img 상에서 연산 진행될 좌표 인덱스 상의 값을 row에 반환
            row[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]

    # H / W 는 filter 연산이 진행되는 위치상의 이미지 값을 의미합니다.
    # (N, C, H, W, OH, OW) -> (N, OH, OW, C, H, W)로 순서 변환
    # 2차원 데이터의 row size : N * out_H * out_W
    # C, H, W를 한 줄에 담아서 reshape
    row = row.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)
    return row
```

교재에선 존재만 언급하고 넘어갔지만, backward 과정을 생각해보면 output feature map과 filter 정보를 행렬 곱 형태로 input feature 이미지로 출력해주는 함수의 존재도 따로 필요해지게 됩니다. 과정은 위 구현의 역과정이며 이를 구현해보겠습니다. 과정은 유사하기 때문에 주석처리만 하였습니다.


```python
def row2im(row, input_shape, filter_h, filter_w, stride=1, pad=0):
    N, C, H, W = input_shape # shpape of input feature map
    out_h = (H + 2*pad - filter_h)//stride + 1
    out_w = (W + 2*pad - filter_w)//stride + 1
    # row는 dout과 filter의 행렬곱 형태입니다.
    # row.shape : (N, C, out_H, out_W)
    row = row.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)
    # transpose 후 row.shape : (N, C, FH, FW, OH, OW)

    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))
    # img : (N, C, OH, OW)
    for y in range(filter_h):
        y_max = y + stride*out_h
        for x in range(filter_w):
            x_max = x + stride*out_w
            img[:, :, y:y_max:stride, x:x_max:stride] += row[:, :, y, x, :, :]
            # 비어있는 이미지 정보인덱스에 conv 연산이 진행된 output feature map 값을 더해줍니다.
    return img[:, :, pad:H + pad, pad:W + pad] # 패딩 사이즈는 제외하고 출력
```

이젠 filter도 2차원으로 펼쳐서 matrix product를 진행하고, 이로 output feature map까지 출력하는 forward 연산을 구현해보겠습니다. 책에서 그냥 지나친 계산그래프 상의 backward 과정도 추가하였습니다. 


```python
class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad

        self.x = None
        self.row = None
        self.col_W = None

        self.db = None
        self.dW = None

    # filter가 가중치 W 역할을 한다
    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = int((H + 2*self.pad - FH)/self.stride + 1)
        out_w = int((W + 2*self.pad - FW)/self.stride + 1)

        # row.shape : (N*OH*OW, -1)
        # col_W.shape : (-1, FN) 
        # F_size = OH * OW
        row = im2row(x, FN, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T
        out = np.dot(row, col_W) + self.b # (N*OH*OW, FH*FW) 이로 인해 output reshape 필요
        # out.shape : (N, OH, OW, C) -> (N, C, OH, OW)
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)

        self.x = x
        self.row = row
        self.col_W = col_W

        return out

    def backward(self, dout):
        FN, C, FH, FW = self.W.shape
        
        # 1. reshape / transpose 역연산 =======================
        # 1-1. transpose : (N, H, W, C) -> (N, C, OH, OW) 
        # transpose는 (0, 1, 2, 3)에 해당하는 인자가 (3, 1, 2, 0) 등으로 이동하도록 하는 함수(헷갈릴 여지가 많다)
        # 1-2. reshape : (-, FN)
        dout = dout.tranpose(0, 2, 3, 1).reshape(-1, FN)
        #-----------------------------------------------------
        
        # 2. + 연산 ===========================================
        self.db = np.sum(dout, axis=0)
        #-----------------------------------------------------
        
        # 3. X 연산 ===========================================
        # 3.1. self.col_W(filter) 방향 backward
        self.dW = np.dot(self.row.T, dout)
        # 3.2. self.row(input feature map) 방향 backward 
        drow = np.dot(dout, self.col_W.T)
        #-----------------------------------------------------
        
        # 4. img2row / fil2col 연산 ===========================
        # 4.1. filter -> row 역연산
        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)
        # 4.2. img -> row 역연산 
        dx = row2im(drow, self.xshape, FH, FW, self.stride, self.pad)
        #-----------------------------------------------------
        return dx
```

Traspose로 순서가 변경된 최종 출력 값 out의 형태는 다음과 같습니다.


```python
img_12 = Image.open('./dataset/images/fig 7-20.png')
img_12
```




    
![png](/assets/images/DLScratch_07_01/img_12.png)
    



<b>_4.4. 풀링계층 구현하기_</b>

풀링계층 연산은 다음과 같이 진행됩니다. 먼저 각 input feature map / pooling window size / stride 에 따른 output feature map 사이즈는 다음의 공식을 따릅니다. 

* Input Feature map Size : ($\boldsymbol H$, $\boldsymbol W$)
* Pooling Window Size : ($\boldsymbol {PH}$, $\boldsymbol {PW}$)
* Stride Size : $\boldsymbol S$
* Output Feature map Size : ($\boldsymbol {OH}$, $\boldsymbol {OW}$)

라고 할 때,

$$
\boldsymbol {OH} = \dfrac{\boldsymbol H-\boldsymbol {PH}}{\boldsymbol S}+1
\\
\\
\boldsymbol {OW} = \dfrac{\boldsymbol W-\boldsymbol {PW}}{\boldsymbol S}+1
$$

라는 공식으로 output feature map 크기를 계산할 수 있습니다. filter 연산과 동일하게 하나의 값을 출력하기 때문에 식 또한 다를게 없습니다. (Pooling 연산은 padding 값을 0을 쓰기에 해당 부분은 제외하였습니다)



```python
img_13 = Image.open('./dataset/images/fig 7-22.png')
img_13
```




    
![png](/assets/images/DLScratch_07_01/img_13.png)
    



위의 과정을 기반으로 구현해보려 합니다. Pooling 연산도 filter 연산과 출력 값이 하나로 같기때문에 im2row 함수를 사용하여 구현합니다. 이때 기존 im2row의 결과값은 N * out_H * out_W 기준으로 C,H,W 값이 나열됩니다. 하지만 풀링계층은 channel이 보존된 상태로 output feature map이 출력되기 때문에 shape을 위의 <b>_전개_</b> 부분처럼 더 다듬어줘야 합니다. 이를 고려하여 forward 및 backward를 구현하면 다음과 같습니다.


```python
class Pooling:
    def __init__(self, pool_h, pool_w, stride=1, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
        
        self.x = None
        self.arg_max = None

    def forward(self, x):
        N, C, H, W = x.shape
        out_h = int((H - self.pool_h) / self.stride + 1)
        out_w = int((H - self.pool_w) / self.stride + 1)

        row = im2row(x, self.pool_h, self.pool_w, self.stride, self.pad)
        # row 값은 기존 (C, H, W) 값의 나열이 (PH, PW) 가 나열되는 형태로 reshape된다
        # 즉 pooling window 사이즈에 맞춰 Channel이 보존된 image 정보 형태로 나열된다
        row = row.reshape(-1, self.pool_h*self.pool_w)
        arg_max = np.argmax(row, axis=1) # 각 row의 max 값 위치 인덱스 저장
        out = np.max(row, axis=1) # 각 row에 대해 max pooling이 이루어진다
        # (N*OH*OW*C, 1)에 대해서 reshape이 이루어진다 (그냥 쓰면 안 될지 궁금하다)
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)

        self.x = x
        self.arg_max = arg_max
        
        return out

    def backward(self, dout):
        # 1. reshape / transpose 역연산 ======================
        # 1.1. transpose
        dout = dout.transpose(0, 2, 3, 1)
        # 1.2. reshape은 flatten되어 있으므로 shape 함수로 대체
        #---------------------------------------------------

        # 2. pooling 역연산 ==================================
        pool_size = self.pool_h * self.pool_w # pooling window 
        dmax = np.zeros((dout.size, pool_size)) # (N*OH*OW*C, PH*PW) 복원
        # dmax에 각 max pooling 된 위치 인덱스를 저장. 아닌 곳은 zero
        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()
        #---------------------------------------------------
        
        # 3. row2im 역연산 ===================================
        # dmax : (N, OH, OW, C, FH*FW)으로 reshape
        dmax = dmax.reshape(dout.shape + (pool_size,))
        # drow : dmax를 통해 (N*OH*OW,C*FH*FW) 형태로 변환, row2im 출력 shape에 해당한다.
        drow = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2],  -1)
        dx = row2im(drow, self.xshape, self.pool_h, self.pool_w, self.stride, self.pad)
        #---------------------------------------------------

        return dx
```

Pooling 연산의 구현은 conv 연산을 구현하는 것과 달리 계산그래프 상에서 곱 / 합 연산들이 들어가지 않기 때문에 상대적으로 간단하게 구현할 수 있었습니다.
글이 길어졌기에 다음 게시물에 내용을 이어가도록 하겠습니다.

> <b>_To be continue..._</b>


---
