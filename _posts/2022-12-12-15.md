## 5. Error Backpropagation

#### _Contents_

---
1. 계산 그래프
    
    1.1. 계산 그래프로 풀다<br>
    1.2. 국소적 계산<br>
    1.3. 왜 계산 그래프로 푸는가?
    
2. 연쇄 법칙
    
    2.1. 계산 그래프의 역전파<br>
    2.2. 연쇄법칙이란?<br>
    2.3. 연쇄법칙과 계산 그래프

3. 역전파

    3.1. 덧셈 노드의 역전파<br>
    3.2. 곱셈 노드의 역전파<br>
    3.3. 사과 쇼핑의 예

4. 단순한 계층 구현하기

    4.1. 곱셈 계층<br>
    4.2. 덧셈 계층
    
5. 활성화 함수 계층 구현하기

    5.1. ReLU 계층<br>
    5.2. Sigmoid 계층

6. Affine / Softmax 계층 구현하기

    6.1. Affine 계층<br>
    6.2. 배치용 Affine 계층<br>
    6.3. Softmax-with-Loss 계층

7. 오차역전파법 구현하기

    7.1. 신경망 학습의 전체그림 <br>
    7.2. 오차역전파법을 적용한 신경망 구현하기<br>
    7.3. 오차역전파법으로 구한 기울기 검증하기<br>
    7.4. 오차역전파법을 사용한 학습 구현하기

8. 정리


---


```python
from matplotlib.image import imread
from sympy.plotting import plot3d
from PIL import Image


import matplotlib.pyplot as plt
import sympy as sy
import pickle
import numpy as np
import sys
import os
```

해당 책에선 Error Backpropagation을 computational graph 방식으로 설명하였습니다. 저도 해당 관점으로 정리, 요약하였습니다.(CS231n based) 

추후 이와관련해 저는 수식을 중심으로 설명하는 게시물을 따로 업로드하겠습니다. 

### 1. 계산 그래프

<b>_1.1. 계산그래프로 풀다_</b>

<b>_1.2. 국소적 계산_</b>

<b>_1.3. 왜 계산그래프로 푸는가?_</b>


본 책에선 다음의 방식으로 그래프 자료구조를 노드와 엣지로 단순하게 표현하였습니다. 

아래의 방식은 순전파(forward propagation)로 기존 4장까지의 방식이나, 이번 장에선 역전파(backward propagation)에 초점을 맞춥니다.


```python
img_1 = Image.open('./dataset/images/fig 5-4.png')
img_1
```




    
![png](/assets/images/DLScratch_05_01/img_01.png)
    



계산 그래프의 도입이유로 저자는 

* 국소적 계산이 가능하다.(복잡한 수식을 잘게 쪼게어 표현)
* 중간 계산 결과를 노드에 보관할 수 있다.

를 들고 있습니다. 이 때 계산식이 미분 가능하다면, <b>역전파 과정을 통해 효율적인 미분 계산이 가능해진다</b>는 장점을 갖게됩니다.

각 노드별 미분 결과를 공유하고, 결과적으로 다수의 미분을 효율적으로 계산할 수 있게 되는 것입니다. 

만약 최종 가격과 각 노드간의 (여기선 사과) gradient를 역으로 계산했다면 다음과 같이 해석할 수 있습니다.


```python
img_2 = Image.open('./dataset/images/fig 5-5.png')
img_2
```




    
![png](/assets/images/DLScratch_05_01/img_02.png)
    



> 사과의 가격이 $n$만큼 오를 때마다, 최종가격은 $2.2n$만큼 상승한다

### 2. 연쇄 법칙

이번 장에선 같은 내용을 미분의 chain rule을 이용하여 설명합니다.

<b>_2.1. 계산 그래프의 역전파_</b> (PASS)

<b>_2.2. 연쇄법칙이란?_</b>

> _합성함수의 미분은 합성함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다._ 

해당 성질을 이용하는 것이 연쇄법칙, Chain rule이며 미분 공식을 이용해 해석적으로 구한 결과는 다음과 같습니다.

$$
\begin{align}
\require{cancel}
let\qquad z=t^2&,\quad t=x+y\quad(\therefore z=(x+y)^2) \nonumber
\\ \nonumber
\\ \nonumber
\dfrac{\partial z}{\partial x}&=\dfrac{\partial z}{\partial t}\dfrac{\partial t}{\partial x}
\\ \nonumber
\\ \nonumber
\dfrac{\partial z}{\partial x}&=\dfrac{\partial z}{\cancel{\partial t}}\dfrac{\cancel{\partial t}}{\partial x}
\\ \nonumber
\\ \nonumber
\dfrac{\partial z}{\partial t}&=2t
\\ \nonumber
\\ \nonumber
\dfrac{\partial t}{\partial x}&=1
\\ \nonumber
\\ \nonumber
\dfrac{\partial z}{\partial x}&=\dfrac{\partial z}{\partial t}\dfrac{\partial t}{\partial x}=2t\cdot1=2(x+y)
\end{align}
$$

<b>_2.3. 연쇄법칙과 계산 그래프_</b>


```python
img_3 = Image.open('./dataset/images/fig 5-7.png')
img_4 = Image.open('./dataset/images/fig 5-8.png')

plt.figure(figsize=(20,10))
plt.subplot(1,2,1)
plt.imshow(img_3)
plt.subplot(1,2,2)
plt.imshow(img_4)
```




    <matplotlib.image.AxesImage at 0x11fabbc70>




    
![png](/assets/images/DLScratch_05_01/img_03.png)
    


위의 수식을 계산 그래프로 나타내면 다음과 같습니다. 가장 좌측의 $x$값의 변화량에 대해선 $z$값이 $2(x+y)$만큼의 변화율로 대응됨을 확인할 수 있습니다.

### 3. Backpropagation

<b>_3.1. 덧셈 노드의 역전파_</b>

$z=x+y$에 대한 역전파를 편미분 입장에서, 그리고 값의 전달 입장에서 보면 다음과 같습니다. 


```python
img_5 = Image.open('./dataset/images/fig 5-9.png')
img_6 = Image.open('./dataset/images/fig 5-11.png')

plt.figure(figsize=(20, 10))
plt.subplot(2, 1, 1)
plt.imshow(img_5)
plt.subplot(2, 1, 2)
plt.imshow(img_6)
```




    <matplotlib.image.AxesImage at 0x11fc6eec0>




    
![png](/assets/images/DLScratch_05_01/img_04.png)
    


앞 노드의 미분 값은 그대로 backpropagation되고, 외엔 1배수만큼의 차이가 나는 것 때문에 상위 노드의 값이 하위 노드로 그대로 흘러내려간다고 볼 수 있습니다.

구현입장에서 보자면, 값이 그대로 흘러가기때문에 입력 신호를 딱히 저장해둘 이유가 없게됩니다. (계산의 용이성))

<b>_3.2. 곱셈 노드의 역전파_</b>

$z=xy$에 대한 역전파를 편미분 입장에서, 그리고 값의 전달 입장에서 보면 다음과 같습니다. 


```python
img_7 = Image.open('./dataset/images/fig 5-12.png')
img_8 = Image.open('./dataset/images/fig 5-13.png')

plt.figure(figsize=(20, 10))
plt.subplot(2, 1, 1)
plt.imshow(img_7)
plt.subplot(2, 1, 2)
plt.imshow(img_8)
```




    <matplotlib.image.AxesImage at 0x11fcf7af0>




    
![png](/assets/images/DLScratch_05_01/img_05.png)
    


앞 노드의 미분 값이 하위 노드에 그대로 흘러가는 것은 동일하고 값의 크기만큼 서로 반대로 상위 노드의 값이 아래로 흘러들어간다고 할 수 있습니다.

여기서 알 수 있는 점은 

> <b>"곱셈의 backpropagation은 순방향 입력신호의 값이 필요하다"</b>


라는 점이고, 이는 구현시 forward 값의 저장 필요성을 의미하게 됩니다.

<b>_3.3. 사과 쇼핑의 예_</b>

위의 내용을 기반으로 forward 간의 (2개 노드 곱 한정) 곱과 합 관계를 보고 backpropagation을 진행할 수 있습니다.

곱의 관계는 반대로 연산하여 흘려보내고, 합의 관계는 항등관계이므로 흘려보내면 됩니다. 

아래의 경우 소비세의 변동이 최종 가격에 가장 큰 영향을 미친다고 해석할 수 있습니다. _(사과 가격의 100배 가량)_


```python
img_9 = Image.open('./dataset/images/fig 5-16.png')
img_9
```




    
![png](/assets/images/DLScratch_05_01/img_06.png)
    



### 4. 단순한 계층 구현하기

이번 장에선 위의 사과 쇼핑의 error backpropagation 과정을 구현해보겠습니다.

<b>_4.1. 곱셈 계층_</b>

이제 계산 그래프 상의 곱셈 노드에 대해 forward / backward를 각각 구현하겠습니다.


```python
class MulLayer:
    def __init__(self):
        self.x = None     
        self.y = None

    def forward(self, x, y):
        self.x = x
        self.y = y
        out = x * y

        return out
    
    def backward(self, dout):   # dout : differential out, 상류에서 내려온 미분 값을 의미.
        dx = dout * self.y
        dy = dout * self.x
        
        return dx, dy
    
```


```python
apple = 100
apple_num = 2
tax = 1.1

# layer (노드 기준)
mul_apple_layer = MulLayer()
mul_tax_layer = MulLayer()

# forward (순방향 엣지 기준)
apple_price = mul_apple_layer.forward(apple, apple_num) # 첫 번째 노드
price = mul_tax_layer.forward(apple_price, tax)         # 두 번째 노드

# backward (역방향 엣지 기준)
dprice = 1 
dapple_price, dtax = mul_tax_layer.backward(dprice)         # 두 번째 노드의 각 엣지들
dapple, dapple_num = mul_apple_layer.backward(dapple_price) # 첫 번재 노드의 각 엣지들

print('price : ', price)
print('dapple : ', dapple)
print('dapple_num : ', dapple_num)
print('dtax : ', dtax)
```

    price :  220.00000000000003
    dapple :  2.2
    dapple_num :  110.00000000000001
    dtax :  200


<b>_4.2. 덧셈 계층_</b>

이번엔 덧셈 계층도 정의하여 아래있는 계산 그래프의 forward, backward를 구현해보겠습니다.


```python
img_10 = Image.open('./dataset/images/fig 5-17.png')
img_10
```




    
![png](/assets/images/DLScratch_05_01/img_07.png)
    




```python
class AddLayer:
    def __init__(self):
        pass        # forward / backward 연산 모두 내부 변수 x, y가 필요하지 않다.
     
    def forward(self, x, y):
        out = x + y

        return out
    
    def backward(self, dout):   # dout : differential out, 상류에서 내려온 미분 값을 의미.
        dx = dout * 1
        dy = dout * 1
        
        return dx, dy
```

책의 표기법은 직관적이지 않아 노드, 엣지 이름을 기준으로 구성했습니다.


```python
edge_1_1 = 2
edge_1_2 = 100
edge_1_3 = 150
edge_1_4 = 3
edge_1_5 = 1.1

# layer (Nodes)
node_1_1 = MulLayer()
node_1_2 = MulLayer()
node_2 = AddLayer()
node_3 = MulLayer()

# forward (Forward edges)
edge_2_1 = node_1_1.forward(edge_1_1, edge_1_2)
edge_2_2 = node_1_2.forward(edge_1_3, edge_1_4)
edge_3 = node_2.forward(edge_2_1, edge_2_2)
edge_4 = node_3.forward(edge_3, edge_1_5)

# backward (Backward edges) # b_edge ; back_edge
b_edge_1 = 1 
b_edge_2_1, b_edge_2_2 = node_3.backward(b_edge_1)         
b_edge_3_1, b_edge_3_2 = node_2.backward(b_edge_2_1)
b_edge_4_1, b_edge_4_2 = node_1_1.backward(b_edge_3_1)
b_edge_4_3, b_edge_4_4 = node_1_2.backward(b_edge_3_2)

print('price : ', edge_4)
print('apple_num diff : ', b_edge_4_1)
print('apple diff : ', b_edge_4_2)
print('orange diff : ', b_edge_4_3)
print('orange_num diff : ', b_edge_4_4)
print('tax_diff : ', b_edge_2_2)
```

    price :  715.0000000000001
    apple_num diff :  110.00000000000001
    apple diff :  2.2
    orange diff :  3.3000000000000003
    orange_num diff :  165.0
    tax_diff :  650


### 5. 활성화 함수 계층 구현하기

5장에선 activation function인 ReLU와 Sigmoid를 구현합니다.

<b>_5.1. ReLU 계층_</b>

Rectified Linear Unit(ReLU) 함수의 수식과 미분에 대해선 다음과 같습니다.

$$
y = \begin{cases}
x\quad (x>0)
\\
0\quad (x\leq0)
\end{cases}
\\\\ \nonumber
\dfrac{\partial y}{\partial x} = 
\begin{cases}1\quad(x>0)
\\
0\quad (x\leq0)
\end{cases}
$$

이를 계산그래프와 코드로 나타내면 다음과 같습니다.


```python
img_11 = Image.open('./dataset/images/fig 5-18.png')
img_11
```




    
![png](/assets/images/DLScratch_05_01/img_08.png)
    




```python
class Relu:
    def __init__(self):
        self.mask = None            # mask는 T/F로 구성된 넘파이 배열로, 범위에 따른 T/F를 반환합니다.
    
    def forward(self, x):
        self.mask = (x <= 0)        # 0 이하면 True 반환, out value 0 반환
        out = x.copy
        out[self.mask] = 0    

    def backward(self, dout):       # dout 값에 곱해지는 값임을 잊지말 것
        dout[self.mask] = dout * 0
        dx = dout * 1
        
        return dx

```

<b>_5.2. Sigmoid 계층_</b>

책에선 sigmoid 수식을 하나하나 뜯어 계산그래프화시키고, 이를 미분하였습니다. 

불필요하다고 생각되기에, 국소계산 ver.은 소개만 하고 간소화된 버전을 한 번에 구현하겠습니다.

먼저 국소 계산 버전입니다.


```python
img_12 = Image.open('./dataset/images/fig 5-20.png')
img_12
```




    
![png](/assets/images/DLScratch_05_01/img_09.png)
    



이제 구현할 간소화버전 및 해당 미분 수식 유도는 다음과 같습니다.


```python
img_13 = Image.open('./dataset/images/fig 5-22.png')
img_13
```




    
![png](/assets/images/DLScratch_05_01/img_10.png)
    



$$
\begin{align}
y &= \dfrac{1}{1+\exp(-x)}\quad,\nonumber
\\ \nonumber
\\
\dfrac{d}{dx}\cdot y &=\dfrac{d}{dx}\cdot(1+\exp(-x))^{-1} \nonumber
\\ \nonumber
\\ 
&=(-1)\cdot(1+\exp(-x))^{-2}\cdot\exp(-x)\cdot(-1) \nonumber
\\ \nonumber 
\\
&=\dfrac{\exp(-x)}{(1+\exp(-x))^2}\nonumber
\\ \nonumber
\\
\dfrac{dy}{dx}&=(\dfrac{1+\exp(-x)-1}{1+\exp(-x)})\cdot(\dfrac{1}{1+\exp(-x)})\nonumber
\\ \nonumber
\\
&=(\dfrac{1+\exp(-x)}{1+\exp(-x)}\cdot\dfrac{-1}{1+\exp(-x)})\cdot(\dfrac{1}{1+\exp(-x)})\nonumber
\\ \nonumber
\\
&=(1-y)\cdot y \nonumber
\end{align}
$$

이것이 의미있는 점은, 미분 결과가 오롯이 순전파의 출력 y로 표현이 된다는 점이고, backward 구현이 훨씬 용이해진다는 점입니다.


```python
class Sigmoid:
    def __init__(self):
        self.out = None 
    
    def forward(self, x):
        out = 1 / ( + np.exp(-x))
        self.out = out

        return out
    
    def backward(self, dout):
        dx = dout * (1 - self.out) * self.out

        return dx
```

### 6. Affine/Softmax 계층 구현하기

<b>_6.1. Affine 계층_</b>

신경망에의 forward 때 수행하는 행렬의 곱을 기하학에선 <b>affine transformation</b>이라고 합니다. 이를 참고하여 책에선 행렬곱 구현을 affine layer 구현이라 명명하였습니다.

다차원 배열끼리의 곱에서 제일 관건은 <b>차원을 일치</b>시키는 것입니다. 

스칼라를 기준으로 했던 계산 그래프 상에서의 역전파와 다른 점은 <b>곱 연산의 역전파에서 전치행렬 변환을 통해 차원을 일치시켜 준다</b>는 점입니다.


```python
img_14 = Image.open('./dataset/images/fig 5-25.png')
img_14
```




    
![png](/assets/images/DLScratch_05_01/img_11.png)
    



위와 같이 곱 연산시에 상류가 반대방향으로 흘러가는 것은 동일하지만, 이때 전치행렬 변환으로 차원을 일치시켜야하는 점이 추가됩니다. 

<b>_6.2. Affine for Batch 계층_</b>

이번엔 1차원 input data $\boldsymbol X$가 아닌 batch input data에 걸맞는 batch affine layer를 고려해보겠습니다.

아래의 케이스는 2차원 데이터 x가 N개 있는 케이스에 대한 batch single layer perceptron으로 볼 수 있습니다.


```python
img_14 = Image.open('./dataset/images/fig 5-27.png')
img_14
```




    
![png](/assets/images/DLScratch_05_01/img_12.png)
    



이상 batch $\boldsymbol X$에 대한 affine 구현은 다음과 같습니다. 


```python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None 
        self.db = None

    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b

        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)     
        self.dW = np.dot(self.xT, dout)
        self.db = np.sum(dout, axis = 0)

        return dx                       

```

<b>_6.3. Softmax-with-Loss 계층_</b>

Softmax 함수는 입력값을 정규화, 즉 값의 합이 1이 되도록하여 출력합니다. 

이를 해당 장에선 손실함수인 cross entropy error와 함께 묶어 softmax with loss layer라 명명하고 구현해보겠습니다.

계산그래프의 모양은 다음과 같습니다.


```python
img_15 = Image.open('./dataset/images/fig a-1.png')
img_15
```




    
![png](/assets/images/DLScratch_05_01/img_13.png)
    



해당 softmax와 cross entropy error 계층까지 white box화시켜 나타내면 다음과 같습니다.

이를 전체 그림으로 삼아 각 계층별 순전파 / 역전파를 하나하나 뜯어보겠습니다.


```python
img_16 = Image.open('./dataset/images/fig a-5.png')
img_16
```




    
![png](/assets/images/DLScratch_05_01/img_14.png)
    



6.3.1. 순전파_Softmax

softmax 함수의 수식은 다음과 같고, 순전파는 아래의 그림으로 이루어집니다.
$$
y_k=\dfrac{\exp(a_k)}{\sum\limits_{i=1}\limits^{n}\exp(a_i)}
$$


```python
img_17 = Image.open('./dataset/images/fig a-2.png')
img_17
```




    
![png](/assets/images/DLScratch_05_01/img_15.png)
    



forward softmax에 대한 코드는 4장에서 구현한 softmax 메소드를 참조합니다.


```python
def softmax(x):
    x = x - np.max(x)   # overflow를 위한 장치

    return np.exp(x) / np.sum(np.exp(x))

```

6.3.2. 순전파_Cross Entropy Error

cross entropy error 함수의 수식은 다음과 같고, 순전파는 아래의 그림으로 이루어집니다.
$$
L=-\sum\limits_{k}t_k\log y_k
$$


```python
img_18 = Image.open('./dataset/images/fig a-3.png')
img_18
```




    
![png](/assets/images/DLScratch_05_01/img_16.png)
    



forward cross entropy error 코드 역시 4장의 코드를 참조하였습니다.


```python
def cross_entropy(y, t):
    # 훈련데이터 : one-hot / 정답데이터 : only label 인 경우를 가정한 CEE 계산 코드

    if y.ndim == 1:                  #true label을 기준으로 차원 변환
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    
    if t.size == y.size:            # 훈련데이터 : one-hot / 정답데이터 : one-hot인 경우 필요한 변형, 정답인덱스만 추출.
        t = t.argmax(axis=1)

    delta = 1e-7
    batch_size = y.shape[0]
    cross_entropy_error = -np.sum(np.log(y[np.arange(batch_size), t])+ delta) / batch_size 
    
    # y[np.arange(batch_size), t] 정답 인덱스에 해당하는 y softmax 값을 추출하기 위한 코드. 역시 1과 유사할 수록 cross entropy error가 작아지는 구조다.
    
    return cross_entropy_error
```

6.3.3. 역전파_Cross Entropy Error

$\log x$의 미분 값은 $\dfrac{1}{x}$이므로 해당 계층의 역전파는 쉽게 계산할 수 있고, 이를 계산그래프로 나타내면 다음과 같습니다.


```python
img_19 = Image.open('./dataset/images/fig a-4.png')
img_19
```




    
![png](/assets/images/DLScratch_05_01/img_17.png)
    



6.3.4. 역전파_Softmax

해당 단계에선 앞의 역전파 값을 의식해야하므로 단계별로 접근해보겠습니다.

* Step 1. 앞의 역전파 값인 $-\dfrac{t_k}{y_k}$가 상류에서 흘러들어옵니다.

* Step 2. 곱 노드 연산을 수행합니다. 각 S와 각 요소 엣지에 대한 계산 식입니다.
$$
\begin{align}
&\because \quad y_1=\dfrac{\exp(a_1)}{S}\nonumber
\\ \nonumber
&-\dfrac{t_1}{y_1}\exp(a_1)=-t_1\dfrac{S}{\exp(a_1)}\exp(a_1)=-t_1S
\\ \nonumber
&-\dfrac{t_1}{y_1}\dfrac{1}{S}=-\dfrac{t_1}{\exp(a_1)}
\end{align}
$$
* Step 3. / 노드의 연산을 수행합니다. (순전파 출력의 제곱 마이너스 값을 상류의 값과 곱합니다)
$$
-(\dfrac{1}{S})^2\cdot S(t_1+t_2+t_3) = \dfrac{1}{S}(t_1+t_2+t_3)
$$

* Step 4. 합 노드 연산을 수행합니다. 즉, 값을 그대로 하류로 보냅니다. ($t$값은 one-hot이므로 그 합이 1인 성질을 이용합니다)

* Step 5. exp 노드 연산을 수행합니다. 
$$
\dfrac{\partial y}{\partial x}=\exp(x)
$$
성질을 이용하여, 두 상류의 값을 더하고 그 값에 $\exp(a_1)$을 곱합니다.
$$
\begin{align}
&(\dfrac{1}{S}-\dfrac{t_1}{\exp(a_1)})\exp(a_1) \nonumber
\\ \nonumber
&(\dfrac{y_1}{\exp(a_1)}-\dfrac{t_1}{\exp(a_1)})\exp(a_1),\quad\because\quad y_1=\dfrac{\exp(a_1)}{S}
\\ \nonumber
\\ \nonumber
\therefore\quad&y_1-t_1
\end{align}
$$
나머지 엣지들도 동일하게 적용할 수 있습니다.

해당 과정은 아래 계산그래프로 표현하였습니다.


```python
img_20 = Image.open('./dataset/images/fig a-4(6).png')
img_20
```




    
![png](/assets/images/DLScratch_05_01/img_18.png)
    



이와 같은 과정을 밟고 간소화한 이미지를 다시 보면 다음과 같습니다.


```python
img_15
```




    
![png](/assets/images/DLScratch_05_01/img_19.png)
    



결과만 놓고 해석하면, softmax 출력 값 y와 true label t의 값 차이가 전달됨을 확인할 수 있습니다. 

<b>처음 신경망을 설계한 목적대로 학습이 진행되면 결과값에 가까워지도록 매개변수의 값이 조정될 것임을 알 수 있습니다.</b>

이를 코드로 구현하면 다음과 같습니다.


```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy(self.y, self.t)

        return self.loss
    
    def backward(self, dout = 1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        
        return dx
```

이제 1-4장 그리고 위의 내용을 종합하여 파이썬 기본 모듈을 통해 신경망을 구현해보겠습니다.

내용이 조금 길어지고, 한 눈에 정리하기 위해 다음 게시물에서 다루도록 하겠습니다.
