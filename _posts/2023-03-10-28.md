---
layout: single
title:  "Linear Algebra for AI_#01"
categories: 
 - Linear Algebra
classes: wide
use_math: true

---
  
## Linear Algebra for AI_1

#### _Contents_

---
<b>_1. Intro_</b>

        1.1. Intro
        1.2. Basic of linear algebra
        
<b>_2. Linear System & Linear Transformation_</b>

        2.1. Linear equation and linear system
        2.2. Practice_1
        2.3. Linear combination
        2.4. Linear independent & dependent
        2.5. Basis and dimension of subspaces
        2.6. Linear transformation
        2.7. Linear transformation with Neural Networks
        2.8. Surjective &  one to one function
    
<b>_3. Least Square_</b>
   
        3.1. Least Squares Problem 
        3.2. Least Squares and the meaning of L.S.
        3.3. Normal equation
        3.4. Orthogonal Projection_1
        3.5. Orthogonal Projection_2
        3.6. Gram schmidt orthogonalization and QR decomposition
        3.7. Practice_2

<b>_4. Eigenvalue Decomposition_</b>
        
        4.1. Eigenvector and eigenvalue
        4.2. Null space and orthogonal complement
        4.3. Charicteristic equation
        4.4. Diagonalization
        4.5. eigenvalue decomposition and linear transformation
        4.6. Practice_3
        4.7. Further Study

<b>_5. Singluar Value Decomposition_</b>

        5.1. Singluar value decomposition_1
        5.2. Singluar value decomposition_2
        5.3. The application of eigenvalue & singular value decomposition
        
---

<span style='color:#808080'>_첨부된 모든 강의자료의 저작권은 NAVER Boostcourse '인공지능을 위한 선형대수' 강의에 있습니다_</span> 

<b>_Essence of Linear Algebra_</b>에서 다루지 않았지만 선형대수에서 중요한 SVD 등의 주제를 추가적으로 살펴보기 위해 네이버 부스트코스에 있는 주재걸 교수님의 '인공지능을 위한 선형대수' 강의를 듣고 정리, 요약한 내용입니다. 

### _Chapter 1. Intro(PASS)_

### _Chapter 2. Linear System & Linear Transformation_

#### <b>1. Linear equation and linear system</b>

선형시스템은 linear equation 들의 set을 지칭합니다. m개의 방정식이 있고 n개의 variable이 있을 때,

1. m = n, determinant를 이용해 해를 구할 수 있습니다. 
2. m < n, undeterminded system 으로써 기하학적으론 squished plane으로 볼 수 있습니다. 일반적으로 무한한 솔루션이 존재하며 regularization을 적용하여 적절한 해의 범위를 제약하여 구합니다.
3. m < n, overdeterminded system 으로써 해가 존재하지 않습니다. 

#### <b>2. Practice_1</b>

기초적인 행렬 연산에 대해 numpy 내장함수 등을 이용해보는 실습을 해보겠습니다.


```python
import numpy as np
```


```python
# column vector 

c = np.array([1, 2, 3])
print(c.shape)
print(c[0])
```

    (3,)
    1



```python
# row vector

r = np.array([[1, 2, 3]])
print(r.shape)
print(r[0, 1])
```

    (1, 3)
    2



```python
a = np.zeros((2, 2))
print(a)

b = np.ones((2, 2))
print(b)

c = np.full((2,2), 7)
print(c)

d = np.random.random((2, 2))
print(d)
```

    [[0. 0.]
     [0. 0.]]
    [[1. 1.]
     [1. 1.]]
    [[7 7]
     [7 7]]
    [[0.75136362 0.02818035]
     [0.01261808 0.76371452]]



```python
A = np.array([[1, 2], [3, 4], [5, 6]])
print(A)
print(A.shape)
```

    [[1 2]
     [3 4]
     [5 6]]
    (3, 2)



```python
B = np.array([[11, 12, 13, 14], [15, 16, 17, 18]])
B
```




    array([[11, 12, 13, 14],
           [15, 16, 17, 18]])




```python
A.T
```




    array([[1, 3, 5],
           [2, 4, 6]])




```python
np.dot(A,B)
```




    array([[ 41,  44,  47,  50],
           [ 93, 100, 107, 114],
           [145, 156, 167, 178]])




```python
np.dot(B, A)
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    /Users/sckwon/Desktop/MODU/Linear Algebra/Linear Algebra for AI/LAforAI_1.ipynb Cell 17 in 1
    ----> <a href='vscode-notebook-cell:/Users/sckwon/Desktop/MODU/Linear%20Algebra/Linear%20Algebra%20for%20AI/LAforAI_1.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a> np.dot(B, A)


    File <__array_function__ internals>:180, in dot(*args, **kwargs)


    ValueError: shapes (2,4) and (3,2) not aligned: 4 (dim 1) != 3 (dim 0)


Matrix-vector multiplication은 shape을 맞춰야하기 때문에 차원이 다르면 당연히 위와같이 오류가 납니다.


```python
A = np.array([[60, 5.5, 1], [65, 5.0, 0], [55, 6.0, 1]])
b = np.array([66, 70, 78])
```


```python
# identity matrix

eye_1 = np.eye(3)
eye_1
```




    array([[1., 0., 0.],
           [0., 1., 0.],
           [0., 0., 1.]])




```python
# inverse matrix

from numpy.linalg import inv
A_inv = inv(A)
A_inv
```




    array([[ 0.08695652,  0.00869565, -0.08695652],
           [-1.13043478,  0.08695652,  1.13043478],
           [ 2.        , -1.        , -1.        ]])




```python
# Wrong! (element-wise multiplication)
A*A_inv
```




    array([[ 5.21739130e+00,  4.78260870e-02, -8.69565217e-02],
           [-7.34782609e+01,  4.34782609e-01,  0.00000000e+00],
           [ 1.10000000e+02, -6.00000000e+00, -1.00000000e+00]])




```python
# Correct

A.dot(A_inv)
```




    array([[ 1.0000000e+00,  0.0000000e+00, -4.4408921e-16],
           [ 8.8817842e-16,  1.0000000e+00, -8.8817842e-16],
           [-8.8817842e-16,  0.0000000e+00,  1.0000000e+00]])



$\boldsymbol AX=\boldsymbol B$ 와 같은 linear system에서 해를 구해보겠습니다.


```python
x = A_inv.dot(b)
x
```




    array([ -0.43478261,  19.65217391, -16.        ])




```python
from numpy.linalg import solve
x = solve(A,b)
x
```




    array([ -0.43478261,  19.65217391, -16.        ])



위의 두 결과는 동일해보이지만 computation 상에서 inverse matrix를 계산하는 과정 중 오차가 누적되기 때문에 해 계산을 직접 접근하는 solve function을 이용하면 더 정확한 해를 계산할 수 있게됩니다. 

#### <b>3. Linear combination</b>

_1. matrix-vector multiplication_

matrix-vector multiplication에 해당하는 matrix equation은 vector equation인 linear combination 꼴(혹은 inner product)로 표현할 수 있습니다. 이는 essential of linear algebra의 <b>Duality</b> 부분에서 언급하였습니다. 

$$
matrix\ equation:
\begin{bmatrix}1&2&3 \\ 4&5&6 \\ 7&8&9\end{bmatrix}
\begin{bmatrix}x_1 \\ x_2 \\ x_3\end{bmatrix}=
\begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}
\quad \boldsymbol \Rightarrow \quad vector\ equation:
\begin{bmatrix}1 \\ 4 \\ 7\end{bmatrix}x_1+
\begin{bmatrix}2 \\ 5 \\ 8\end{bmatrix}x_1+
\begin{bmatrix}3 \\ 6 \\ 9\end{bmatrix}x_1=
\begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}
$$

이때 결과벡터 $$\begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}$$은 세 벡터의 span 안에 포함되어야 해가 존재하는 셈이 됩니다. 해당 내용은 matrix의 column을 vector로 보게하는 시야를 제공해줍니다. 이를 transpose로 나타내면 더욱 극명해지는데, 

$$
let\ A\boldsymbol X=\overrightarrow V,\quad (A\boldsymbol X)^T=\overrightarrow V^T\quad \boldsymbol \Rightarrow\quad \boldsymbol X^TA^T=\begin{bmatrix}x_1 &x_2 &x_3\end{bmatrix}\begin{bmatrix}1&4&7 \\ 2&5&8 \\ 3&6&9\end{bmatrix}=\begin{bmatrix}1 &2 &3\end{bmatrix}
$$

즉 1, 4, 7의 $x_1$배, 2, 5, 8의 $x_2$배, 3, 6, 9의 $x_3$배로 결과벡터가 표현되는 것입니다. 

_2. outer product_

한 예시를 들어보겠습니다. 

$$\begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}\begin{bmatrix}1&2&3\end{bmatrix}=\begin{bmatrix}1&2&3 \\1&2&3\\ 1&2&3 \end{bmatrix}$$ 

위의 경우처럼 outer product는 기하학적으로 차원이 확장되는 연산입니다. 해당 연산의 역 연산을 통해 확장된 차원을 저차원으로 돌리는 아이디어가 많이 사용됩니다. 

$$\begin{bmatrix} & & & & & \\  & & & & & \\ & & & & & \end{bmatrix}^{(100,50)}=\begin{bmatrix} \\ \\&  \\ \end{bmatrix}^{(100,)}\begin{bmatrix} & & & \end{bmatrix}^{(,50)}+\begin{bmatrix} \\ \\ & \\ \end{bmatrix}^{(100,)}\begin{bmatrix} & & & \end{bmatrix}^{(,50)}$$

다음과 같이 5000개의 숫자를 (100 + 50) * 2 = 300개로 근사적으로 표현하는 것이 한 예가 될 수 있습니다. 데이터 분석의 경우 

* work2vec
* PCA
* covariant matrix(gaussian)
* gram matrix(style transfer)

등에서 활용됩니다. 

#### <b>4. Linear dependent & independent</b>

Matrix equation 해의 유무는 선형독립/종속 여부로 구분할 수 있습니다. 

1. $x_1\overrightarrow v_1+x_2\overrightarrow v_2+\cdots+x_p\overrightarrow v_p=0$ 일때 이 식을 만족하는 $$\begin{bmatrix}x_1\\ \vdots\\ x_p\end{bmatrix}=\begin{bmatrix}0\\ \vdots\\ 0\end{bmatrix}$$가 유일하면 선형독립 그렇지 않고 다른 $x$ 값들이 존재하면 선형종속이라 합니다. 
2. 위와같은 식 꼴을 <b>_homogeneous linear system_</b>이라 합니다. 해당 사례 외에도 homogeneous system = b 꼴의 식일 때도 1번의 논리를 적용할 수 있습니다(원점 평행이동 관점).
3. 이를 통해 $A\boldsymbol X=B$ 에서도 유일 해를 가지면 linearly independent, 아닌 경우는 linearly dependent로 정의할 수 있습니다.

#### <b>5. Basis and dimension of subspaces</b>

1. Subspace : linear transformation에 closed 인 모든 vector 집합을 의미합니다. $\approx Span$ 
2. basis : given subspace H를 fully span하면서 independent 한 vector를 의미합니다. basis는 unique하지 않지만 차원에서 basis 개수는 유한합니다. 
3. rank : dimension of column space A

essential of linear algebra 에서 언급한 내용들이므로 간단하게 설명하고 넘어가겠습니다. 

#### <b>6. Linear transformation</b>

    
![png](/assets/images/LAforAI/img_01.png){:style="display:block; margin-left:auto; margin-right:auto"}
    


1. Domain : 정의역
2. Codomain : 공역(가능한 output value의 set)
3. Image : 원소(given x에 해단 output y)
4. Range : 치역
5. 함수는 input이 있으면 output이 존재해야하며, 1개의 intput은 unique한 함수 값을 갖습니다. 
일반적으로 domain / codomain은 $\mathbb{R}^3$(3차원 실수 벡터공간)을 가정합니다. 
6. linear transformation $T=T(cu+dw)=c\cdot T(u)+d\cdot T(w)$를 만족하는 $T$. 해당 변환은 아래처럼 언제나 matrix-vector multiplication으로 나타낼 수 있습니다.

$$T(\begin{bmatrix}1\\ 0\end{bmatrix})=\begin{bmatrix}2\\ 3\\ 5\end{bmatrix},\ T(\begin{bmatrix}0\\ 1\end{bmatrix})=\begin{bmatrix}3\\ 5\\ 7\end{bmatrix}\quad \boldsymbol \Rightarrow \quad x_1\begin{bmatrix}2\\ 3\\ 5\end{bmatrix}+x_2\begin{bmatrix}3\\ 5\\ 7\end{bmatrix}=\begin{bmatrix}2&3 \\ 3&5 \\ 5&7\end{bmatrix}\begin{bmatrix}x_1\\ x_2\end{bmatrix}$$

#### <b>7. Linear transformation with neural networks</b>

기존 sigmoid, $$\dfrac{1}{1+e^{-x}}$$ 함수를 변환의 입장에서 해석해보면, 대부분의 값을 0부터 1까지로 squishing 시키는 non linear transformation이고 원점 주위는 <b>_선형(과 유사하게)_</b> 변형시켜주는 함수입니다. Affine transformation으로 불리는 fully connected layer의 경우, $\boldsymbol WX+\boldsymbol b$ 꼴이므로 원점을 지나지 않아 선형변환이 아니게 되는데 bias term 만큼 $X$의 차원을 늘리고 $\boldsymbol W$에 bias column을 추가하는 trick으로 선형변환 문제에 대응할 수 있습니다. 

예를들면 $$\begin{bmatrix}w_{11}& w_{12}& b_1\\ w_{21}& w_{22}& b_2\\ w_{31}& w_{32}& b_3\end{bmatrix}\begin{bmatrix}x_1\\ x_2\\ b_1\end{bmatrix}$$ 과 같이 쓸 수 있습니다. 

#### <b>8. Surjective &  one to one function</b>

    
![png](/assets/images/LAforAI/img_02.png){:style="display:block; margin-left:auto; margin-right:auto"}    


(1) Onto(전사) : 공역(domain) = 치역(codomain)

$$\begin{bmatrix}2&4 \\ 3&5\end{bmatrix}\begin{bmatrix}i \\ j\end{bmatrix}$$ 를 생각해보면, $$\overrightarrow V=\begin{bmatrix}2 \\ 3\end{bmatrix}, \overrightarrow w=\begin{bmatrix}4 \\ 5\end{bmatrix}$$의 span이 모든 미지벡터 $$\overrightarrow X=\begin{bmatrix}x_1 \\ x_2\end{bmatrix}$$에 대해 치역이 됩니다. 하지만 $$\begin{bmatrix}3&6 \\ 4&7 \\ 5&8\end{bmatrix}\begin{bmatrix}i \\ j\end{bmatrix}$$의 경우, $\mathbb{R}^n\rightarrow \mathbb{R}^m$인데, $(n<m)$ 이므로 not onto에 해당됩니다. 이와 같은 경우는 <b>_decoding_</b> 과정으로 볼 수 있습니다. 결국 강제로 차원을 확장하는 개념이기 때문에 일부 차원만을 cover하는 변환이 됩니다. 물론 <b>_encoding_</b> 과정이 잘 이루어졌다면 decoding data가 mapping되는 공간도 유의미하게 됩니다. (codomain 영역을 유의미하게 제한시킬 수 있습니다) 다른 말로 <b>_manifold_</b>를 잘 근사할 수 있게 됩니다. 

논지를 연장시켜 neural network 상황을 생각해보겠습니다. Forward network의 경우 encoding 과정으로 볼 수 있습니다. 두 번째 노드의 값이 100이 나왔을 때 이 값은 첫 번째 노드 값들의 합인 40+60 일 수도 70+30 일 수도 있습니다. 이처럼 layer가 진행되면서 세세한 정보를 의도적으로 없애고 100 이란 유효한 정보만 추출되는 과정으로 볼 수도 있습니다. (정보 손실 or 정보 정제 과정) 만약 이러한 과정에서 onto가 되지 않는 경우는 학습 후 특정 노드의 edge 값이 0인 경우로 보면 됩니다. 

(2) Manifold : 치역이 존재할법한 영역.

(3) One-to-one : 일대일 함수. 꼭 onto일 필요는 없습니다. (; linearly independent)

Neural network의 forward 과정을 one-to-one으로도 볼 수 있는데요, 키와 몸무게가 동일한 다른 사람들을 생각하면 될 것 같습니다.

> <b>_To be continue..._</b>


