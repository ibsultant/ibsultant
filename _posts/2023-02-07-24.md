<span style='color:#808080'>_모든 첨부 이미지/영상은 3Blue1Brown에 저작권이 있습니다_</span> 

## Essence of Linear Algebra

#### _Contents_

---
01. Vectors

02. Linear combinations, span, and basis vectors

03. Linear transformations and matrices

04. Matrix multiplication as composition

05. Three-dimensional linear transformations

06. The determinant

07. Inverse matrices, column space and null space

08. Nonsquare matrices as transformations between dimensions

09. Dot porducts and duality

10. Cross products

11. Cross products in the light of linear transformations

12. Cramer's rule, explained geometrically

13. Change of basis

14. Eigenvectors and eigenvalues

15. A quick trick for computing eigenvalues

16. Abstract vector spaces

---


```python
from PIL import Image
```

### 4. Matrix multiplication as composition

이번 장에선 <b>_composition_</b>이라 불리는 linear transformation이 연속적으로 일어나는 경우를 생각해보겠습니다. 이 경우도 basis $i, j$ 의 최종 도착지점을 표기하는 방법을 이용함으로써 하나의 linear transformation으로 표현 가능합니다. 예를들면 shear 변환 이후 rotation 변환을 다음과 같이 표현할 수 있습니다. 

$$
\begin{bmatrix}1&1\\ 0&1\end{bmatrix}(\begin{bmatrix}0&-1\\ 1&0\end{bmatrix}\begin{bmatrix}x\\ y\end{bmatrix}) = \begin{bmatrix}1&-1\\ 1&0\end{bmatrix}\begin{bmatrix}x\\ y\end{bmatrix}
$$

로 정리하여 표현할 수 있습니다. 애니매이션으로 살펴보면 다음과 같습니다. 


<center><video src="https://user-images.githubusercontent.com/33462847/217172796-59e445cc-de64-4c1e-800a-510ce9998696.mov" controls="controls" height="70%" width="70%"></video></center>


즉 기하학적으로 두 행렬의 곱(composition; matrix multiplication)은 transformation이 연속적으로 일어난 것과 
같습니다. 유의할 점은 그 순서가 좌측에서 우측으로 일어난다는 점인데, 결국 함수의 입장에서 이해해보면 $f(g(h(x)))$을 해석할 때와 같음을 알 수 있습니다. 한 행렬곱의 예를 들어보겠습니다.

$$
\begin{align}
\begin{bmatrix}0&2\\ 1&0\end{bmatrix}\begin{bmatrix}1&-2\\ 1&0\end{bmatrix}
&=\begin{bmatrix}0&2\\ 1&0\end{bmatrix}\begin{bmatrix}1\\ 1\end{bmatrix}
+\begin{bmatrix}0&2\\ 1&0\end{bmatrix}\begin{bmatrix}-2\\ 0\end{bmatrix}
\nonumber
\\ 
&= 1\cdot \begin{bmatrix}0\\ 1\end{bmatrix}
+1\cdot \begin{bmatrix}2\\ 0\end{bmatrix}
+(-2)\cdot \begin{bmatrix}0\\ 1\end{bmatrix}
+0\cdot \begin{bmatrix}2\\ 0\end{bmatrix}
\nonumber
\end{align}
$$

이를 일반화하면 다음과 같습니다.

$$
\begin{bmatrix}a&b\\ c&d\end{bmatrix}\begin{bmatrix}e&f\\ g&h\end{bmatrix}
=e\cdot \begin{bmatrix}a\\ c\end{bmatrix}
+g\cdot \begin{bmatrix}b\\ d\end{bmatrix}
+f\cdot \begin{bmatrix}a\\ c\end{bmatrix}
+h\cdot \begin{bmatrix}b\\ d\end{bmatrix}
=\begin{bmatrix}ae+bg&af+bh\\ ce+dg&cf+dh\end{bmatrix}
$$

우린 좌측 row와 우측 column의 순서를 차례차례 채워넣으며 행렬곱을 푸는 방법을 (주로 고등교육을 받을 때) 외웠지만 실제론 좌측의 transformation에 대해 $transformed\ basis\  i, j$ 에 대한 vector-matrix multiplication을 알고리즘화하여 푼 것에 지나지 않습니다. 잘 알려진 <b>_행렬곱은 역이 성립하지 않는다 (Noncommutativity)_</b>에 대한 이유도 직접 좌표축을 기하학적인 입장에서 transformation 해보면 직관적으로 알 수 있습니다. 이는 <b>_결합법칙이 성립한다(Associativity)_</b>에 대한 부분도 같은 방식으로 직관적으로 알 수 있습니다. 너무나 trivial하여 증명도 필요 없습니다. 하지만 numerical 입장에서 증명한다면 지저분하게 참임을 보일 수 밖에 없는 점에서 기하학적인 직관이 주는 중요성을 알 수 있습니다.


<center><video src="https://user-images.githubusercontent.com/33462847/217173003-4b9fd72e-e7ce-4d7e-9661-04fd50cb95a5.mov" controls="controls" height="70%" width="70%"></video></center>


### 5. Three-dimensional linear transformations

3차원 linear transformation이 기존 2차원 선형변환과 다른 점은 basis가 하나 더 추가되어 $\hat{i},\hat{j},\hat{k}$ 가 된다는 점입니다. 외엔 다르게 해석할 부분이 없습니다. 

### 6. The determinant

Linear transformation은 좌표 평면이 grid 하게 변하는 과정을 나타내긴 했지만 공간이 커졌는지 / 작아졌는지 그리고 그 정도는 어느정도인지에 대한 질문에 대해선 대답을 해주지 않습니다. 이번 장의 determinant는 그에 대한 설명을 해주고 있습니다. 


<center><video src="https://user-images.githubusercontent.com/33462847/217173098-57b72d6a-9df5-4bdc-9203-b1e8b1373bca.mp4" controls="controls" height="70%" width="70%"></video></center>


Linear transformation 입장에서 단위 정사각형이 얼마나 변하는지 알면 공간상 어떤 지역이 어떻게 변할지 예측할 수 있습니다. 왜냐하면 grid가 평행하고 evenly spaced 가 조건이기 때문입니다. 또한 비선형 공간은 작은 단위 정사각형으로 근사할 수 있기 때문에 공간 변화도 동일하게 scaled 된다고 해석할 수 있습니다. 이러한 scaled factor 를 선형변환에 의한 영역의 크기 변환을 나타내는 factor라 하며 <b>_Determinant_</b>라 칭합니다.

예를들어 determinant를 det라 썼을 때,

* det = 2 : transformation 후 vector의 영역 크기가 2배 증가
* det = 0 : transformation 후 영역이 찌그러져 line or a point가 된다 (작은 차원으로 뭉게지는 셈)
* det = -2 : transformation 후 공간이 뒤집히고 vector의 영역 크기가 2배 증가

로 해석할 수 있습니다.

3차원에선 영역이 아닌 공간(부피)로 그 표현 범위가 확장하게 되기에 determinant가 의미하는 것도 부피의 변화를 의미하게 됩니다. 이때 column vector가 linearly dependent 하다면 linear transformation 이후 평행육면체에서 직사각형 / 선 / 점 등으로 차원이 줄어드는 것을 확인할 수 있을 것이고, $\hat{i},\hat{j},\hat{k}$ 가 오른손 규칙에 부합한다면 양수, 아니면 음수인 determinant가 됩니다. 

이제 2차원 정사각행렬에 대한 determinant를 계산하는 식 

$$
det(\begin{bmatrix}a&b\\ c&d\end{bmatrix})=ad-bc
$$

이 왜 위와같이 도출되는지 직관적으로 살펴보겠습니다. 결국 변환에 따른 넓이변화에 대한 문제이므로, 넓이가 1인 basis의 선형변환에 대해 넓이가 어떻게 변할지를 따지면 됩니다. (비선형 공간은 미분의 개념처럼 근사할 수 있다고 하였기에 대표적인 사례만 따집니다) 

* $b=c=0$ 인 경우 직사각형의 가로, 세로의 길이만 변하는 변환으로 determinant는 ad 입니다
* $b=0,c\neq 0$ or $ b\neq 0, c=0$ 인 경우 평행사변형 변환이 되므로 determinant도 ad로 동일하게 됩니다
* $b\neq 0, c\neq 0$ 인 경우 공간이 얼마나 찌그러지는 지를 의미하므로 전체 넓이에서 나머지 넓이를 빼는 식으로 접근해야 합니다

    
![png](/assets/images/EofLA/img_01.png)


위의 세 번째 케이스 즉 일반화된 경우를 토대로 ad-bc 에 대해 유추할 수 있게 되었습니다.

3차원 determinant는 다음과 같이 계산하여 구하는 것으로 알려져 있습니다. 다만 그 유도 과정이 강의의 본질과 거리가 멀어져 결과만 도식하고 넘어가도록 하겠습니다.

$$
det(\begin{bmatrix}a&b&c\\ d&e&f\\ g&h&i\end{bmatrix})
=a\cdot det(\begin{bmatrix}e&f\\ h&i\end{bmatrix})
-b\cdot det(\begin{bmatrix}d&f\\ g&i\end{bmatrix})
+c\cdot det(\begin{bmatrix}d&e\\ g&h\end{bmatrix})
$$

### 7. Inverse matrices, column space and null space

행렬의 쓰임새를 두 가지로 단순화하면

    1. Describe manipulation of space
    2. Solve certain systems of equations 

로 볼 수 있는데 여기선 2번의 내용을 짚어보겠습니다. 

$$
-2x+5y+3z=-3\\
4x+0y+8z = 0\\
1x+3y+0z = 2
$$

와 같은 _linear systems of equations_ 를 푸는 문제가 주어질 경우 행렬을 이용해 _matrix-vector equations_ 문제로 표현할 수 있습니다. 

$$
\begin{align}
&\Rightarrow \begin{bmatrix}2&5&3\\ 4&0&8\\ 1&3&0\end{bmatrix}
\begin{bmatrix}x\\ y\\ z\end{bmatrix}=
\begin{bmatrix}-3\\ 0\\ 2\end{bmatrix}\nonumber
\\
&\Rightarrow \boldsymbol A\overrightarrow X=\overrightarrow V \nonumber
\end{align}
$$

이러면 문제는 행렬 A로 인한 선형변환 후 $\overrightarrow V$가 되는 $\overrightarrow X$를 찾는 문제로 변형이 됩니다. 위의 예시는 3차원인데 2차원으로 문제를 축소하여 살펴보겠습니다. 먼저 생각해봐야 할 것은 해당 선형변환이 공간을 squish하는지 아니면 공간을 그대로 남겨두는지 확인해봐야합니다. 이때 _determinant_ 가 이용됩니다. 


<center><video src="https://user-images.githubusercontent.com/33462847/217173382-7abe88a4-fbc4-48b8-b90f-4861d91aa8a9.mp4" controls="controls" height="70%" width="70%"></video></center>


<b>_Case 1._</b> $det(A) \neq 0$

$\overrightarrow V$ 에 대응하는 $\overrightarrow X$는 오직 하나에 해당하며 이 경우 $\boldsymbol A$ 에 대해 inverse transformation을 할 수 있게되어 $\overrightarrow X$ 를 찾을 수 있습니다. 이러한 역변환 된 <b>_inverse matrix_</b> $\boldsymbol A$ 를 $\boldsymbol A^{-1}$ 로 표기합니다. inverse matrix 의 대표적인 속성으로,

$$
\boldsymbol A^{-1}\boldsymbol A = \boldsymbol A\boldsymbol A^{-1} = \begin{bmatrix}1&0 \\ 0&1\end{bmatrix}
$$

을 들 수 있는데, 이때 등장하는 $\begin{bmatrix}1&0 \\ 0&1\end{bmatrix}$ 은 <b>_identity matrix_</b> 로써 basis의 위치가 그대로 유지되는 transformation matrix에 해당됩니다.

위의 내용을 다시 참고하여 정리하면, 

$$
\begin{align}
&\boldsymbol A\overrightarrow X=\overrightarrow V \nonumber \nonumber \\
&\Rightarrow \boldsymbol A^{-1}\boldsymbol A\cdot \overrightarrow X=\boldsymbol A^{-1}\cdot \overrightarrow V \nonumber \\
&\Rightarrow \overrightarrow X=\boldsymbol A^{-1}\cdot \overrightarrow V \nonumber
\end{align}
$$

와 같이 정리하여 찾을 수 있습니다. 결국 $\overrightarrow V$ 에 역변환을 통해 $\overrightarrow X$ 를 찾는 것입니다. 


<center><video src="https://user-images.githubusercontent.com/33462847/217173470-98972048-1c31-4d95-90b6-c8fbc2b8dbde.mp4" controls="controls" height="70%" width="70%"></video></center>


<b>_Case 2._</b> $det(A)=0$

해당 케이스의 경우 $\boldsymbol A$ 선형변환은 공간을 squish 하므로 이를 역변환하는 matrix는 존재하지 않게 됩니다. 

* 여기서 존재하지 않는다는 의미를 짚어보면, inverse transformation에서 simple input에 multiple output을 mapping 하는 경우에 해당하기 때문입니다. Transformation은 function으로써 함수의 정의인 'one input one output'에 부합해야 합니다. 이는 2차원이 아닌 3차원에서도 동일하게 적용됩니다. 


<center><video src="https://user-images.githubusercontent.com/33462847/217173560-eb0e4d30-edd7-4258-b841-586a269dfd96.mp4" controls="controls" height="70%" width="70%"></video></center>


<b>_7.1. Rank_</b>

Linear transformation의 결과가 갖는 차원 수를 의미합니다. 만약 2X2 matrix transformation 이라면 최대 rank는 2가 됩니다. 3X3 matrix의 rank 값이 2라면 해당 벡터의 변환 결과는 공간이 붕괴(squish)된 것으로 해석할 수 있습니다. 

<b>_7.2. Column space_</b>

Set of all possible outputs of $\boldsymbol A\overrightarrow X$. 여기서 zero vector는 linear transformation의 필수 요소에 해당하기 때문에 언제나 col space에 포함됩니다. 여기서 의미하는 column은 basis의 변환 후 위치를 가리키는데 변환 후 basis의 span(<b>Span of columns</b>)을 column space 라고 생각하면 됩니다. 이를 기반으로 rank를 설명하면,

    * Rank : The number of dimensions in the column space
    * Full rank : column 갯수 = rank. Full rank에 해당하는 linear transformation에서 origin으로 변하는 vector는 zero vector 뿐입니다. 

만약 full rank가 아니라면 수 많은 vector $\overrightarrow V$ 가 zero vector로 변환될 것입니다. 이렇게 linear transformation 시 원점으로 이동되는 set of vectors를 해당 matrix의 <b>_Null Space_</b> or <b>_Kernel_</b>로 부릅니다. 예를들어 $\boldsymbol A\overrightarrow X =\begin{bmatrix}0\\ 0\end{bmatrix}$의 해는 null space입니다.


<center><video src="https://user-images.githubusercontent.com/33462847/217173669-110f5599-914e-4a33-ad30-479647af0dc7.mp4" controls="controls" height="70%" width="70%"></video></center>


<b>_7.3. Overview_</b>

Linear equation systems을 geometric하게 생각해보면 각 system은 선형변환을 갖습니다. 이때 inverse transformation을 갖는다면 system을 풀기위해 역변환을 하면 됩니다. 그게 안 된다면 column space가 해의 존재여부를 알려줄 것입니다. Null space는 모든 가능한 해집합이 어떻게 될지 알려줍니다. 

> <b>_To be continue..._</b>
