---
layout: single
title:  "Essence of Linear Algebra_#03"
categories: 
 - Linear Algebra
classes: wide
use_math: true

---
  

## Essence of Linear Algebra_3

#### _Contents_

---
01. Vectors

02. Linear combinations, span, and basis vectors

03. Linear transformations and matrices

04. Matrix multiplication as composition

05. Three-dimensional linear transformations

06. The determinant

07. Inverse matrices, column space and null space

08. Nonsquare matrices as transformations between dimensions

09. Dot porducts and duality 

10. Cross products

11. Cross products in the light of linear transformations

12. Cramer's rule, explained geometrically

13. Change of basis

14. Eigenvectors and eigenvalues

15. A quick trick for computing eigenvalues

16. Abstract vector spaces

---

<span style='color:#808080'>_모든 첨부 이미지/영상은 3Blue1Brown에 저작권이 있습니다_</span> 


```python
from PIL import Image
```

### 8. Nonsquare matrices as transformations between dimensions

본 강의에서 다루는 대부분의 matrix 는 square matrix 지만, 데이터 분석을 할 때 다루는 대부분의 matrix는 nonsquare에 해당합니다. 이에 간략히 짚고 넘어가겠습니다. 

<b>'Linear'</b>의 의미는 원점이 유지되면서 grid 가 평행하고 균일 간격임을 의미합니다. 그럼 두 가지 케이스를 살펴보겠습니다. 

1.$$\begin{bmatrix}2&0 \\ -1&1 \\ -2&1 \end{bmatrix}$$ 의 경우는 column space가 3차원 원점을 가로지르는 2차원 상의 모든 vector로써 column을 span 하였을 때 차원이 input dimension과 같기 때문에 full rank에 해당합니다. 이는 기하학적으로 2차원을 3차원에 mapping 하는 것으로 볼 수 있습니다. (<b>Squish</b>)

2.$$\begin{bmatrix}3&5&4 \\ 1&1&9 \end{bmatrix}$$ 의 경우는 3개의 basis vector가 2차원으로 이동하는 변환입니다. 즉 3차원에서 2차원으로의 변환/투영으로 해석할 수 있습니다. (<b>Projection</b>)

### 9. Dot product and duality

Typical dot product 의 하나로 $$\begin{bmatrix}1\\ 3\end{bmatrix}\cdot\begin{bmatrix}2\\ 4\end{bmatrix}=1\cdot 2+3\cdot 4=14$$ 의 예를 들 수 있습니다. 기하학적으로 dot product를 해석하자면 $$\overrightarrow V\cdot\overrightarrow W$$ 는 projection된 $$\overrightarrow W$$의 길이에 $$\overrightarrow V$$의 길이만큼 곱하는 것입니다. 방향이 다른 경우 결과 값은 음수가 됩니다. 이제 duality까지와 연관성을 위해 필요한 질문 두 가지를 생각해보며 내용을 전개해보겠습니다. 

<b>_Q1. The Symmetry of inner product_</b>

    
![png](/assets/images/EofLA/img_02.png)
    


위의 그림을 참조하여 대칭성이 성립하는지 알아보겠습니다. Projection 된 $\overrightarrow W$의 길이가 0이면 당연히 $\overrightarrow V\cdot\overrightarrow W=0$ 이며 이는 역이 성립됩니다. 직관적으로 $\overrightarrow V, \overrightarrow W$ 길이가 같으면 둘의 대칭성을 이용해 $\overrightarrow V\cdot\overrightarrow W=\overrightarrow W\cdot\overrightarrow V$ 를 이끌어 낼 수 있습니다. 



![png](/assets/images/EofLA/img_03.png)
    



이제 위의 그림처럼 $\overrightarrow V$의 길이를 두 배로하여 대칭성을 깨뜨리고 $\overrightarrow W$를 $2\overrightarrow V$ 에 projection 시키면 $$\overrightarrow W\cdot(2\overrightarrow V)=2(\overrightarrow W\cdot\overrightarrow V)$$ 임을 당연하게 이끌어낼 수 있습니다. 왜냐하면 $\lvert projected\quad \overrightarrow W\rvert$는 그대로이기 떄문입니다. 

반대로 $2\overrightarrow V$를 $\overrightarrow W$ 쪽에 투사시킨다면 $\overrightarrow V$ 와 $2\overrightarrow V$는 닮음이기에 $\lvert projected 2\overrightarrow V\rvert$도 두 배입니다. 즉 이 경우도 $\overrightarrow W\cdot(2\overrightarrow V)=(\overrightarrow W\cdot\overrightarrow V)\cdot 2$ 가 성립됩니다. 이렇게 대칭성이 깨져도 dot product 결과값은 순서에 상관없다는 사실을 이끌어낼 수 있습니다. 

<b>_Q2. The Relation between dot product and projection (Duality)_</b>

최종 목적인 duality를 설명하기 위해 vector의 관점(Q1)과 matrix & vector의 관점(Q2)을 통해 두 관련성을 짚어볼 수 있는데, 저차원 선형변환과 dot product의 비교를 통해 matrix와 vector간의 관련성에 더 직관적으로 접근할 수 있습니다. 먼저 2차원에서 1차원으로의 선형변환에 대해 살펴보겠습니다. matrix $\begin{bmatrix}1&-2\end{bmatrix}$ 가 있다 가정하면 $\overrightarrow V$의 선형변환 결과를 알 수 있습니다. $\overrightarrow V=\begin{bmatrix}4\\ 3\end{bmatrix}$ 라 할 때,

1. matrix 관점인 선형변환으로 봤을 때 $4\cdot (1)+3\cdot (-2)=-2$ 즉 1차원 수선에 (-2)에 위치하는 변환이 됩니다. Matrix-vector multiplication으로 표현하면 $$\begin{bmatrix}1& -2\end{bmatrix}\begin{bmatrix}4\\ 3\end{bmatrix}=1\cdot 4+3\cdot (-2)=-2$$ 이 됩니다. 


<center><video src="https://user-images.githubusercontent.com/33462847/217368980-3c284964-aac4-48c9-ad80-ccab39bd92be.mp4" controls="controls" height="70%" width="70%"></video></center>


2. vector 관점인 dot product로 봤을 땐 $$\begin{bmatrix}1\\ -2\end{bmatrix}\cdot\begin{bmatrix}4\\ 3\end{bmatrix}=1\cdot 4+(-2)\cdot 3=-2$$ 로 표현할 수 있습니다. 

이로서 <b>_(1X2) matrix와 2-d vector 간의 numerical 관계를 알 수 있게 됩니다._</b> 해당 내용을 기하학적 관점에서 다시 해석해보겠습니다. 

<b>_Q3. Duality?_</b>



    
![png](/assets/images/EofLA/img_04.png)
    



원점을 지나는 수선이 있다고 생각을 해보겠습니다. 그리고 해당 수선위에 있는듯 보이지만, 2차원 평면에 위치한 단위벡터 $\overrightarrow u$가 있습니다. 여기에 위의 분홍색 포인트를 edge로 하는 벡터들을 입력받아 수선에 투영시키면 이는 vector가 아닌 숫자를 내놓는 transformation을 정의한게 됩니다. (<b>2-dim vector가 아님에 유의</b>) 해당 수선은 linear transformation이므로 matrix $$\begin{bmatrix}?& ?\end{bmatrix}$$ 꼴로 나타낼 수 있습니다. 해당 값을 알아내기 위해 basis $i, j$가 어디에 도착할지 생각해보면 (당연히) 수선에 해당하고 이때 <b>Q1. The symmetry of inner product</b>의 성질을 이용해 ($\because\lvert\hat{i}\rvert=\lvert\hat{u}\rvert=1$) $\hat{i}$를 $\hat{u}$ 에 투영시키는 대신, $\hat{u}$를 $\hat{i}, \hat{j}$에 각각 투영시키면 이게 곧 $$\begin{bmatrix}?& ?\end{bmatrix}$$ 값인 $$\begin{bmatrix}u_x& u_y\end{bmatrix}$$ 가 됩니다. 이는 basis의 선형변환 $$\begin{bmatrix}u_x& u_y\end{bmatrix}\begin{bmatrix}i\\ j\end{bmatrix}$$임을 내적의 대칭성과 수선과 동일상에 있던 $\hat{u}$ 을 통해 알아낸 것과 같습니다. 이를 일반화된 벡터에 적용을 하면,

$$
\begin{bmatrix}u_x&u_y\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=u_x\cdot x+u_y\cdot y
$$

가 되고 결과적으로 <b>dot product</b>와 동일하게 됩니다. 이로써

    1. dot product는 기하학적으로 projection이란 점 (+ 대칭성을 이용한다는 점)
    2. 2-dim 에서 1-dim 으로의 선형변환이 dot product와 numerical 상 동일하다는 점
    3. 2-dim의 단위벡터와 1-dim 수선을 이용해 transformation matrix 를 해당 단위벡터의 projection으로 표현할 수 있다는 점

에서 <b>_matrix-vector product = dot product_</b>라는 사실을 알 수 있습니다. 여기서 만약 $\hat{u}$가 non unit vector라면 linear 가정에 의해 해당 $i, j$도 각각 scaling 되기 때문에 non unit vector의 dot product도 동일한 문제로 생각할 수 있습니다. 

이렇게 natural but surprisingly correspondence 가 두 수학적 대상물에 나타나는 것을 <b>_duality_</b>라고 하며, 이때의 $\hat{u}$를 <b>_dual vector_</b>라 합니다. 이번에 다룬 dot product는 벡터의 동일 방향성을 나타내는대 유용한 도구가 됩니다. Duality 관점에서 두 벡터를 dot product 한다는 것은 그 중 한 벡터를 transformation으로 보는 것입니다. 이렇게 양 쪽의 관점을 바꿔가며 해석하면 선형대수에 대해 더 깊은 통찰력을 얻게 해줍니다. 

### 10. Cross products

2차원 상에서 외적, cross product는 두 벡터가 만드는 평생사변형의 넓이를 뜻합니다. 

* $\overrightarrow V\times\overrightarrow W=$ Area of parallelogram

부호의 경우 $\overrightarrow V$ 가 $\overrightarrow W$의 오른쪽이면 양수, 왼쪽이면 음수에 해당합니다. 

* $\overrightarrow V\times\overrightarrow W=-\overrightarrow W\times\overrightarrow V$

'관례적'으로 $$\overrightarrow V=\begin{bmatrix}3\\ 1\end{bmatrix},\overrightarrow W=\begin{bmatrix}2\\ -1\end{bmatrix}, \overrightarrow V\times\overrightarrow W=det\begin{bmatrix}3& 2\\ 1&-1\end{bmatrix}$$ 입니다. 이를 기하학적으로 봤을 땐, $$\begin{bmatrix}3& 2\\ 1& -1\end{bmatrix}$$는 $\hat{i},\hat{j}$를 구하려는 평행사면형으로 변환시키는 linear transformation 이고 이에 대한 determinant는 그 영역 내 단위 사각형 면적의 scaling을 의미하므로 $$det\begin{bmatrix}3& 2\\ 1& -1\end{bmatrix}$$를 받아들일 수 있습니다. 물론 반대가 반대인 경우 determinant 계산 시 orientation flip이 반영된 것으로 이해하면 되기에 이 또한 자명하게 받아들일 수 있습니다. 

이제 이해한 외적의 개념을 바탕으로 

1. 두 vector 간 간격이 수직에 가까울수록 넓이가 최대가 되므로 외적 값 또한 클 것입니다.
2. 특정 vector를 scaling하면 그 면적이 scaling에 비례해 커지므로 외적 값은 vector scaling에 비례합니다.

라는 합리적 직관을 이끌어낼 수 있습니다. 

하지만 3-dim cross product의 정의는 사뭇 다른데요, 좀 더 정확한 정의는 다음과 같습니다. (위에서 관례적이라 표현한 이유도 이 때문입니다)

<b>_3-dim vector 2개를 결합해 새로운 3-dim vector를 만들어 내는 무엇_</b>이며 그 결과 값이 <b>vector</b>라는 것에 주목해야합니다. 

* $\overrightarrow V\times\overrightarrow W=\overrightarrow P$

이 때 length $\overrightarrow P$는 $\overrightarrow V\times\overrightarrow W$가 만드는 평행사변형의 넓이와 같으며, direction of $\overrightarrow P$는 $\overrightarrow V\times\overrightarrow W$에 오른손 규칙에 의거하여 수직입니다. 계산은 아래와 같습니다.

$$
\begin{bmatrix}V_1\\ V_2\\ V_3\end{bmatrix}
\times
\begin{bmatrix}W_1\\ W_2\\ W_3\end{bmatrix}
=
\begin{bmatrix}V_2 W_3-V_3 W_2\\ V_3 W_1-V_1 W_3\\ V_1 W_2-V_2 W_1\end{bmatrix}
\quad or \quad 
det(
\begin{bmatrix}i& V_1& W_1\\ j& V_2& W_2\\ k& V_3& W_3\end{bmatrix}
)
=
\hat{i}(V_2 W_3-V_3 W_2)
-
\hat{j}(V_1 W_3-V_3 W_1)
+
\hat{k}(V_1 W_2-V_2 W_1)
$$

두 번째 표기법은 notational trick에 해당하는데 이러한 basis를 이용해 linear combination된 조합식은 $\overrightarrow V, \overrightarrow W$ 에 수직인 유일한 vector가 됩니다. 

> <b>_To be continue..._</b>
