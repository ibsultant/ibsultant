---
layout: single
title:  "Deep Learning from Scratch_#06_(2/2)"
categories: 
 - Deep Learning_Basic
classes: wide
use_math: true

---

### &lt;밑바닥부터 시작하는 딥러닝&gt; 코드 및 내용을 정리해보았습니다.

 * Deep Learning from Scratch_06_(2/2)

---

## 6. Training Options for Learning_(2/2)

#### _Contents_

---
1. 매개변수 갱신
    
    1.1. 모험가 이야기<br>
    1.2. 확률적 경사 하강법(SGD)<br>
    1.3. SGD의 단점<br>
    1.4. 모멘텀<br>
    1.5. AdaGrad<br>
    1.6. Adam<br> 
    1.7. 어느 갱신 방법을 이용할 것인가?<br>
    1.8. MNIST 데이터 셋으로 본 갱신 방법 비교
    
2. 가중치의 초깃값
    
    2.1. 초깃값을 0으로 하면?<br>
    2.2. 은닉층의 활성화 값 분포<br>
    2.3. ReLU를 사용할 떄의 가중치 초깃값<br>
    2.4. MNIST 데이터셋으로 본 가중치 초깃값 비교

3. 배치 정규화

    3.1. 배치 정규화 알고리즘<br>
    3.2. 배치 정규화의 효과

4. 바른 학습을 위해

    4.1. Overfitting<br>
    4.2. Weight Decay<br>
    4.3. Dropout
    
5. 적절한 하이퍼파라미터 값 찾기

    5.1. 검증 데이터<br>
    5.2. 하이퍼파라미터 최적화<br>
    5.3. 하이퍼파라미터 최적화 구현하기

6. 정리

---


```python
from matplotlib.image import imread
from sympy.plotting import plot3d
from collections import OrderedDict
from mnist import load_mnist
from PIL import Image
from tqdm import tqdm


import matplotlib.pyplot as plt
import sympy as sy
import numpy as np
import os, sys
import pickle
```

Chapter 6. Part_1 을 이어서 작성해보겠습니다.

### 3. Batch Normalization

<b>_3.1. Batch Normalization Algorithm_</b>

앞 장에서 <b>가중치 값의 분표가 일정하게 다음 계층까지 전달되는 것을 초깃값이 잘 선정되었다</b>고 했습니다.

Batch Normalization은 이를 각 <b>layer별로 강제</b>하여 학습이 잘 되도록 하는 방법론입니다. 

특장점은 다음과 같습니다.

* <b>_학습 속도가 빨라집니다.(ImageNet 기준 기존 7% 수준의 학습량에서 성능 도달)_</b>
* <b>_초깃값에 크게 의존하지 않아도 됩니다._</b>
* <b>_Overfitting을 억제합니다._</b>

모두 신경망 설계에 있어 골치아픈 문제들입니다. Batch Normalization은 이를 해결해주는 획기적인 방법입니다.

미니배치 단위로 평균이 0 분산이 1이 되도록 정규화하는 것으로 해당 특장점이 나타나는 것인데요,

사용하는 방법은 데이터 분포를 정규화하는 Batch Norm 계층을 

> 선형성을 갖는 활성화함수 뒤에 사용하는 것은 문제가 발생할 수 있다. 

> 비선형성을 갖는 활성화함수(ReLU) 앞에 Scale & Shift 계수와 함께 사용.

> 활성화함수 뒤에 쓸 때엔 Scale & Shift를 이용하지 않고 사용.

> CNN의 경우 Convolution & Fully Connected Layer 직후 넣어준다. 

> Dropout 보다 앞에 넣어준다.

정도로 정리할수 있습니다. 




```python
img_07 = Image.open('./dataset/images/fig 6-16.png')
img_07
```




    
![png](/assets/images/DLScratch_06_02/img_01.png)
    



수식은 다음과 같습니다. 미니배치에 대한 평균 / 편차 / 정규화에 대한 식입니다.

$$
\begin{align}
\mu_B&\leftarrow\dfrac{1}{m}\sum\limits^{m}_{i=1}x_i\nonumber
\\ \nonumber
\sigma^2_B&\leftarrow\dfrac{1}{m}\sum\limits^{m}_{i=1}(x_i-\mu_B)^2
\\ \nonumber
\hat{x_i}&\leftarrow\dfrac{x_i-\mu_B}{\sqrt{\sigma^2_B+\epsilon}}
\\ \nonumber
y_i&\leftarrow\gamma\hat{x_i}+\beta
\end{align}
$$

각 입력데이터는 위의 값으로 변환하여 데이터 분포의 치우침을 방지합니다. 이후 정규화된 분포 값에 scale($\gamma$) 및 shift($\beta$) 를 수행합니다.

$\gamma=1,\beta=0$에서 시작해 차츰 적합한 값으로 조정을 거칩니다.($\epsilon$은 overflow를 방지하기 위한 값입니다)

여기서 하나 감안할 부분은, 미니배치에 대한 평균과 편차 값을 학습에 사용한다는 것입니다. 이는 오차를 야기합니다.

그러므로 실제 추론시엔 모집단에 대한 평균 / 편차에 대한 기대값을 사용해야 하는데, 수식으로 나타내면 다음과 같습니다.

$$
\begin{align}
E[x]&\leftarrow E_B[\mu_B]\nonumber
\\ \nonumber
\\ \nonumber
Var[x]&\leftarrow\dfrac{m}{m-1}E_B[\sigma^2_B]
\end{align}
$$

물론 모든 미니배치의 평균과 편차를 수집하기에 현실적인 어려움이 있을 수 있습니다. 

이에 모평균, 모분산의 기댓값으로 미치배치에 대한 지수이동 평균 / 분산을 이용합니다. 

$$
\mu_{mov} = \alpha\cdot\mu_{mov}+(1-\alpha)\cdot\mu_B
\\
\\
\sigma^2_{mov} = \alpha\cdot\sigma^2_{mov}+(1-\alpha)\cdot\sigma^2_B
$$

$\mu_{mov}$와 $\sigma^2_{mov}$는 모평균, 모분산의 기댓값입니다. $\alpha$는 이전 통계량에 주어지는 momentum 지수로 

> 작은 배치 : 높은 momentum (0.9 - 0.99)

> 큰 배치 : 낮은 momentum (0.6 - 0.85) 혹은 <b>decay momentum</b>

를 이용합니다.


계산그래프로 나타내면 다음과 같습니다.



```python
img_08 = Image.open('./dataset/images/fig 6-17.png')
img_08
```




    
![png](/assets/images/DLScratch_06_02/img_02.png)
    



단순 합/곱 연산이 아니기때문에 계산 노드 9개를 세분화해서 하나하나 접근해야합니다.

특히 forward는 쭉 따라가면 되지만 backward 연산이 미분이 연계되어 조금 나눠서 고려해야하는데요, 

역순으로 forward 연산과 backward 연산을 계산그래프 상으로 도식화한 후에 구현해보겠습니다.

_(그림의 출처를 남깁니다)_

https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html


<b>3.1.1. Step_9</b>

$\gamma\hat{x_i}+\beta$ 식에 해당하는 backward 입니다. 합 연산이므로 그대로 흘러갑니다. $\beta$의 경우 차원에 맞춰 $\sum$ 연산이 이루어집니다.


```python
img_09 = Image.open('./dataset/DLfromScratch_06_07.png')
img_09
```




    
![png](/assets/images/DLScratch_06_02/img_03.png)
    



<b>3.1.2. Step_8</b>

$\gamma\hat{x_i}$에 해당하는 backward 입니다. 곱 연산이므로 자리가 바뀌어 흘러갑니다. $\gamma$는 차원에 맞춰 $\sum$ 연산이 이루어집니다.


```python
img_10 = Image.open('./dataset/DLfromScratch_06_08.png')
img_10
```




    
![png](/assets/images/DLScratch_06_02/img_04.png)
    



<b>3.1.3. Step_7</b>

$({x_i-\mu_B})\cdot\dfrac{1}{\sqrt{\sigma^2_B}}$ 둘의 곱 연산입니다. 여기서 $\epsilon$는 생략합니다. 편의상 $x\mu$와 $inverted\ var$ 약어인 $ivar$로 표기하겠습니다.

$ivar$의 경우도 차원이 맞지 않아 $\sum$ 연산이 이루어집니다.


```python
img_11 = Image.open('./dataset/DLfromScratch_06_09.png')
img_11
```




    
![png](/assets/images/DLScratch_06_02/img_05.png)
    



<b>3.1.4. Step_6</b>

$\sqrt{\sigma^2_B}$의 역수 연산입니다. 

$$
y = \dfrac{1}{x},\quad\dfrac{dy}{dx}=-\dfrac{1}{x^2}
$$

를 반영하여 backward edge $divar$와 forward edge $sqrtvar$ 도함수 간의 곱 연산이 이루어집니다. 


```python
img_12 = Image.open('./dataset/DLfromScratch_06_10.png')
img_12
```




    
![png](/assets/images/DLScratch_06_02/img_06.png)
    



<b>3.1.5. Step_5</b>

$\sigma^2_B$의 square root 연산입니다.

$$
y =x^{1/2},\quad\dfrac{dy}{dx}=\dfrac{1}{2}x^{-1/2}
$$

를 반영하여 backward edge $dsqrtvar$와 forward edge $var$ 도함수 간의 곱 연산이 이루어집니다. 


```python
img_13 = Image.open('./dataset/DLfromScratch_06_11.png')
img_13
```




    
![png](/assets/images/DLScratch_06_02/img_07.png)
    



<b>3.1.6. Step_4</b>

$(x_i-\mu_B)^2$에 대한 $\dfrac{1}{N}\sum$ 연산입니다. 이는 forward 입장에서 column-wise summation에 해당합니다.

결국 합 연산과 동일하므로, $sq$와 차원이 같은 원소가 1인 matrix를 만들고, row의 수(N)에 맞춰 나눠주면 됩니다.


```python
img_14 = Image.open('./dataset/DLfromScratch_06_12.png')
img_14
```




    
![png](/assets/images/DLScratch_06_02/img_08.png)
    



<b>3.1.7. Step_3</b>

$x_i-\mu_B$에 대한 제곱 연산입니다. 

$$
y =x^{2},\quad\dfrac{dy}{dx}=2x
$$

를 반영하여 backward edge $dsq$와 forward edge $x\mu$ 도함수 간의 곱 연산이 이루어집니다. 


```python
img_15 = Image.open('./dataset/DLfromScratch_06_13.png')
img_15
```




    
![png](/assets/images/DLScratch_06_02/img_09.png)
    



<b>3.1.8. Step_2</b>

계산그래프 상에서 한 노드에 두 gradient가 올 땐 단순히 더해주면 됩니다. 

이후 - 앞 뒤 순서에 맞게 그대로 흘려줍니다. 이때 $d\mu$ 엣지는 차원을 맞추기위해 $\sum$ 연산이 진행됩니다.


```python
img_16 = Image.open('./dataset/DLfromScratch_06_14.png')
img_16
```




    
![png](/assets/images/DLScratch_06_02/img_10.png)
    



<b>3.1.9. Step_1</b>

$x_i$에 대한 $\dfrac{1}{N}\sum$ 연산입니다. <b>Step_3</b>와 동일하게 원소가 1인 matrix를 곱하고 이를 row의 수(N)에 맞춰 나눠줍니다.


```python
img_17 = Image.open('./dataset/DLfromScratch_06_15.png')
img_17
```




    
![png](/assets/images/DLScratch_06_02/img_11.png)
    



<b>3.1.10. Step_0</b>

마지막 노드엔 두 개의 gradient input이 존재합니다. 단순히 더해주면 됩니다.


```python
img_18 = Image.open('./dataset/DLfromScratch_06_16.png')
img_18
```




    
![png](/assets/images/DLScratch_06_02/img_12.png)
    



이렇게 계산그래프 입장에서 batch normalization에 대한 수식적인 접근이 끝났습니다. 클래스로 구현해보면 다음과 같습니다. 각 forward / backward 과정이 많다보니 한눈에 들어오지 않습니다만 private 함수로 구현된 __forward / __backward 함수에서 계산그래프 연산이 이루어지고, 그 값이 각각 forward / backward 함수로 전달되어 출력되는 구조입니다. 


```python
class BatchNormalization:
    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):
        # keyword arguments
        self.gamma = gamma
        self.beta = beta
        self.momentum = momentum
        self.input_shape = None
        self.running_mean = running_mean
        self.running_var = running_var
        self.batch_size = None
        # forward kargs
        self.xmu = None         # deviation of x
        self.sqrtvar = None     # standard deviation
        # backward kargs
        self.dgamma = None
        self.dbeta = None
    
    # train flag : 학습 여부에 따라 이동평균/분산 업데이트를 할지 결정해야한다. 
    # 학습 시엔 업데이트, 추론 시엔 누적된 이동평균/분산을 사용한다. 
    def forward(self, x, train_flag=True):
        self.input_shape = x.shape
        if x.ndim != 2:
            N, C, H, W = x.shape    # conv-img 등 다양한 데이터의 차원을 고려..?
            x = x.reshape(N, -1)     

        out = self.__forward(x, train_flag)

        return out.reshape(*self.input_shape)
        # 여기서 star(*)는 컨테이너 타입의 데이터를 unpacking하기 위한 목적이다. 텐서의 경우 가변인자가 되므로 사용한 것으로 추측.

    def __forward(self, x, train_flag):
        if self.running_mean is None:
            N, D = x.shape
            self.running_mean = np.zeros(D)
            self.running_var = np.zeros(D)

        if train_flag:  # 학습 시
            mu = x.mean(axis=0) # column
            xmu = x - mu
            var = np.mean(xmu**2, axis=0)
            sqrtvar = np.sqrt(var + 10e-7)
            xhat = xmu / sqrtvar

            # forward kargs
            self.batch_size = x.shape[0]
            self.xmu = xmu
            self.xhat = xhat
            self.sqrtvar = sqrtvar
            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu            
            self.running_var = self.momentum * self.running_var + (1-self.momentum) * mu
        else:           # 추론 시
            xmu = x - self.running_mean
            xhat = xmu / ((np.sqrt(self.running_var + 10e-7)))

        out = self.gamma * xhat + self.beta
        return out

    def backward(self, dout):
        if dout.ndim != 2:
            N, C, H, W = dout.shape
            dout = dout.reshape(N, -1)

        dx = self.__backward(dout)
        dx = dx.reshape(*self.input_shape)

        return dx

    def __backward(self, dout):
        dbeta = dout.sum(axis=0)
        dgamma = np.sum(self.xhat * dout, axis=0)
        dxhat = self.gamma * dout
        dxmu = dxhat / self.sqrtvar
        divar = np.sum(dxhat * self.xmu, axis=0)
        dsqrtvar = (- divar) / (self.sqrtvar * self.sqrtvar)
        dvar = 0.5 *  dsqrtvar / self.sqrtvar
        dxmu = dxmu + (2 / self.batch_size) * self.xmu * dvar
        dmu = np.sum(dxmu, axis=0)
        dx = dxmu - dxhat / self.batch_size

        self.dgamma = dgamma
        self.dbeta = dbeta

        return dx
```

<b>_3.2. Batch Normalization의 효과_</b>

위의 클래스를 기반으로 당장 구현해보고 싶지만, 안타깝게도 batch normalization을 반영하지 않은 신경망이 구축되었기 때문에 새로 코드를 짜야합니다. 이번엔 딥러닝 입문전에 앞서 배운 학습 튜닝 기술까지 반영한 multi-layer nueral network를 구현해보려 합니다. 어쩔 수 없이 뒤에 배울 Dropout에 대한 구현 코드도 들어가 있습니다. 간단히 코드 설명만 하겠습니다.

> Dropout

오버피팅을 억제하기 위해 뉴런을 임의로 삭제하는 방법입니다. 동작은 input data shape의 무작위 데이터에 대해 베르누이 분포를 따르는 0-1 사이 무작위 값을 할당하고, dropout 값 보다 큰 경우만 마스킹하여 출력하는 방식입니다. 여기서 주목해야 할 점은, 훈련시 노드의 병목화로 인해 입력값의 inflation이 발생한다는 점입니다. 이를 감안하여 입력 값에 (1 - dropout ratio)만큼의 값을 곱해주어 값을 neutral 하게 만들어 줍니다.


```python
class Dropout:

    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None

    def forward(self, x, train_flg=True):
        if train_flg:       # 훈련 시
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask
        else:               # 추론 시
            return x * (1 - self.dropout_ratio)

    def backward(self, dout):
        return dout * self.mask
```

이를 반영한 신경망 코드는 다음과 같습니다.

> MultiLayerNetExtend


```python
class MultiLayerNetExtend:
    def __init__(self, input_size, hidden_size_list, output_size, activation='relu', weight_init_std='relu', weight_decay_lambda=0, use_dropout=False, dropout_ratio=0.5, use_batchnorm=False):
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size_list = hidden_size_list
        self.hidden_layer_num = len(hidden_size_list)
        self.use_dropout = use_dropout
        self.weight_decay_lambda = weight_decay_lambda
        self.use_batchnorm = use_batchnorm
        self.params = {}

        # weight init
        self.__init_weight(weight_init_std)

        # layers; Affine-(Batchnorm)-Activation-(Dropout)-Affine
        activation_layer = {'sigmoid':Sigmoid, 'relu':Relu}
        self.layers = OrderedDict()
        for idx in range(1, self.hidden_layer_num+1):
            self.layers['Affine' + str(idx)] = Affine(self.params['W'+str(idx)],self.params['b'+str(idx)])
            
            if self.use_batchnorm:
                self.params['gamma'+str(idx)] = np.ones(hidden_size_list[idx-1])    # scale init
                self.params['beta'+str(idx)] = np.zeros(hidden_size_list[idx-1])    # shift init
                self.layers['BatchNorm'+str(idx)] = BatchNormalization(self.params['gamma'+str(idx)],self.params['beta'+str(idx)])
            
            self.layers['Activation_function'+str(idx)] = activation_layer[activation]()

            if self.use_dropout:
                self.layers['Dropout'+str(idx)] = Dropout(dropout_ratio)
            
        idx = self.hidden_layer_num + 1

        # 마지막 forward 계층은 activation 연산이 아닌 softmax-cross entropy error 연산 수행.
        self.layers['Affine'+str(idx)] = Affine(self.params['W'+str(idx)], self.params['b'+str(idx)])
        self.last_layer = SoftmaxWithLoss()
    
    def __init_weight(self, weight_init_std):
        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]
        for idx in range(1, len(all_size_list)):
            scale = weight_init_std # activation 짝에 맞는 가중치 초기화 진행
            if str(weight_init_std).lower() in ('relu', 'he'):
                scale = np.sqrt(2.0 / all_size_list[idx - 1])  
            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):
                scale = np.sqrt(1.0 / all_size_list[idx - 1])
            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])
            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])

    def predict(self, x, train_flg=False):
        for key, layer in self.layers.items():
            # Dropout과 BatchNorm의 경우는 훈련 / 추론여부에 따라 내부 변수업데이트가 달라진다.
            if "Dropout" in key or "BatchNorm" in key:
                x = layer.forward(x, train_flg)
            else:
                x = layer.forward(x)
            
        return x
               
    def loss(self, x, t, train_flg=False):
        y = self.predict(x, train_flg)
        
        weight_decay = 0
        for idx in range(1, self.hidden_layer_num + 2):
            W = self.params['W' + str(idx)]
            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)
        # weight decay update
        return self.last_layer.forward(y, t) + weight_decay
        
    def accuracy(self, X, T):
        Y = self.predict(X, train_flg=False)
        Y = np.argmax(Y, axis=1)
        if T.ndim != 1 : T = np.argmax(T, axis=1)

        accuracy = np.sum(Y == T) / float(X.shape[0])
        return accuracy

    def gradient(self, x, t):
        # forward
        self.loss(x, t, train_flg=True)

        # backward / Affine instance 내 backward 연산 수행
        dout = 1    # backward 함수 return dx를 chain 형식으로 전달
        dout = self.last_layer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        # forward/ backward 연산을 마쳐야 저장되는 Affine 클래스 인스턴스 내 self.dW/ .db 출력
        grads = {}
        for idx in range(1, self.hidden_layer_num+2):
            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]
            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db

            # 최종 layer 전까지 gamma / beta update 진행
            if self.use_batchnorm and idx != self.hidden_layer_num+1:
                grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma
                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta

        return grads
```

해당 신경망으로 훈련을 진행하겠습니다.


```python
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# 학습 데이터를 줄임
x_train = x_train[:5000]
t_train = t_train[:5000]

max_epochs = 20
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.01

def __train(weight_init_std):
    bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, weight_init_std=weight_init_std, use_batchnorm=True)
    network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, weight_init_std=weight_init_std)
    optimizer = SGD(lr=learning_rate)
    
    train_acc_list = []
    bn_train_acc_list = []
    
    iter_per_epoch = train_size / batch_size
    epoch_cnt = 0
    
    # Big #를 두고 epoch 기준으로 stop.
    for i in range(1000000000):
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = x_train[batch_mask]
        t_batch = t_train[batch_mask]
    
        for _network in (bn_network, network):
            grads = _network.gradient(x_batch, t_batch)
            optimizer.update(_network.params, grads)
    
        if i % iter_per_epoch == 0:
            train_acc = network.accuracy(x_train, t_train)
            bn_train_acc = bn_network.accuracy(x_train, t_train)
            train_acc_list.append(train_acc)
            bn_train_acc_list.append(bn_train_acc)
    
           # print("epoch:" + str(epoch_cnt) + " | " + str(train_acc) + " - " + str(bn_train_acc))
    
            epoch_cnt += 1
            if epoch_cnt >= max_epochs:
                break
                
    return train_acc_list, bn_train_acc_list

weight_scale_list = np.logspace(0, -4, num=16)
x = np.arange(max_epochs)

plt.figure(figsize=(20,20))
for i, w in enumerate(weight_scale_list):
    print( "============== " + str(i+1) + "/16" + " ==============")
    train_acc_list, bn_train_acc_list = __train(w)
    plt.subplot(4,4,i+1)
    plt.title("W:" + str(w))
    if i == 15:
        plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)
        plt.plot(x, train_acc_list, linestyle = "--", label='Normal(without BatchNorm)', markevery=2)
        plt.legend(loc='lower right')

    else:
        plt.plot(x, bn_train_acc_list, markevery=2)
        plt.plot(x, train_acc_list, linestyle="--", markevery=2)

    plt.ylim(0, 1.0)
    if i % 4:
        plt.yticks([])
    else:
        plt.ylabel("accuracy")
    if i < 12:
        plt.xticks([])
    else:
        plt.xlabel("epochs")    
plt.show()
```

    ============== 1/16 ==============
    ============== 2/16 ==============


    /var/folders/b9/mp1fh7ds7wz6829fc1klmjqr0000gn/T/ipykernel_67559/3095544286.py:64: RuntimeWarning: overflow encountered in square
      weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)
    /var/folders/b9/mp1fh7ds7wz6829fc1klmjqr0000gn/T/ipykernel_67559/3095544286.py:64: RuntimeWarning: invalid value encountered in double_scalars
      weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)


    ============== 3/16 ==============
    ============== 4/16 ==============
    ============== 5/16 ==============
    ============== 6/16 ==============
    ============== 7/16 ==============
    ============== 8/16 ==============
    ============== 9/16 ==============
    ============== 10/16 ==============
    ============== 11/16 ==============
    ============== 12/16 ==============
    ============== 13/16 ==============
    ============== 14/16 ==============
    ============== 15/16 ==============
    ============== 16/16 ==============



    
![png](/assets/images/DLScratch_06_02/img_13.png)
    


비교 실험조건은 다음과 같습니다. 

* network : (100,784)-(100-100-100-100-100)-10
* train_data : 10000 / 교재의 1000개는 너무 성능과장이 될듯하여 늘렸습니다.
* mini-batch : 100
* epoch : 20
* optimizer : SGD
* dropout : False
* weight_decay : True
* <b>_weight_init_std : 0-1 까지 log-scale로 16등분_</b>
* <b>_batchnorm : T/F_</b>

보다시피 Batch Normalization(;BN) 여부에 따라 학습진도가 확연히 빠름을 알 수 있습니다. 특히 이 경우 일부러 초깃값을 좋지 않게 주었는데, 

그 경우에도 BN의 경우는 학습이 잘 되는 것을 확인할 수 있습니다. 이처럼 BN은 학습속도뿐만 아니라 초깃값에 의존하지 않아도 된다는 큰 장점을 지닙니다. 

### 4. 바른 학습을 위해

Meachine Learning을 하다보면 train dataset에 지나치게 학습이 되어 되려 test dataset에는 성능이 나오지 않는 경우가 있습니다. 

당연히 좋지않습니다. train/test set의 성능차이가 적을 수록 generalization 성능이 높다고 합니다. 아래 그림과 같은 Robust한 성향을 갖도록 모델을 설계해야합니다.


```python
img_19 = Image.open('./dataset/DLfromScratch_06_17.png')
img_19
```




    
![png](/assets/images/DLScratch_06_02/img_14.png)
    



<b>_4.1. 오버피팅_</b>

Overfitting은 주로 다음의 두 경우에 일어납니다.

> 매개변수가 많고 표현력이 높은 모델

> 훈련 데이터가 적음

위 조건에 맞도록 임의로 오버피팅을 일으켜 구현해보겠습니다.


```python
(x_train, t_train),(x_test, t_test) = load_mnist(normalize=True)

x_train = x_train[:300]
t_train = t_train[:300]

network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10)
optimizer = SGD(lr=0.01)
max_epochs = 200
train_size = x_train.shape[0]
batch_size = 100

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = train_size / batch_size
epoch_cnt = 0

for i in tqdm(range(1000000000)):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grads = network.gradient(x_batch, t_batch)
    optimizer.update(network.params, grads)

    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)

        epoch_cnt += 1
        if epoch_cnt > max_epochs:
            break

plt.figure(figsize=(8, 6))
plt.plot(train_acc_list,label='train', marker='s', markevery=10 )
plt.plot(test_acc_list,label='test', marker='x',markevery=10)
plt.xlabel('epochs'), plt.ylabel('accuracy')
plt.xlim(0,200), plt.ylim(0,1)
plt.legend(loc='lower right', fontsize=15)
plt.show()
```

      0%|          | 600/1000000000 [00:59<27523:13:52, 10.09it/s]



    
![png](/assets/images/DLScratch_06_02/img_15.png)
    


보다시피 train set과 test set과 성능차이가 많이 나는 전형적인 overfitting 문제를 확인할 수 있습니다. 

<b>_4.2. 가중치 감소_</b>

오버피팅은 일반적으로 모델의 가중치가 너무 큰 경우 발생하는 경우가 많습니다. 이에 해당값에 패널티를 부여하여 오버피팅을 억제하는 방법이 <b>_weight decay_</b>입니다. 

이제 신경망의 목적인 <b>_손실함수 감소_</b>의 손실함수에 정확도 뿐만 아니라 가중치까지 추가하여 손실함수를 수정합니다.

Cross Entropy Loss 및 weight w에 대해 penalty를 추가한 식은 아래와 같습니다.

$$
\begin{align}\nonumber
Lo&ss(\boldsymbol W,\boldsymbol X) = DataLoss(\boldsymbol W,\boldsymbol X)+\dfrac{1}{2}\lambda\left\|\boldsymbol W\right\|^2\nonumber
\\ \nonumber
\\ \nonumber
\boldsymbol W&\leftarrow\boldsymbol W-\eta(\dfrac{\partial DataLoss}{\partial\boldsymbol W}+\lambda\boldsymbol W)
\\ \nonumber
\\ \nonumber
&\leftarrow\boldsymbol W(1-\eta\lambda)-\eta\dfrac{\partial DataLoss}{\partial\boldsymbol W}
\end{align}
$$

해당 식과 같이 $\boldsymbol W$와 $\boldsymbol X$로 구성된 손실함수식에 $\boldsymbol W$에 대한 L2-norm을 더해주는 것입니다. 

Gradient descent가 진행되는 동안 $\boldsymbol W$ 값은 $\lambda$ 계수만큼 추가적으로 조금씩 제약을 받는 모습에 착안하여 <b>_Weight Decay_</b> 라 명명되었습니다.

$\lambda$는 정규화의 세기를 결정하는 계수로 책에선 0.1을 적용하였습니다.

> L2-norm(L2-regularization)에 대한 설명은 회귀분석의 정규화 내용에 해당하는데, 추후 포스팅에서 좀 더 자세히 다루도록하겠습니다.

직관적으로 큰 가중치는 추가된 제약식으로 인해 손실함수의 상승에 지수배만큼 더욱 기여한다고 볼 수 있습니다. 

또한 L2-norm은 미분 가능하기때문에 closed form solution을 갖게 됩니다. 

구현시 적용되는 과정은,

* 순전파 : 최종 layer에서 계산된 Softmax with CrossEntropy 값에 누적된 L2 norm이 더해집니다
* 역전파 : 각 layer 별 gradient에 $\lambda\boldsymbol W$만큼 추가됩니다

이제 구현하여 Overfitting 감소수준을 확인해보겠습니다.


```python
weight_decay_lambda = 0.1

network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,weight_decay_lambda=weight_decay_lambda)


max_epochs = 200
train_size = x_train.shape[0]
batch_size = 100

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)
epoch_cnt = 0

for i in tqdm(range(10000)):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grads = network.gradient(x_batch, t_batch)
    optimizer.update(network.params, grads)

    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)

        #print("epoch:" + str(epoch_cnt) + ", train acc:" + str(train_acc) + ", test acc:" + str(test_acc))

        epoch_cnt += 1
        if epoch_cnt > max_epochs:
            break

plt.figure(figsize=(8, 6))
plt.plot(train_acc_list,label='train', marker='s', markevery=10 )
plt.plot(test_acc_list,label='test', marker='x',markevery=10)
plt.xlabel('epochs'), plt.ylabel('accuracy')
plt.xlim(0,200), plt.ylim(0,1)
plt.legend(loc='lower right', fontsize=15)
plt.show()
```

      6%|▌         | 600/10000 [01:01<16:00,  9.79it/s]



    
![png](/assets/images/DLScratch_06_02/img_16.png)
    


이전 실험대비 두 데이터셋 간의 성능 격차가 많이 줄어든 것을 확인할 수 있습니다.

하지만 test set의 성능이 좋아진 것이 아닌 train set의 성능이 감소하여 그 차이가 줄었다는 점을 주목해야 합니다.

<b>_4.3. Dropout_</b>

신경망이 복잡해지면 Weight Decay 만으론 과적합을 대응하기 어려워지는데, 이때 임의의 뉴런을 삭제하며 학습하는 <b>_Dropout_</b> 기법을 이용합니다. 

상기 기술하였듯, 동작은 input data shape의 무작위 데이터에 대해 베르누이 분포를 따르는 0-1 사이 무작위 값을 할당하고, dropout 값 보다 큰 경우만 마스킹하여 출력하는 방식입니다. 

여기서 주목해야 할 점은, 훈련시 노드의 병목화로 인해 입력값의 inflation이 발생한다는 점입니다. 

이를 감안하여 입력 값에 (1 - dropout ratio)만큼의 값을 곱해주어 값을 neutral 하게 만들어 줍니다.

구현을 간소화키셔주는 Trainer 클래스로 구현한 Dropout 의 효과는 다음과 같습니다.


```python
class Trainer:
    def __init__(self, network, x_train, t_train, x_test, t_test, epochs=20, mini_batch_size=100, optimizer='SGD', optimizer_param={'lr':0.01}, evaluate_sample_num_per_epoch=None, verbose=True):
        self.network = network
        self.verbose = verbose
        self.x_train = x_train
        self.t_train = t_train
        self.x_test = x_test
        self.t_test = t_test
        self.epochs = epochs
        self.batch_size = mini_batch_size
        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch

        # optimizer class instance화
        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'adagrad':AdaGrad, 
        'adam':Adam}
        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)
        
        self.train_size = x_train.shape[0]
        # batch size로 나눠지지 않으면 훈련이 돌아가지 않게 설계
        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1) 
        self.max_iter = int(epochs * self.iter_per_epoch)
        self.current_iter = 0
        self.curren_epoch = 0

        self.train_loss_list = []
        self.train_acc_list = []
        self.test_acc_list = []

    def train_step(self):
        batch_mask = np.random.choice(self.train_size, self.batch_size)
        x_batch = self.x_train[batch_mask]
        t_batch = self.t_train[batch_mask]

        grads = self.network.gradient(x_batch, t_batch)
        self.optimizer.update(self.network.params, grads)
        if self.verbose: print('train loss : ' + str(loss))
        
        if self.current_iter % self.iter_per_epoch == 0:
            self.curren_epoch += 1

            x_train_sample, t_train_sample = self.x_train, self.t_train
            x_test_sample, t_test_sample = self.x_test, self.t_test
            if not self.evaluate_sample_num_per_epoch is None:
                t = self.evaluate_sample_num_per_epoch
                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]
                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]

            train_acc = self.network.accuracy(x_train_sample, t_train_sample)
            test_acc = self.network.accuracy(x_test_sample, t_test_sample)
            self.train_acc_list.append(train_acc)
            self.test_acc_list.append(test_acc)

            if self.verbose:
                print("=== epoch : "+str(self.curren_epoch)+", train acc : "+str(train_acc)+", test acc : "+str(test_acc)+" ===")
        self.current_iter += 1

    def train(self):
        for i in range(self.max_iter):
            self.train_step()

        test_acc = self.network.accuracy(self.x_test, self.t_test)

        if self.verbose:
            print("========== Final Test Accuracy ===========")
            print("test acc : "+str(test_acc))
```


```python
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

x_train = x_train[:300]
t_train = t_train[:300]

use_dropout = True
dropout_ratio = 0.15

network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],output_size=10, use_dropout=use_dropout, dropout_ratio=dropout_ratio)
trainer = Trainer(network, x_train, t_train, x_test, t_test,
                  epochs=300, mini_batch_size=100,
                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=False)
trainer.train()
train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list
```

    Dropout: 100%|██████████| 900/900 [01:39<00:00,  9.01it/s]



```python
plt.figure(figsize=(8, 6))
plt.plot(train_acc_list,label='train with dropout', marker='s', markevery=10 )
plt.plot(test_acc_list,label='test with dropout', marker='x',markevery=10)
plt.title('MLP with Dropout : 0.15')
plt.xlabel('epochs'), plt.ylabel('accuracy')
plt.xlim(0,200), plt.ylim(0,1)
plt.legend(loc='lower right', fontsize=15)
plt.show()
```


    
![png](/assets/images/DLScratch_06_02/img_17.png)
    


정확도가 올라가는 양상은 줄었지만 train / test set 간의 성능차이는 유의미하게 줄어들었습니다. 이처럼 Dropout을 이용해 표현력을 높이면서도 Overfitting을 억제할 수 있습니다. 

### 5. Hyperparameter Search

<b>Hyperparameter는 학습을 진행하기 전에 직접 정해주어야 하는 값</b>을 의미하는데, 신경망 사이즈, mini-batch, learning rate, dropout 등을 말합니다. 

그동안 다양한 기법으로 신경망을 효율적으로 학습하는 방법을 배웠지만 다양한 기법의 최적 값을 찾는 것은 다른 문제입니다. 

이번 절에선 해당 값들을 효율적으로 탐색하는 방법을 설명합니다. 

<b>_5.1. Validation Dataset_</b>

기존에는 train data로 학습을 진행하고 test data로 범용성능을 평가했습니다. 하지만 <b>Hyperparameter 성능을 평가할 때는 test data로 평가해선 안 됩니다.</b> 

이는 좋고 나쁨의 기준이 test dataset이 되는 것이고, 결국 hyperparameter가 test data에 Overfitting 되어버리기 때문입니다. 

이러면 원래 의도한 모델의 범용성능을 기대할 수 없게 됩니다. 이리하여 hyperparmater의 성능 체크를 위한 전용 dataset이 필요해지는데, 

이러한 dataset을 <b>_Validataion Dataset_</b>이라 부릅니다.

> Train Dataset : Parameter Learning

> Validation Dataset : Hyperparameter Performance Evaluation

> Test Dataset : Neural Network's Generalization Performance Evaluation

일반적으로 데이터셋은 train / test 이렇게 두 가지로 분류되어있는데요, 훈련 데이터 셋의 20% 정도를 따로 validation set으로 나눠사용하면 됩니다. 

코드는 다음과 같습니다. 


```python
def shuffle_dataset(x, t):
    # n개의 숫자를 뒤섞어 출력합니다.
    permutation = np.random.permutation(x.shape[0])
    if x.ndim == 2:
        x = x[permutation,:]  
    else:
        x = x[permutation,:,:,:]
    t = t[permutation]

    return x, t

(x_train, t_train), (x_test, t_test) = load_mnist()
# 전체 데이터를 한 번 뒤섞어줍니다
x_train, t_train = shuffle_dataset(x_train, t_train)

validation_rate = 0.2
validation_num = int(x_train.shape[0] * validation_rate)

x_val = x_train[:validation_num]
t_val = t_train[:validation_num]
x_train = x_train[validation_num:]
t_train = t_train[validation_num:]
```

<b>_5.2. Hyperparameter Optimization_</b>

Hyperparameter 최적화의 핵심은 <b>optimization value가 존재하는 범위를 조금씩 줄여나가는 것</b>입니다. 

이를 다음의 순서로 요약할 수 있습니다.

Step_1. Hyperparameter 값의 범위를 설정합니다

Step_2. 설정한 범위에서 hyperparameter의 값을 무작위로 추출합니다

Step_3. 추출된 값을 사용하여 학습하고, validation set으로 정확도를 설정합니다 (작은 epoch 설정)

Step_4. 2단계, 3단계를 반복(100회 등)하며, 해당 정확도의 결과를 통해 hyperparameter의 범위를 좁힙니다

Step_5. 4단계를 반복하여 hyperparameter의 범위가 어느정도 좁아지면 압축된 범위에서 값을 하나 골라냅니다

<b>_5.3. Hyperparameter Optimization Implemetation_</b>

위의 단계를 참고하여 MNIST dataset에 대하여 learning rate와 weight decay $\lambda$에 대한 hypyerparameter 최적화를 진행해보겠습니다. (cs231n based)

Step_1. learning rate는 $10^{-6}~10^{-2}$, $\lambda$는 $10^{-8}~10^{-4}$으로 정합니다

Step_2. Uniform distribution 상에서 랜덤 추출을 진행합니다

Step_3. 부턴 코드로 구현해보겠습니다.


```python
# Dataset setting ---------------------------------------
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# tuning이니만큼 data 수를 작게 설정합니다
x_train = x_train[:500]
t_train = t_train[:500]

# validation / train dataset set 분할
validation_rate = 0.20
validation_num = int(x_train.shape[0] * validation_rate)
x_train, t_train = shuffle_dataset(x_train, t_train)
x_val = x_train[:validation_num]
t_val = t_train[:validation_num]
x_train = x_train[validation_num:]
t_train = t_train[validation_num:]

# Learning -----------------------------------------------
def __train(lr, weight_decay, epocs=50):
    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10, weight_decay_lambda=weight_decay)
    trainer = Trainer(network, x_train, t_train, x_val, t_val, epochs=epocs, mini_batch_size=100, optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)
    trainer.train()

    return trainer.test_acc_list, trainer.train_acc_list

# Hyperparameter Search -----------------------------------
optimization_trial = 100
results_val = {}
results_train = {}
for i in tqdm(range(optimization_trial),desc='Optimization Trial Rate'):
    # Step_1 / Step_2 범위지정 및 추출
    weight_decay = 10 ** np.random.uniform(-8, -4)
    lr = 10 ** np.random.uniform(-6, -2)
    
    val_acc_list, train_acc_list = __train(lr, weight_decay)
    #print("val acc:" + str(val_acc_list[-1]) + " | lr:" + str(lr) + ", weight decay:" + str(weight_decay))
    key = "lr:" + str(lr) + ", weight decay:" + str(weight_decay)
    results_val[key] = val_acc_list
    results_train[key] = train_acc_list
```

    Optimization Trial Rate: 100%|██████████| 100/100 [03:04<00:00,  1.84s/it]


이제 특정 범위 내에서 100개의 (lr, $\lambda) 조합을 추출하고 그 결과값을 저장하였습니다.

해당 결과값이 높은 순서대로 상위 20개의 그래프를 도식하면 다음과 같습니다.


```python
print("=========== Hyper-Parameter Optimization Result ===========")
graph_draw_num = 20
col_num = 5
row_num = int(np.ceil(graph_draw_num / col_num)) # ceil : 버림
i = 0

# 내림차순으로 정렬
plt.figure(figsize=(20,16))
for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):
    print("Best-" + str(i+1) + "(val acc:" + str(val_acc_list[-1]) + ") | " + key)

    plt.subplot(row_num, col_num, i+1)
    plt.title("Best-" + str(i+1))
    plt.ylim(0.0, 1.0)
    if i % 5: plt.yticks([])
    plt.xticks([])
    x = np.arange(len(val_acc_list))
    plt.plot(x, val_acc_list)
    plt.plot(x, results_train[key], "--")
    i += 1

    if i >= graph_draw_num:
        break

plt.show()
```

    =========== Hyper-Parameter Optimization Result ===========
    Best-1(val acc:0.72) | lr:0.007922719561859335, weight decay:2.5859610179835156e-06
    Best-2(val acc:0.69) | lr:0.007513226503139132, weight decay:5.738642341664799e-06
    Best-3(val acc:0.65) | lr:0.00456715306709843, weight decay:2.5426803115540295e-06
    Best-4(val acc:0.63) | lr:0.005822596610846045, weight decay:9.454495073251518e-06
    Best-5(val acc:0.59) | lr:0.004176480091907319, weight decay:1.111551382456206e-05
    Best-6(val acc:0.58) | lr:0.0055430481123394425, weight decay:5.296817692229537e-06
    Best-7(val acc:0.55) | lr:0.005144971961186659, weight decay:2.2464447616006923e-07
    Best-8(val acc:0.54) | lr:0.00468924143309318, weight decay:3.564501922411136e-05
    Best-9(val acc:0.52) | lr:0.0034579891473523134, weight decay:2.8296247007335762e-08
    Best-10(val acc:0.48) | lr:0.0033479158940125796, weight decay:1.88715150383735e-08
    Best-11(val acc:0.46) | lr:0.003204326392101528, weight decay:2.7773414042696258e-06
    Best-12(val acc:0.39) | lr:0.001944052030303057, weight decay:2.9254976272014356e-06
    Best-13(val acc:0.33) | lr:0.002035990074188782, weight decay:8.657936989497747e-05
    Best-14(val acc:0.33) | lr:0.0018814950797293702, weight decay:5.443755002387164e-07
    Best-15(val acc:0.32) | lr:0.000848725360218261, weight decay:3.140502439695672e-08
    Best-16(val acc:0.26) | lr:0.0011355529130971376, weight decay:1.2433108229444428e-07
    Best-17(val acc:0.24) | lr:0.0016779727979245368, weight decay:6.348797685982492e-08
    Best-18(val acc:0.23) | lr:0.0007029735572952065, weight decay:6.925349042867373e-06
    Best-19(val acc:0.22) | lr:0.0012886331411642142, weight decay:7.709350522387071e-07
    Best-20(val acc:0.22) | lr:0.00034132255548405407, weight decay:5.956583485651111e-06



    
![png](/assets/images/DLScratch_06_02/img_18.png)
    


위의 결과로 보면, Best-7까지는 제대로 학습이 이루어지는 것을 알 수 있습니다.

또한, 학습률은 0.001 - 0.01, 가중치 계수는 1e-8 - 1e-5 수준에서 높은 성능을 가짐을 확인할 수 있습니다.

이제 위의 압축된 범위를 다시 반복하며 최적값을 하나 뽑아내는 식으로 최적화가 이루어집니다.

### 6. Overview

* 매개변수 갱신 방법에는 확률적 경사 하강법(SGD) 외에도 모멘텀, AdaGrad, Adam 등이 있다.
* 가중치 초깃값을 정한느 방법은 올바른 학습을 하는데 매우 중요하다.
* 가중치의 초깃값으로는 'Xavier 초깃값'과 'He 초깃값'이 효과적이다.
* 배치 정규화를 이용하면 학습을 빠르게 진행할 수 있으며, 초깃값에 영향을 덜 받게 된다.
* 오버피팅을 억제하는 정규화 기술로는 가중치 감소와 드롭아웃이 있다.
* 하이퍼파라미터 값 탐색은 최적 값이 존재할 법한 범위를 점차 좁히면서 하는 것이 효과적이다.

> 한 줄 평 : 1-5장보다 6장 정리하고 구현하는데 쓰인 시간이 많을 정도로 고생했습니다..
