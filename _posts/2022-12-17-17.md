---
layout: single
title:  "Deep Learning from Scratch_#06_(1/2)"
categories: 
 - Deep Learning_Basic
classes: wide
use_math: true

---

### &lt;밑바닥부터 시작하는 딥러닝&gt; 코드 및 내용을 정리해보았습니다.

 * Deep Learning from Scratch_06_(1/2)

---

## 6. Training Options for Learning_(1/2)

#### _Contents_

---
1. 매개변수 갱신
    
    1.1. 모험가 이야기<br>
    1.2. 확률적 경사 하강법(SGD)<br>
    1.3. SGD의 단점<br>
    1.4. 모멘텀<br>
    1.5. AdaGrad<br>
    1.6. Adam<br> 
    1.7. 어느 갱신 방법을 이용할 것인가?<br>
    1.8. MNIST 데이터 셋으로 본 갱신 방법 비교
    
2. 가중치의 초깃값
    
    2.1. 초깃값을 0으로 하면?<br>
    2.2. 은닉층의 활성화 값 분포<br>
    2.3. ReLU를 사용할 떄의 가중치 초깃값<br>
    2.4. MNIST 데이터셋으로 본 가중치 초깃값 비교

3. 배치 정규화

    3.1. 배치 정규화 알고리즘<br>
    3.2. 배치 정규화의 효과

4. 바른 학습을 위해

    4.1. Overfitting<br>
    4.2. Weight Decay<br>
    4.3. Dropout
    
5. 적절한 하이퍼파라미터 값 찾기

    5.1. 검증 데이터<br>
    5.2. 하이퍼파라미터 최적화<br>
    5.3. 하이퍼파라미터 최적화 구현하기

6. 정리

---


```python
from matplotlib.image import imread
from sympy.plotting import plot3d
from collections import OrderedDict
from mnist import load_mnist
from PIL import Image
from tqdm import tqdm


import matplotlib.pyplot as plt
import sympy as sy
import numpy as np
import os, sys
import pickle
```

이번 장에서는 손실함수 최적화를 위한 다양한 방법들 / 가중치 매개변수의 적절한 초깃값 지정 및 하이퍼파라미터 설정 방법,

오버피팅 대응을 위한 Weight Decay, Dropout 등의 정규화 방법 및 Batch Normalization을 알아보고 구현해보겠습니다.

이를 통해 이전 장처럼 <b>_신경망 구현_</b> 에 목적을 두는 것이 아닌 <b>_신경망 학습의 효율과 정확도_</b> 에 목적을 두고 구현할 수 있게 됩니다.

### 1. 매개변수 갱신

신경망 학습의 목표는 전체 loss function에 해당하는 cost function의 값을 최소화하는 매개변수 W / b를 찾는 것입니다.

이를 optimization이라고 하며, 매개변수의 공간이 넓고 복잡하면 optimization value를 찾기는 매우 어려워집니다. 

신경망으로 치면 layer가 추가될 수록 기하급수적으로 어려워지는 것입니다.

앞 장에서 사용한 optimization methodology는 SGD(Stochastic Gradient Descent)로, 미분을 통해 매개변수의 기울기에 접근하는 아이디어입니다.

표본 매개변수의 기울기를 구하길 반복하고 기울기의 방향으로 업데이트를 반복하여 최적값을 탐색하였습니다. 

이번 절에선 SGD를 포함하여 다양한 Optim methodology를 알아보고 각 장단점을 체크해보겠습니다.

<b>_1.1. 모험가 이야기_</b>

강화학습의 exploration처럼 최적값에 대한 접근 방법을 <b>어느 방향으로, 얼마나 그리고 가끔은 다른 곳으로도</b> 가는지 정하는 부분은 탐험의 성질과 유사합니다.

<b>_1.2. 확률적 경사 하강법(SGD)_</b>

SGD를 통한 매개변수 기울기 갱신은 다음과 같습니다.

$$
\boldsymbol W \leftarrow\boldsymbol W-\eta\dfrac{\partial L}{\partial\boldsymbol W}
$$

이를 코드로 표현하면 다음과 같습니다. 앞 장과 달리 모듈화하여 구현하였기에 매개변수 갱신 알고리즘을 쉽게 골라서 쓸 수 있습니다. 


```python
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr

    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
'''
network = TwoLayerNet(...)
optimizer = SGD()

for i in range(10000):
    ...
    x_batch, t_batch = get_mini_batch(...)
    grads = network.gradient(x_batch, t_batch)
    params = network.params
    optimizer.update(params, grads)
    ...
'''
```




    '\nnetwork = TwoLayerNet(...)\noptimizer = SGD()\n\nfor i in range(10000):\n    ...\n    x_batch, t_batch = get_mini_batch(...)\n    grads = network.gradient(x_batch, t_batch)\n    params = network.params\n    optimizer.update(params, grads)\n    ...\n'



<b>_1.3. SGD의 단점_</b>

특정 공간을 한 예로 들어보겠습니다. 


```python
def func_2(x_1, x_2):
    return ((1/20)*x_1**2 + x_2**2)

x_1 = sy.symbols('x_1')
x_2 = sy.symbols('x_2')

plot3d(func_2(x_1, x_2), (x_1, -10, 10), (x_2, -10, 10), size = (10, 20))
```


    
![png](/assets/images/DLScratch_06_01/img_01.png)
    






    <sympy.plotting.plot.Plot at 0x125840eb0>



위와 같이 $x_1$은 완만하고 $x_2$는 가파른 경우 기울기가 쉽게 수렴하지 않음을 아래 gradient plot을 보면 확인할 수 있습니다.


```python
# 범위 설정
x1 = np.arange(-10, 11, 1)
x2 = np.arange(-5, 6, 1)
X1, X2 = np.meshgrid(x1, x2)    # 각 범위에 대해 모든 지점을 Grid 형식의 좌표로 반환

X1 = X1.flatten()
X2 = X2.flatten()

plt.figure()
plt.quiver(X1, X2, -(1/10)*X1, -2*X2)  # 화살표 플롯 생성, 직접 편미분했습니다.
plt.xlim([-10, 10])
plt.ylim([-5, 5])
plt.xlabel('x1')
plt.ylabel('x2')

plt.grid()
plt.show()
```


    
![png](/assets/images/DLScratch_06_01/img_02.png)
    


전역 최적점은 (0, 0)에 해당하지만 대부분의 gradient가 (0,0)이 아닌 다른 지점을 가리키고 있습니다.

추후 가시화할 갱신 경로까지 확인해보면 이는 더욱 명확해집니다. $\eta$ 비율만큼 일정하게 진동하며 천천히 최적값에 다가갑니다.

이처럼, 

> <b>SGD는 비등방성(anisotropy) 함수에 대한 탐색경로가 비효율적입니다.</b>

이러한 단점을 대체하기 위한 기법들인 <b>_Momentum, AdaGrad, Adam_</b> 을 알아보겠습니다.

<b>_1.4. Momentum_</b>

모멘텀은 기존의 기울기 갱신 방향에 가중치를 줌으로써 local optimum에 도달해도 빠져나올 수 있는 여지를 제공합니다.

수식은 다음과 같습니다.

$$ 
v\leftarrow\alpha v-\eta\dfrac{\partial L}{\partial\boldsymbol W}
\\
\\
\boldsymbol W\leftarrow\boldsymbol W + v
$$

즉, 기존의 학습률에 모멘텀 관성계수 $\alpha$가 추가되어 학습이 진행됩니다. 

외에 $v$는 이동벡터, $\eta$는 학습률, $\boldsymbol W$는 갱신을 위한 가중치 매개변수입니다.

관성계수가 1이면 SGD와 같은 식이 되는만큼 관성계수의 비중을 조절할 수 있으며 보통 0.9 정도로 설정합니다.

모멘텀도 모듈화하기 위해 클래스로 구현해보겠습니다.


```python
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momemtum = momentum
        self.v = None

    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)    # 이동벡터를 W / b 의 shape만큼 생성
        
        for key in params.keys():
            self.v[key] = self.momemtum*self.v[key] - self.lr*grads[key]
            params[key] += self.v[key]
```

<b>_1.5. AdaGrad_</b>

AdaGrad는 Adaptive Gradient의 약자로, learning rate에 대해 각 gradient를 그 값에 따라 차등 적용하는 특징이 있습니다.

수식은 다음과 같습니다.

$$
\boldsymbol h\leftarrow\boldsymbol h+\dfrac{\partial L}{\partial\boldsymbol W}\odot\dfrac{\partial L}{\partial\boldsymbol W}
\\
\\
\boldsymbol W\leftarrow\boldsymbol W-\eta\dfrac{1}{\sqrt{\boldsymbol h+\epsilon}}\dfrac{\partial L}{\partial\boldsymbol W}
$$

$\boldsymbol h$는 학습률 조정계수로, gradient의 제곱 합이 누적되는 구조입니다. $\odot$은 원소별 곱(hadamard product)을 의미하고 $\epsilon$은 발산을 막기위한 계수로 0에 가까운 값을 취합니다.

즉 gradient(갱신률) 값이 클 수록 그만큼 학습률을 낮춰 매개변수 수렴을 늦추고, gradient 값이 작을 수록 학습률을 크게하여 매개변수 수렴을 가속화합니다.

하지만 누적합이 갖는 특징으로 인해 gradient 갱신이 진행될 수록 학습률이 0에 가까워지게 되는 단점이 있습니다. 

이에 지수이동 평균으로 지나치게 커지는 $\boldsymbol h$를 개선한 RMSProp이 있습니다. 책에선 설명하지 않았기에 수식만 언급하고 넘어가겠습니다.

> <b>_RMSProp_</b> (Root Mean Square Propagation)

$$
\boldsymbol h\leftarrow\boldsymbol \gamma h+(1-\gamma)\dfrac{\partial L}{\partial\boldsymbol W}\odot\dfrac{\partial L}{\partial\boldsymbol W}
\\
\\
\boldsymbol W\leftarrow\boldsymbol W-\eta\dfrac{1}{\sqrt{\boldsymbol h}}\dfrac{\partial L}{\partial\boldsymbol W}
$$

이제 AdaGrad를 모듈화하기 위한 코드를 구현해보겠습니다.


```python
class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
        
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)
```

<b>_1.6. Adam_</b>

Adam은 Adaptive Moment Estimation의 약자로, 기존의 Momentum / AdaGrad / RMSProp의 장점을 융합한 방법론입니다.

Adam이 참고한 각 방법론의 특징은 다음과 같습니다. 
* Momentum : 기울기 갱신 방향에 가중치를 준다.
* AdaGrad  : gradient 수준에 따라 각 원소별 학습률을 차등적용한다.
* RMSProp  : 지수이동평균으로 지나치게 작아지는 학습률 갱신을 방지한다.

이를 수식으로 보면 다음과 같습니다.

$$
\begin{align}
\boldsymbol m &\leftarrow\beta_1\boldsymbol m+(1-\beta_1)\dfrac{\partial L}{\partial\boldsymbol W}\nonumber
\\ \nonumber
\\ \nonumber
\boldsymbol g &\leftarrow\beta_2\boldsymbol g+(1-\beta_2)\dfrac{\partial L}{\partial\boldsymbol W}\odot\dfrac{\partial L}{\partial\boldsymbol W} \nonumber
\\ \nonumber
\\ \nonumber
\hat{\boldsymbol m}&=\dfrac{\boldsymbol m}{1-\beta_1^t},\quad \hat{\boldsymbol g}=\dfrac{\boldsymbol g}{1-\beta_2^t}\nonumber
\\ \nonumber
\\ \nonumber
\boldsymbol W&\leftarrow\boldsymbol W-\dfrac{\eta}{\sqrt{\hat{\boldsymbol g}+\epsilon}}\cdot\hat{\boldsymbol m} \nonumber
\\ \nonumber
\\ \nonumber
\boldsymbol W&\leftarrow\boldsymbol W-\dfrac{\eta}{\sqrt{\dfrac{\boldsymbol g}{1-\beta_2^t}+\epsilon}}\cdot(\dfrac{\boldsymbol m}{1-\beta_1^t})
\\ \nonumber
\\ \nonumber
\boldsymbol W&\leftarrow\boldsymbol W-\dfrac{\eta}{\sqrt{\boldsymbol g+\epsilon'}\cdot\sqrt{\dfrac{1}{1-\beta^t_2}}}\cdot(\dfrac{1}{1-\beta_1^t})\cdot\boldsymbol m
\\ \nonumber
\\ \nonumber
\boldsymbol W&\leftarrow\boldsymbol W-(\eta\cdot\dfrac{\sqrt{1-\beta^t_2}}{1-\beta_1^t})\cdot\dfrac{1}{\sqrt{\boldsymbol g+\epsilon'}}\cdot\boldsymbol m
\\ \nonumber
\\ \nonumber
\boldsymbol W&\leftarrow\boldsymbol W-\eta'\cdot\dfrac{\boldsymbol m}{\sqrt{\boldsymbol g+\epsilon'}}\quad (\because \eta'=\eta\cdot\dfrac{\sqrt{1-\beta^t_2}}{1-\beta_1^t})\nonumber
\end{align}
$$

$\boldsymbol m$은 momentum, $\boldsymbol g$와 $\epsilon$은 adagrad를 참조한 부분입니다. $t$는 갱신횟수에 해당합니다.

각 $\beta$는 지수이동평균을 반영하였으며 실험적 결과에 따라 $\beta_1$은 <b>0.9</b>, $\beta_2$는 <b>0.999</b> 정도의 값을 활용합니다.

조금 다른부분은 $\hat{\boldsymbol m}$, $\hat{\boldsymbol g}$ 부분인데, $\boldsymbol m$과 $\boldsymbol g$의 초깃값이 0에 가깝고 gradient 또한 작아버리면 0에 수렴해버릴 수 있습니다. 

이를 방지하기 위한 평활 보정(<b>_bias correction_</b>)입니다. 결국 $t$ 값이 커지면 $\dfrac{1}{1-\beta^t}$ 값은 1에 수렴하기때문에 $\hat{\boldsymbol m}\approx\boldsymbol m$, $\hat{\boldsymbol g}\approx\boldsymbol g$가 됩니다.

식을 대입하여 정리하면 최종 식은 위와같이 간결해지게 됩니다. 이제 Adam을 모듈화하기 위한 코드를 구현해보겠습니다.


```python
class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.m = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1
        lr_t = self.lr * np.sqrt(1 - self.beta2**self.iter) / (1 - self.beta1**self.iter)

        for key in params.keys():
            self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
```

<b>_1.7. 어느 갱신방법을 이용할 것인가?_</b>

지금까지 언급한 네 개의 갱신기법들을 그림으로 비교하면 다음과 같습니다.


```python
def f(x, y):
    return x**2 / 20.0 + y**2

def df(x, y):
    return x / 10.0, 2.0*y

init_pos = (-7.0, 2.0)
params = {}
params['x'], params['y'] = init_pos[0], init_pos[1]
grads = {}
grads['x'], grads['y'] = 0, 0

optimizers = OrderedDict()
optimizers["SGD"] = SGD(lr=0.95)
optimizers["Momentum"] = Momentum(lr=0.1)
optimizers["AdaGrad"] = AdaGrad(lr=1.5)
optimizers["Adam"] = Adam(lr=0.3)

idx = 1

plt.figure(figsize=(15,10)) 

for key in optimizers:
    optimizer = optimizers[key]
    x_history = []
    y_history = []
    params['x'], params['y'] = init_pos[0], init_pos[1]
    
    for i in range(30):
        x_history.append(params['x'])
        y_history.append(params['y'])
        
        grads['x'], grads['y'] = df(params['x'], params['y'])
        optimizer.update(params, grads)
    

    x = np.arange(-10, 10, 0.01)
    y = np.arange(-5, 5, 0.01)
    
    X, Y = np.meshgrid(x, y) 
    Z = f(X, Y)
    
    # 외곽선 단순화
    mask = Z > 7
    Z[mask] = 0
    
    # 그래프 그리기
    plt.subplot(2, 2, idx)
    idx += 1
    plt.plot(x_history, y_history, 'o-', color="red")
    plt.contour(X, Y, Z)
    plt.ylim(-5, 5)
    plt.xlim(-10, 10)
    plt.plot(0, 0, '+')
    #colorbar()
    #spring()
    plt.title(key)
    plt.xlabel("x")
    plt.ylabel("y")

plt.show()
```


    
![png](/assets/images/DLScratch_06_01/img_03.png)
    


그림의 특징을 설명하면 다음과 같습니다. 

* SGD : 지나치게 탄성이 커서 수렴이 늦습니다.
* Momentum : 관성의 영향을 받고 있어 진동이 덜합니다. 하지만 기울기 변화폭이 큰 경우 여전히 새로운 관성의 영향을 받습니다.
* AdaGrad : 기울기 변화폭이 큰만큼 갱신폭도 커져서 빠르게 수렴함을 알 수 있습니다. 다만 너무 빨리 수렴하게 됩니다.
* Adam : Momentum과 AdaGrad의 특징을 둘 다 갖고 있습니다. <b>_현재 가장 무난하게 쓰이는 방법론입니다._</b>

이로서 대표적인 네 가지 매개변수 갱신 방법론을 알아보고 구현해보았습니다.

<b>_1.8. MNIST 데이터셋으로 본 갱신 방법 비교_</b>

이번엔 5장 마지막에서 진행했던 Backprop을 위의 네 가지 갱신방법만 바꾸어 진행해보고, 반복 횟수별로 Loss function의 값 추이를 지켜보겠습니다. 

전체 코드를 아래 올려보겠습니다. 바뀐건 n-layer에 대응할 수 있는 신경망과 모듈화된 최적화 방법론과 

후술할 가중치 초가화 및 weight decay 부분입니다.


```python
def sigmoid(a):
    return 1 / (1 + np.exp(-a))

class Sigmoid:
    def __init__(self):
        self.out = None

        def forward(self, x):
            out = sigmoid(x)
            self.out = out
            
            return out

        def backward(self, dout):
            dx = dout * (1 - self.out) * self.out 

            return dx

class Relu:
    def __init__(self):
        self.mask = None            # mask는 T/F로 구성된 넘파이 배열로, 범위에 따른 T/F를 반환합니다.
    
    def forward(self, x):
        self.mask = (x <= 0)        # 0 이하면 True 반환, out value 0 반환
        out = x.copy()
        out[self.mask] = 0 

        return out   

    def backward(self, dout):       # dout 값에 곱해지는 값임을 잊지말 것
        dout[self.mask] = 0         # dout * 0
        dx = dout                   # dout * 1
        
        return dx

class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.original_x_shape = None
        self.dw = None
        self.db = None

    def forward(self, x):
        ###########################################################         
        # tensor 대응
        self.original_x_shape = x.shape # tensor 차원을 튜플형식으로 반환
        x = x.reshape(x.shape[0],-1)    # data input에 맞춰 array 변환
        ###########################################################
        self.x = x

        out = np.dot(self.x, self.W) + self.b
        
        return out

    def backward(self, dout):
        # Batch data shape을 맞추기위한 transpose T
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis = 0)

        dx = dx.reshape(*self.original_x_shape) 
        # 여기서 star(*)는 컨테이너 타입의 데이터를 unpacking하기 위한 목적이다. 텐서의 경우 가변인자가 되므로 사용한 것으로 추측.

        return dx

def softmax(x):
    # batch 데이터의 경우 데이터의 갯수와 flatten 된 입력값을 포함하면 (100, 784)와 같은 2차원이되므로 차원을 필수로 고려해야한다.
    # 784 내에서 최대값만 빼야하는데 np.max의 경우 스칼라 최댓값만 고려하므로 axis를 지정해야하고,
    # batch 개수만큼 최댓값을 빼줘야하는데 이를 위해선 transpose가 필요하다. 이를 위해 불필요해보이는 두 번의 transpose가 수행되는 것
    if x.ndim == 2:                 # 기존 X : (N, x) 가정
        x = x.T                     # new X : (x, N)
        x = x - np.max(x, axis=0)   # np.max(x, axis=0) : (1, N)
                                    # (x, N) - (1, N) //차 연산 수행가능, (x', N) 반환
        y = np.exp(x) / np.sum(np.exp(x), axis=0)   # 동일하게 적용, (x'', N) 반환
        return y.T                  # (N, x'') 반환

    x = x - np.max(x)   # exp(x + c')을 통해 resizing을 하여 overflow 문제 해결.
                        # 이때 c'값을 - max(x)를 취하여 지수함수의 발산문제 해결
    return np.exp(x) / np.sum(np.exp(x))

def cross_entropy(y, t):
    # 훈련데이터 : one-hot / 정답데이터 : only label 인 경우를 가정한 CEE 계산 코드
    if y.ndim == 1:                  #true label을 기준으로 차원 변환
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    
    if t.size == y.size:            # 훈련데이터 : one-hot / 정답데이터 : one-hot인 경우 필요한 변형, 정답인덱스만 추출.
        t = t.argmax(axis=1)

    delta = 1e-7                    # t = 1일 때 CEE 값인 log(1/y + delta)를 통해 overflow를 방지.
    batch_size = y.shape[0]
    cross_entropy_error = -np.sum(np.log(y[np.arange(batch_size), t])+ delta) / batch_size 
    # y[np.arange(batch_size), t] 정답 인덱스에 해당하는 y softmax 값을 추출하기 위한 코드. 역시 1과 유사할 수록 cross entropy error가 작아지는 구조다.
    
    return cross_entropy_error
    
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy(self.y, self.t)

        return self.loss
    
    def backward(self, dout = 1):
        batch_size = self.t.shape[0]

        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때
            dx = (self.y - self.t) / batch_size
        else:                          # 정답 레이블이 단순 레이블인 경우
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            dx = dx / batch_size
                
        return dx

class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr

    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
            
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momemtum = momentum
        self.v = None

    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)    # 이동벡터를 W / b 의 shape만큼 생성
        
        for key in params.keys():
            self.v[key] = self.momemtum*self.v[key] - self.lr*grads[key]
            params[key] += self.v[key]

class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
        
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)

class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.m = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1
        lr_t = self.lr * np.sqrt(1 - self.beta2**self.iter) / (1 - self.beta1**self.iter)

        for key in params.keys():
            self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
```

새롭게 추가된  MultiLayerNet 클래스입니다.

후술할 가중치 초기화 및 weight decay 옵션 등이 추가되어있습니다.


```python
class MultiLayerNet:
    def __init__(self, input_size, hidden_size_list, output_size, activation='relu', weight_init_std='relu', weight_decay_lambda=0):
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size_list = hidden_size_list
        self.hidden_layer_num = len(hidden_size_list)
        self.weight_decay_lambda = weight_decay_lambda
        self.params = {}

        # init params
        self.__init_weight(weight_init_std)

        # 계층 생성. 각 layer 및 activation을 모듈화하여 적용.
        activation_layer = {'sigmoid' : Sigmoid, 'relu': Relu}
        self.layers = OrderedDict()

        # 매개변수는 affine 계층의 곱 연산 뒤에 지정한 activation 연산 수행
        for idx in range(1, self.hidden_layer_num+1):
            self.layers['Affine'+str(idx)] = Affine(self.params['W'+str(idx)], self.params['b'+str(idx)])
            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()

        idx = self.hidden_layer_num + 1

        # 마지막 forward 계층은 activation 연산이 아닌 softmax-cross entropy error 연산 수행.
        self.layers['Affine'+str(idx)] = Affine(self.params['W'+str(idx)], self.params['b'+str(idx)])
        self.last_layer = SoftmaxWithLoss()

    def __init_weight(self, weight_init_std):   # 각 activation function에 따라 두 초기화방법론 적용하여 진행
                                                # 기존엔 0.01 * 랜덤정규화 숫자로 초기화
        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]
        for idx in range(1, len(all_size_list)):
            scale = weight_init_std
            if str(weight_init_std).lower() in ('relu', 'he'):
                scale = np.sqrt(2 / all_size_list[idx -1])
            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):
                scale = np.sqrt(1 / all_size_list[idx -1])
            # 종류별로 초기회된 scale 수준을 각 W1, W2 ,... b1, b2,... 등에 배분
            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1],all_size_list[idx])
            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])
            
    def predict(self, x):
        # Layer 연산별로 각기 구현해 놓은 forward 연산 수행
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    def loss(self, x, t):
        y = self.predict(x) # SoftmaxWithLoss 연산 전 layer까지 값 반환

        weight_decay = 0    # 초기화
        for idx in range(1, self.hidden_layer_num + 2): # last layer Affine 연산까지
            W = self.params['W'+str(idx)]
            # weight_decay 식 적용
            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)
        
        return self.last_layer.forward(y, t) + weight_decay

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)    # one-hot idx
        if t.ndim != 1 : t = np.argmax(t, axis=1)   # just-label 도 one-hot 결과값에 맞추는 조건문

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def gradient(self, x, t):
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.last_layer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
        
        # 결과 저장
        grads = {}
        for idx in range(1, self.hidden_layer_num+2):
            grads['W'+str(idx)] = self.layers['Affine'+str(idx)].dW + self.weight_decay_lambda * self.layers['Affine'+str(idx)].W
            grads['b'+str(idx)] = self.layers['Affine'+str(idx)].db

        return grads
```

정의한 MultiLayerNet 클래스를 통해 MNIST dataset을 이용, 각 최적화 방법론을 적용하고 가시화하여 비교해보겠습니다.


```python
# 0. Data Load ========================
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

train_size = x_train.shape[0]
batch_size = 100
max_iterations = 30000


# 1. Experiment Setting ===============
optimizers = {}
optimizers['SGD'] = SGD()
optimizers['Momentum'] = Momentum()
optimizers['AdaGrad'] = AdaGrad()
optimizers['Adam'] = Adam()

networks = {}
train_loss = {}
for key in optimizers.keys():
    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100], output_size=10)
    train_loss[key] = []


# 2. Learning =========================
for i in tqdm(range(max_iterations), desc='Multi-Layer Neural Network Learning'):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    for key in optimizers.keys():
        grads = networks[key].gradient(x_batch, t_batch)
        optimizers[key].update(networks[key].params, grads)

        loss = networks[key].loss(x_batch, t_batch)
        train_loss[key].append(loss)

    if i % 6000 == 0:
        print("===========iteration  "+str(i)+"===========")
        for key in optimizers.keys():
            loss = networks[key].loss(x_batch, t_batch)
            print(key + ":" + str(loss))


# 3. Plotting =========================
def smooth_curve(x):
    """
    손실 함수의 그래프를 매끄럽게 하기 위해 사용
    kaiser window와 convolution 연산이 사용되었습니다. 
    본 내용과는 거리가 있어 자세한 내용은 생략합니다.
    """
    window_len = 11
    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]
    w = np.kaiser(window_len, 2)
    y = np.convolve(w/w.sum(), s, mode='valid')
    return y[5:len(y)-5]

```

    Multi-Layer Neural Network Learning:   0%|          | 11/30000 [00:00<10:13, 48.91it/s]

    ===========iteration  0===========
    SGD:2.3898080669787602
    Momentum:2.450717144211397
    AdaGrad:2.181990487925832
    Adam:2.1606680563755325


    Multi-Layer Neural Network Learning:  20%|██        | 6007/30000 [02:15<08:55, 44.77it/s]

    ===========iteration  6000===========
    SGD:0.1174465834224263
    Momentum:0.05587329048718801
    AdaGrad:0.03455605465474033
    Adam:0.021882141450446688


    Multi-Layer Neural Network Learning:  40%|████      | 12009/30000 [04:35<06:47, 44.13it/s]

    ===========iteration  12000===========
    SGD:0.08207222690343502
    Momentum:0.009886056200417753
    AdaGrad:0.005841315761211372
    Adam:0.02397574592721362


    Multi-Layer Neural Network Learning:  60%|██████    | 18005/30000 [07:01<04:29, 44.50it/s]

    ===========iteration  18000===========
    SGD:0.06616267278263127
    Momentum:0.0005565733919909079
    AdaGrad:0.001786612341343754
    Adam:0.00270994686161459


    Multi-Layer Neural Network Learning:  80%|████████  | 24008/30000 [09:39<03:06, 32.16it/s]

    ===========iteration  24000===========
    SGD:0.2105654268351562
    Momentum:0.0006786921534519645
    AdaGrad:0.0260193064483063
    Adam:0.04535915263184826


    Multi-Layer Neural Network Learning: 100%|██████████| 30000/30000 [12:39<00:00, 39.48it/s]



```python
markers = {"SGD": "o", "Momentum": "x", "AdaGrad": "s", "Adam": "D"}
x = np.arange(max_iterations)
plt.figure(figsize=(15, 15))
for key in optimizers.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)
plt.xlabel("iterations")
plt.ylabel("loss")
plt.xlim(0, 2000)
plt.ylim(0, 1)
plt.legend()
plt.show()
```


    
![png](/assets/images/DLScratch_06_01/img_04.png)
    


해당 그래프는 (100, 784)-(100)-(100)-(100)-(100)-(10) 모양으로 구성된 신경망입니다.

Activation function은 ReLu를 사용하였고, 가중치 초기화 방법론으론 He initialization을 사용하였습니다. 

Softmax를 통해 최종 estimation value 값을 출력 후 Cross Entropy Loss를 손실함수로 Error Backpropagation을 진행하였습니다. 

이때 사용한 optimization 기법은 SGD/ Momentum / AdaGrad/ Adam 등 입니다.

위의 그래프는 각 최적화 기법들의 반복에 따른 loss 값 비교 변화량을 나타낸 것입니다.

0에 가까운 loss에 근접하는 것을 수렴한다고 보았을 때, SGD가 가장 느리고 AdaGrad가 빠르게 학습됨을 확인할 수 있습니다.

전체 반복수를 다 나타내면 조금 명확하지 않아 2000 반복수까지 그렸습니다.

### 2. Weight Initialization

가중치의 초깃값 또한 신경망의 성능에 크게 영향을 미칩니다. 이번 절에선 초깃값을 설정하는 방법들에 대해 알아보겠습니다.

<b>_2.1. 초깃값을 0으로 하면?_</b>

 <b>Weight Decay</b>(가중치 감소)는 매개변수의 값이 작아지도록 학습하는 방법으로 오버피팅을 억제해 일반화 성능을 높이는데 도움을 줍니다.

그래서 기존에는 정규분포상의 값에 0.01을 곱하여 사용했는데요, 이와달리 전부 초깃값을 0으로 두는 것은 큰 차이가 있습니다.

값을 모두 0으로 두면 역전파시 곱 노드의 갱신이 동일해지기 때문에 학습이 유지되기떄문에 무작위 값을 사용하는 것입니다.

<b>_2.2. 은닉층의 활성화 값 분포_</b>

가중치의 초깃값에 따라 은닉층의 활성화 값의 변화를 관찰하면 초깃값의 영향력을 파악할 수 있습니다. (based CS231n)

이를 확인하기위한 코드는 다음과 같습니다. 세팅은 단순 정규분포 값입니다.


```python
x = np.random.randn(1000,100)
node_num = 100
hidden_layer_size = 5
activations = {}

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]

    w = np.random.randn(node_num, node_num) * 1
    a = np.dot(x, w)
    z = sigmoid(a)
    activations[i] = z

plt.figure(figsize=(12,4))
for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + "-layer")
    if i != 0: plt.yticks([], [])   # 첫 y에만 limit 표기
    
    plt.ylim(0, 7000)
    plt.hist(a.flatten(), 30 , range=(0, 1))
plt.show()
```


    
![png](/assets/images/DLScratch_06_01/img_05.png)
    


단순 dot 연산과 sigmoid 연산을 반복했을 뿐이지만 대부분의 activation value 값이 0과 1에 치우쳐져 있습니다. 

이것이 문제가 되는 이유는 역전파시의 시그모이드 미분 형태가 

$$
y = \dfrac{1}{1+\exp^{-x}}
\\
\dfrac{\partial y}{\partial x}=y(1-y)
$$

꼴로 정리되는데, 위와같이 그 값이 0과 1인 경우 미분값이 0이 되기때문에 문제가 됩니다. 

이를 <b>_Vanishing Gradient Problem_</b>이라고 부르며, 그동안 신경망을 깊게 설계할 수 없었던 주된 이유였습니다. 

이번엔 초깃값을 위의 정규분포 값 * 0.01 로 표준편차를 1에서 0.01로 바꿔 결과를 확인해보겠습니다.


```python
# when we use     
# w = np.random.randn(node_num, node_num) * 0.01

img_01 = Image.open('./dataset/DLfromScratch_06_01.png')
img_01
```




    
![png](/assets/images/DLScratch_06_01/img_06.png)
    



확실히 값의 좌우 치우침은 덜함을 확인할 수 있습니다만 <b>가운데 치우침</b>은 표현력 관점에서 또 다른 문제가 될 수 있습니다. 

> 각 층의 활성화 값은 적당히 고루 분포되어야 (+ 원점 대칭) 신경망 학습이 효율적으로 이루어진다. 

이번엔 Xavier와 Bengio의 논문에서 제안되어 업계 표준으로 쓰이는 <b>_Xavier_</b> 초깃값을 써보겠습니다.

자세한 내용은 논문에 쓰여있으며, 결론만 말씀드리면 노드 갯수($n$)에 따라 표준편차가 $\dfrac{1}{\sqrt{n}}$인 정규분포를 사용하는 것입니다.


```python
# when we use     
# w = np.random.randn(node_num, node_num) * (1/np.sqrt(node_num))

img_02 = Image.open('./dataset/DLfromScratch_06_02.png')
img_02
```




    
![png](/assets/images/DLScratch_06_01/img_07.png)
    



앞의 두 초깃값 방법론에 비해 월등한 차이를 보여주고 있습니다.

sigmoid 함수의 표현력에 제한 없이 효율적으로 학습이 이루어질 것임을 기대할 수 있습니다.

추가적으로 activation function을 sigmoid가 아닌 쌍곡선 함수 tanh 함수를 사용하면 위의 일그러지는 모양까지 개선할 수 있습니다.


```python
# when we use  
# z = tanh(a)   
# w = np.random.randn(node_num, node_num) * (1/np.sqrt(node_num))

img_03 = Image.open('./dataset/DLfromScratch_06_03.png')
img_03
```




    
![png](/assets/images/DLScratch_06_01/img_08.png)
    



<b>_2.3. ReLU를 사용할 때의 가중치 초깃값_</b>

각 activation 함수마다 권장되는 초깃값 갱신 방법론이 다릅니다. 

* Sigmoid / tanh : Xavier initialization
* ReLU : He initialization

Xavier의 경우, <b>선형함수</b>를 전제로 하고 있습니다. 이에 ReLU는 부합되지 않습니다.

Sigmoid 함수와 tanh 함수는 아래와같이 좌우대칭이며 중심부가 선형인 함수로 가정할 수 있습니다. 


```python
def tanh(x):
    return np.tanh(x)

def ReLU(x):
    return np.maximum(0, x)

x = np.arange(-2, 2, 0.01)

plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.plot(x, sigmoid(x))
plt.title('Sigmoid')
plt.subplot(1, 3, 2)
plt.plot(x, tanh(x))
plt.title('tanh')
plt.subplot(1, 3, 3)
plt.plot(x, ReLU(x))
plt.title('ReLU')
```




    Text(0.5, 1.0, 'ReLU')




    
![png](/assets/images/DLScratch_06_01/img_09.png)
    


 이러한 ReLU에 어울리는 initialization 기법은 Kaiming He 의 기법으로 He initialization이라 부릅니다.

 다른 점은 n개의 노드인 경우, 같은 정규분포 값에 $\sqrt{\dfrac{2}{n}}$의 표준편차 값을 이용한다는 것입니다.

 직관적으로 해석하면 ReLU 함수 양의 부분의 분포를 음의 영역이 0인만큼 2배 확장한다. 정도로 해석할 수 있겠습니다.

 이번엔 ReLU 활성화함수에 0.01 편차의 정규분포 / Xavier / He initialization 순서대로 도식해보겠습니다.


```python
img_04 = Image.open('./dataset/DLfromScratch_06_04.png')
img_05 = Image.open('./dataset/DLfromScratch_06_05.png')
img_06 = Image.open('./dataset/DLfromScratch_06_06.png')

plt.figure(figsize=(15,17))
plt.subplot(311), plt.axis('off'), plt.title('Standard Normal, sigma - 0.01'), plt.imshow(img_04) 
plt.subplot(312), plt.axis('off'), plt.title('Xavier Initialization'), plt.imshow(img_05)
plt.subplot(313), plt.axis('off'), plt.title('He Initialization'), plt.imshow(img_06)

```




    (<AxesSubplot: title={'center': 'He Initialization'}>,
     (0.0, 1.0, 0.0, 1.0),
     Text(0.5, 1.0, 'He Initialization'),
     <matplotlib.image.AxesImage at 0x12b589210>)




    
![png](/assets/images/DLScratch_06_01/img_10.png)
    


각 그래프를 해석하자면, 

* <b>Standard Normal(sigma-0.01)</b>은 0 부근으로 활성화 값이 많이 쏠려 학습이 제대로 이루어지지 않을 것으로 추측할 수 있습니다.
* <b>Xavier initialization</b>은 층이 깊어질 수록 치우침이 심해집니다. 학습이 진행될 수록 Vanishing Gradient가 심해질 것으로 추측할 수 있습니다.
* <b>He initialization</b>은 층이 깊어짐에도 값의 분포가 균일하게 잘 유지되고 있습니다. 좋은 학습결과를 기대할 수 있습니다. 

앞의 결과들로 정리하자면,

> 활성화함수가 ReLU > He Initialization

> 활성화함수가 Sigmoid > Xavier Initialization

> 활성화함수가 tanh > Xavier Initialization

을 쓰는것이 적절하다고 할 수 있겠습니다.

<b>_2.4. MNIST 데이터셋으로 본 가중치 초깃값 비교_</b>

이번엔 MNIST 데이터셋으로 활성화함수-초깃값의 영향력을 확인해보도록 하겠습니다.

사용한 네트워크는 뉴런 수가 100개인 5층 신경망입니다.


```python
# 0. Data Load ========================
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

train_size = x_train.shape[0]
batch_size = 128
max_iterations = 2000

# 1. Experiment Setting ========================
weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}
optimizer = SGD(lr=0.01)

networks = {}
train_loss = {}
for key, weight_type in weight_init_types.items():
    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],
                                  output_size=10, weight_init_std=weight_type)
    train_loss[key] = []

# 2. Learning ========================
for i in tqdm(range(max_iterations), desc='Multi-Layer Neural Network Learning'):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    for key in weight_init_types.keys():
        grads = networks[key].gradient(x_batch, t_batch)
        optimizer.update(networks[key].params, grads)
    
        loss = networks[key].loss(x_batch, t_batch)
        train_loss[key].append(loss)
    
    if i % 500 == 0:
        print("===========" + "iteration:" + str(i) + "===========")
        for key in weight_init_types.keys():
            loss = networks[key].loss(x_batch, t_batch)
            print(key + ":" + str(loss))
```

    Multi-Layer Neural Network Learning:   1%|          | 13/2000 [00:00<00:31, 62.22it/s]

    ===========iteration:0===========
    std=0.01:2.3025175664186444
    Xavier:2.303862642438251
    He:2.3827282515682757


    Multi-Layer Neural Network Learning:  25%|██▌       | 507/2000 [00:08<00:30, 48.83it/s]

    ===========iteration:500===========
    std=0.01:2.3026603348809433
    Xavier:1.0321529802747018
    He:0.4276658756581122


    Multi-Layer Neural Network Learning:  50%|█████     | 1008/2000 [00:20<00:22, 43.17it/s]

    ===========iteration:1000===========
    std=0.01:2.3023436687220347
    Xavier:0.39962012083929876
    He:0.23786322544168978


    Multi-Layer Neural Network Learning:  75%|███████▌  | 1508/2000 [00:32<00:12, 38.53it/s]

    ===========iteration:1500===========
    std=0.01:2.3066479820169725
    Xavier:0.3999142528924511
    He:0.24224731352440737


    Multi-Layer Neural Network Learning: 100%|██████████| 2000/2000 [00:45<00:00, 43.51it/s]



```python
# 3. Plotting ========================

markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}
x = np.arange(max_iterations)

plt.figure(figsize=(10, 10))
for key in weight_init_types.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)
plt.xlabel("iterations")
plt.ylabel("loss")
plt.ylim(0, 2.5)
plt.legend()
plt.show()
```


    
![png](/assets/images/DLScratch_06_01/img_11.png)
    


(먼저, optimizer가 sigmoid로 통일되었다는 점을 참고해야합니다)

단순 표준정규분포 값으로 초기화를 하면 학습이 전혀 이루어지지 않는다는 사실을 확인할 수 있습니다. 

이유는 forward 연산시에 0 부근의 값으로 출력이 이뤄지기 때문에 역전파시에 기울기 갱신이 되지 않기 때문입니다.

반면 Xavier와 He 초기화 값의 경우 순조롭게 학습이 이루어짐을 확인할 수 있습니다. 

이로인해 확인할 수 있는 점은, <b>_가중치 초깃값 선정이 신경망 학습에서 매우 중요하다_</b>는 사실입니다.

3절에 대해선 내용이 길어저 다음 게시물에 이어쓰도록 하겠습니다.

> <b>_To Be Continue..._</b>

---
