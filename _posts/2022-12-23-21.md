---
layout: single
title:  "Deep Learning from Scratch_#08"
categories: 
 - Deep Learning_Basic
classes: wide
use_math: true

---

### &lt;밑바닥부터 시작하는 딥러닝&gt; 코드 및 내용을 정리해보았습니다.

 * Deep Learning from Scratch_08 

---

## 8. Deep Learning

#### _Contents_

---
#### 1. Deeper

        1.1. 더 깊은 신경망으로
        1.2. 정확도를 높이려면  
        1.3. 깊게 하는 이유     
#### 2. Early history of Deep Learning
    
        2.1. ImageNet
        2.2. VGG
        2.3. GoogLeNet
        2.4. ResNet
#### 3. Faster

        3.1. 풀어야 할 숙제
        3.2. GPU를 활용한 고속화
        3.3. 분산 학습
        3.4. 연산 정밀도와 비트 줄이기
#### 4. Application of Deep Learning
        
        4.1. Object Detection 
        4.2. Segmentation
        4.2. Image Caption
#### 5. Future of Deep Learning

        5.1. Style Transfer
        5.2. Generative Adversarial Network (GAN)
        5.3. Autonomous Driving
        5.4. Deep Q-Network (Reinforcement Learning)

#### 6. Overview


---

이제 그동안 배운 것을 바탕으로 Deep Neural Network를 설계할 수 있습니다. 하지만 단지 일반 차에 배기량이 높은 엔진을 단다고 슈퍼카가 아니듯, 깊어진 신경망으로 인해 생기는 커다란 문제가 있습니다. 이번 장에선 딥러닝의 특징 / 과제 / 미래 등을 살펴보겠습니다.


```python
from matplotlib.gridspec import GridSpec
from matplotlib.image import imread
from sympy.plotting import plot3d
from collections import OrderedDict
from mnist import load_mnist
from PIL import Image
from tqdm import tqdm

import matplotlib.pyplot as plt
import sympy as sy
import numpy as np
import os, sys
import pickle
```

### 1. Deeper

<b>_1.1. 더 깊게_</b>

이번 장에선 기존과는 달리 신경망을 깊게 쌓아서 구현해보겠습니다. 구조는 다음과 같습니다. 2번의 (Conv-ReLU) layer 이후 Pooling을 한 번씩 넣은 것이 특징입니다.


```python
im_ratio = 0.8
img_01 = Image.open('./dataset/images/fig 8-1.png')
img_01 = img_01.resize((int(img_01.size[0]*im_ratio),int(img_01.size[1]*im_ratio)))
img_01
```




    
![png](/assets/images/DLScratch_08/img_01.png)
    



구현을 위한 spec은 다음과 같습니다.

>       1. Filter Channel : 16 / 16 / 32 / 32 / 64 / 64
>       2. Every Filter : shape (3, 3), pad 1 (Conv 4 : pad 2), stride 1
>       3. Every Pooling : shape (2, 2), stride 2
>       4. Every Hidden Layer : 50
>       5. Activation Function : ReLU
>       6. Optimizer : Adam
>       7. Weight Initialization : He's initialization       
>       8. Dropout : ReLU / Affine 등 완전연결 계층 뒤
>       9. BatchNormalization : False

이를 위해 지금까지의 코드를 아래와 같이 가져오겠습니다.

해당 구현을 위해 지금까지 구현했던 관련된 코드들을 다음의 순서대로 모아보았습니다. 상당히 길지만, 한 게시물에 보는게 도움이 된다 생각하여 올립니다.

---

<b>_1. Layer 구현_</b>

        1.1.Relu class
        1.2. Affine class
        
                1.3.1. softmax function
                1.3.2. cross_entropy function
        1.3. SoftmaxWithLoss class
        1.4. Dropout class
        1.5. BatchNormalization class
                1.6.1. im2row function
                1.6.2. row2im function
        1.6. Convolution class
        1.7. Pooling class
    
<b>_2. Optimization 구현_</b>
   
        2.1. SGD class
        2.2. Momentum class
        2.3. AdaGrad class
        2.4. Adam class

<b>_3. Train 구현_</b>
        
        3.1. Trainer class

---


```python
#-----------------------------------------------------------------------
# 1.1. Relu class
########################################################################
class Relu:
    def __init__(self):
        self.mask = None            # mask는 T/F로 구성된 넘파이 배열로, 범위에 따른 T/F를 반환합니다.
    
    def forward(self, x):
        self.mask = (x <= 0)        # 0 이하면 True 반환, out value 0 반환
        out = x.copy()
        out[self.mask] = 0 

        return out   

    def backward(self, dout):       # dout 값에 곱해지는 값임을 잊지말 것
        dout[self.mask] = 0         # dout * 0
        dx = dout                   # dout * 1
        
        return dx
#-----------------------------------------------------------------------
# 1.2. Affine class
########################################################################
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.original_x_shape = None
        self.dw = None
        self.db = None

    def forward(self, x):
        ###########################################################         
        # tensor 대응
        self.original_x_shape = x.shape # tensor 차원을 튜플형식으로 반환
        x = x.reshape(x.shape[0],-1)    # data input에 맞춰 array 변환
        ###########################################################
        self.x = x

        out = np.dot(self.x, self.W) + self.b
        
        return out

    def backward(self, dout):
        # Batch data shape을 맞추기위한 transpose T
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis = 0)

        dx = dx.reshape(*self.original_x_shape) 
        # 여기서 star(*)는 컨테이너 타입의 데이터를 unpacking하기 위한 목적이다. 텐서의 경우 가변인자가 되므로 사용한 것으로 추측.
        return dx
#-----------------------------------------------------------------------
# 1.3.1. softmax functioin
########################################################################
def softmax(x):
    # batch 데이터의 경우 데이터의 갯수와 flatten 된 입력값을 포함하면 (100, 784)와 같은 2차원이되므로 차원을 필수로 고려해야한다.
    # 784 내에서 최대값만 빼야하는데 np.max의 경우 스칼라 최댓값만 고려하므로 axis를 지정해야하고,
    # batch 개수만큼 최댓값을 빼줘야하는데 이를 위해선 transpose가 필요하다. 이를 위해 불필요해보이는 두 번의 transpose가 수행되는 것
    if x.ndim == 2:                 # 기존 X : (N, x) 가정
        x = x.T                     # new X : (x, N)
        x = x - np.max(x, axis=0)   # np.max(x, axis=0) : (1, N)
                                    # (x, N) - (1, N) //차 연산 수행가능, (x', N) 반환
        y = np.exp(x) / np.sum(np.exp(x), axis=0)   # 동일하게 적용, (x'', N) 반환
        return y.T                  # (N, x'') 반환

    x = x - np.max(x)   # exp(x + c')을 통해 resizing을 하여 overflow 문제 해결.
                        # 이때 c'값을 - max(x)를 취하여 지수함수의 발산문제 해결
    return np.exp(x) / np.sum(np.exp(x))
#-----------------------------------------------------------------------
# 1.3.2. cross entropy function
########################################################################
def cross_entropy(y, t):
    # 훈련데이터 : one-hot / 정답데이터 : only label 인 경우를 가정한 CEE 계산 코드
    if y.ndim == 1:                  #true label을 기준으로 차원 변환
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    
    if t.size == y.size:            # 훈련데이터 : one-hot / 정답데이터 : one-hot인 경우 필요한 변형, 정답인덱스만 추출.
        t = t.argmax(axis=1)

    delta = 1e-7                    # t = 1일 때 CEE 값인 log(1/y + delta)를 통해 overflow를 방지.
    batch_size = y.shape[0]
    cross_entropy_error = -np.sum(np.log(y[np.arange(batch_size), t]+ delta)) / batch_size 
    # y[np.arange(batch_size), t] 정답 인덱스에 해당하는 y softmax 값을 추출하기 위한 코드. 역시 1과 유사할 수록 cross entropy error가 작아지는 구조다.
    
    return cross_entropy_error
#-----------------------------------------------------------------------
# 1.3. SoftmaxWithLoss class
########################################################################
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy(self.y, self.t)

        return self.loss
    
    def backward(self, dout = 1):
        batch_size = self.t.shape[0]

        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때
            dx = (self.y - self.t) / batch_size
        else:                          # 정답 레이블이 단순 레이블인 경우
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            dx = dx / batch_size
                
        return dx
#-----------------------------------------------------------------------
# 1.4. Dropout class
########################################################################
class Dropout:

    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None

    def forward(self, x, train_flg=True):
        if train_flg:       # 훈련 시
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask
        else:               # 추론 시
            return x * (1 - self.dropout_ratio)

    def backward(self, dout):
        return dout * self.mask
#-----------------------------------------------------------------------
# 1.5. BatchNormalization class
########################################################################
class BatchNormalization:
    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):
        # keyword arguments
        self.gamma = gamma
        self.beta = beta
        self.momentum = momentum
        self.input_shape = None
        self.running_mean = running_mean
        self.running_var = running_var
        self.batch_size = None
        # forward kargs
        self.xmu = None         # deviation of x
        self.sqrtvar = None     # standard deviation
        # backward kargs
        self.dgamma = None
        self.dbeta = None
    
    # train flag : 학습 여부에 따라 이동평균/분산 업데이트를 할지 결정해야한다. 
    # 학습 시엔 업데이트, 추론 시엔 누적된 이동평균/분산을 사용한다. 
    def forward(self, x, train_flag=True):
        self.input_shape = x.shape
        if x.ndim != 2:
            N, C, H, W = x.shape    # 4차원인 batch img feature map
            x = x.reshape(N, -1)     

        out = self.__forward(x, train_flag)

        return out.reshape(*self.input_shape)
        # 여기서 star(*)는 컨테이너 타입의 데이터를 unpacking하기 위한 목적이다. 텐서의 경우 가변인자가 되므로 사용한 것으로 추측.

    def __forward(self, x, train_flag):
        if self.running_mean is None:
            N, D = x.shape
            self.running_mean = np.zeros(D)
            self.running_var = np.zeros(D)

        if train_flag:  # 학습 시
            mu = x.mean(axis=0) # column
            xmu = x - mu
            var = np.mean(xmu**2, axis=0)
            sqrtvar = np.sqrt(var + 10e-7)
            xhat = xmu / sqrtvar

            # forward kargs
            self.batch_size = x.shape[0]
            self.xmu = xmu
            self.xhat = xhat
            self.sqrtvar = sqrtvar
            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu            
            self.running_var = self.momentum * self.running_var + (1-self.momentum) * mu
        else:           # 추론 시
            xmu = x - self.running_mean
            xhat = xmu / ((np.sqrt(self.running_var + 10e-7)))

        out = self.gamma * xhat + self.beta
        return out

    def backward(self, dout):
        if dout.ndim != 2:
            N, C, H, W = dout.shape
            dout = dout.reshape(N, -1)

        dx = self.__backward(dout)
        dx = dx.reshape(*self.input_shape)

        return dx

    def __backward(self, dout):
        dbeta = dout.sum(axis=0)
        dgamma = np.sum(self.xhat * dout, axis=0)
        dxhat = self.gamma * dout
        dxmu = dxhat / self.sqrtvar
        divar = np.sum(dxhat * self.xmu, axis=0)
        dsqrtvar = (- divar) / (self.sqrtvar * self.sqrtvar)
        dvar = 0.5 *  dsqrtvar / self.sqrtvar
        dxmu = dxmu + (2 / self.batch_size) * self.xmu * dvar
        dmu = np.sum(dxmu, axis=0)
        dx = dxmu - dxhat / self.batch_size

        self.dgamma = dgamma
        self.dbeta = dbeta

        return dx
#-----------------------------------------------------------------------
# 1.6.1. im2row function
########################################################################
def im2row(input_data, filter_h, filter_w, stride=1, pad=0):
    N, C, H, W = input_data.shape
    out_h = (H + 2*pad - filter_h)//stride + 1  # 나눠지지 않는 케이스 고려
    out_w = (W + 2*pad - filter_w)//stride + 1  # 나눠지지 않는 케이스 고려

    # 4차원 array의 H, W 요소마다 왼/오른쪽 pad 삽입
    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')
    # 6차원 array 생성
    row = np.zeros((N, C, filter_h, filter_w, out_h, out_w)) 
    # x, y : 필터의 각 원소
    # y - y_max/x - x_max는 필터의 원소 (x, y)가 img를 stride 단위로 이동하며 가는 인덱스(발자취)
    for y in range(filter_h):
        y_max = y + stride*out_h
        for x in range(filter_w):
            x_max = x + stride*out_w
            # 각 filter 원소 y, x에 대해 img 상에서 연산 진행될 좌표 인덱스 상의 값을 row에 반환
            row[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]

    # H / W 는 filter 연산이 진행되는 위치상의 이미지 값을 의미합니다.
    # (N, C, H, W, OH, OW) -> (N, OH, OW, FN, H, W)로 순서 변환
    # 2차원 데이터의 row size : N * out_H * out_W
    # FN, H, W를 한 줄에 담아서 reshape
    row = row.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)
    return row
#-----------------------------------------------------------------------
# 1.6.2. row2im function
########################################################################
def row2im(row, input_shape, filter_h, filter_w, stride=1, pad=0):
    N, C, H, W = input_shape # shpape of input feature map
    out_h = (H + 2*pad - filter_h)//stride + 1
    out_w = (W + 2*pad - filter_w)//stride + 1
    # row는 dout과 filter의 행렬곱 형태입니다.
    # row.shape : (N, FN, out_H, out_W)
    row = row.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)
    # transpose 후 row.shape : (N, C, FH, FW, OH, OW)

    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))
    # img : (N, C, OH, OW)
    for y in range(filter_h):
        y_max = y + stride*out_h
        for x in range(filter_w):
            x_max = x + stride*out_w
            img[:, :, y:y_max:stride, x:x_max:stride] += row[:, :, y, x, :, :]
            # 비어있는 이미지 정보인덱스에 conv 연산이 진행된 output feature map 값을 더해줍니다.
    return img[:, :, pad:H + pad, pad:W + pad] # 패딩 사이즈는 제외하고 출력
#-----------------------------------------------------------------------
# 1.6. Convolution class
########################################################################
class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad

        self.x = None
        self.row = None
        self.col_W = None

        self.db = None
        self.dW = None

    # filter가 가중치 W 역할을 한다
    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = int((H + 2*self.pad - FH)/self.stride) + 1
        out_w = int((W + 2*self.pad - FW)/self.stride) + 1

        # row.shape : (N*OH*OW, -1)
        # col_W.shape : (-1, FN) 
        # F_size = OH * OW
        row = im2row(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T
        out = np.dot(row, col_W) + self.b # (N*OH*OW, FH*FW) 이로 인해 output reshape 필요
        # out.shape : (N, OH, OW, FN) -> (N, FN, OH, OW)
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)

        self.x = x
        self.row = row
        self.col_W = col_W

        return out

    def backward(self, dout):
        FN, C, FH, FW = self.W.shape
        
        # 1. reshape / transpose 역연산 =======================
        # 1-1. transpose : (N, H, W, FN) -> (N, FN, OH, OW) 
        # transpose는 (0, 1, 2, 3)에 해당하는 인자가 (3, 1, 2, 0) 등으로 이동하도록 하는 함수(헷갈릴 여지가 많다)
        # 1-2. reshape : (N*OH*OW, FN)
        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)
        #-----------------------------------------------------
        
        # 2. + 연산 ===========================================
        self.db = np.sum(dout, axis=0)
        #-----------------------------------------------------
        
        # 3. X 연산 ===========================================
        # 3.1. self.col_W(filter) 방향 backward
        self.dW = np.dot(self.row.T, dout)
        # 3.2. self.row(input feature map) 방향 backward 
        drow = np.dot(dout, self.col_W.T)
        #-----------------------------------------------------
        
        # 4. img2row / fil2col 연산 ===========================
        # 4.1. filter -> row 역연산
        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)
        # 4.2. img -> row 역연산 
        dx = row2im(drow, self.x.shape, FH, FW, self.stride, self.pad)
        #-----------------------------------------------------
        return dx
#-----------------------------------------------------------------------
# 1.7. Pooling class
########################################################################
class Pooling:
    def __init__(self, pool_h, pool_w, stride=1, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
        
        self.x = None
        self.arg_max = None

    def forward(self, x):
        N, C, H, W = x.shape    # conv 연산 이후이므로 x shape : (N, FN, OH, OW)
        p_out_h = int((H - self.pool_h) / self.stride + 1)
        p_out_w = int((H - self.pool_w) / self.stride + 1)

        row = im2row(x, self.pool_h, self.pool_w, self.stride, self.pad)
        # row 값은 기존 (C, H, W) 값의 나열이 (PH, PW) 가 나열되는 형태로 reshape된다
        # 즉 pooling window 사이즈에 맞춰 Channel이 보존된 image 정보 형태로 나열된다
        row = row.reshape(-1, self.pool_h*self.pool_w)
        arg_max = np.argmax(row, axis=1) # 각 row의 max 값 위치 인덱스 저장
        p_out = np.max(row, axis=1) # 각 row에 대해 max pooling이 이루어진다
        # (N*P_OH*P_OW*C, 1)에 대해서 reshape이 이루어진다 (그냥 쓰면 안 될지 궁금하다)
        p_out = p_out.reshape(N, p_out_h, p_out_w, C).transpose(0, 3, 1, 2)

        self.x = x
        self.arg_max = arg_max
        # out shape : (N, FN, P_OH, P_OW)
        return p_out

    def backward(self, dout):
        # 1. reshape / transpose 역연산 ======================
        # 1.1. transpose
        dout = dout.transpose(0, 2, 3, 1)
        # 1.2. reshape은 flatten되어 있으므로 shape 함수로 대체
        #---------------------------------------------------

        # 2. pooling 역연산 ==================================
        pool_size = self.pool_h * self.pool_w # pooling window 
        dmax = np.zeros((dout.size, pool_size)) # (N*OH*OW*C, PH*PW) 복원
        # dmax에 각 max pooling 된 위치 인덱스를 저장. 아닌 곳은 zero
        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()
        #---------------------------------------------------
        
        # 3. row2im 역연산 ===================================
        # dmax : (N, OH, OW, C, FH*FW)으로 reshape
        dmax = dmax.reshape(dout.shape + (pool_size,))
        # drow : dmax를 통해 (N*OH*OW,C*FH*FW) 형태로 변환, row2im 출력 shape에 해당한다.
        drow = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2],  -1)
        dx = row2im(drow, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)
        #---------------------------------------------------

        return dx
#-----------------------------------------------------------------------
# 2.1. SGD class
########################################################################
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr

    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
#-----------------------------------------------------------------------
# 2.2. Momentum class
########################################################################
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momemtum = momentum
        self.v = None

    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)    # 이동벡터를 W / b 의 shape만큼 생성
        
        for key in params.keys():
            self.v[key] = self.momemtum*self.v[key] - self.lr*grads[key]
            params[key] += self.v[key]
#-----------------------------------------------------------------------
# 2.3. AdaGrad class
########################################################################
class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
        
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)
#-----------------------------------------------------------------------
# 2.4. Adam class
########################################################################
class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.m = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1
        lr_t = self.lr * np.sqrt(1 - self.beta2**self.iter) / (1 - self.beta1**self.iter)

        for key in params.keys():
            self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
#-----------------------------------------------------------------------
# 3.1. Trainer class
########################################################################
class Trainer:
    def __init__(self, network, x_train, t_train, x_test, t_test, epochs=20, mini_batch_size=100, optimizer='SGD', optimizer_param={'lr':0.01}, evaluate_sample_num_per_epoch=None, verbose=True):
        self.network = network
        self.verbose = verbose
        self.x_train = x_train
        self.t_train = t_train
        self.x_test = x_test
        self.t_test = t_test
        self.epochs = epochs
        self.batch_size = mini_batch_size
        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch

        # optimizer class instance화
        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'adagrad':AdaGrad, 
        'adam':Adam}
        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)
        
        self.train_size = x_train.shape[0]
        # batch size로 나눠지지 않으면 훈련이 돌아가지 않게 설계
        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1) 
        self.max_iter = int(epochs * self.iter_per_epoch)
        self.current_iter = 0
        self.curren_epoch = 0

        self.train_loss_list = []
        self.train_acc_list = []
        self.test_acc_list = []

    def train_step(self):
        # data 분할
        batch_mask = np.random.choice(self.train_size, self.batch_size)
        x_batch = self.x_train[batch_mask]
        t_batch = self.t_train[batch_mask]
        # 각 가중치의 gradient 저장 / optimizer에 따른 업데이트 진행
        grads = self.network.gradient(x_batch, t_batch)
        self.optimizer.update(self.network.params, grads)
        # cross entropy loss 출력
        loss = self.network.loss(x_batch, t_batch)
        self.train_loss_list.append(loss)
        if self.verbose: print('train loss : ' + str(loss))
        # epoch 단위로 데이터 샘플 정확도 계산
        if self.current_iter % self.iter_per_epoch == 0:
            self.curren_epoch += 1
            
            x_train_sample, t_train_sample = self.x_train, self.t_train
            x_test_sample, t_test_sample = self.x_test, self.t_test
            # epoch당 샘플 수 제한하여 평가
            if not self.evaluate_sample_num_per_epoch is None:
                t = self.evaluate_sample_num_per_epoch
                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]
                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]

            train_acc = self.network.accuracy(x_train_sample, t_train_sample)
            test_acc = self.network.accuracy(x_test_sample, t_test_sample)
            self.train_acc_list.append(train_acc)
            self.test_acc_list.append(test_acc)

            if self.verbose:
                print("=== epoch : "+str(self.curren_epoch)+", train acc : "+str(train_acc)+", test acc : "+str(test_acc)+" ===")
        self.current_iter += 1

    def train(self):
        for i in tqdm(range(self.max_iter), desc='DeepConvNet Learning'):   
            self.train_step()

        test_acc = self.network.accuracy(self.x_test, self.t_test)

        if self.verbose:
            print("========== Final Test Accuracy ===========")
            print("test acc : "+str(test_acc))
```

위의 긴 코드들을 활용하여 딥러닝 모델을 구현해보겠습니다. 


```python
class DeepConvNet:
    def __init__(self, input_dim=(1, 28, 28), conv_param_1={'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1}, conv_param_2={'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1}, 
                conv_param_3={'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1}, conv_param_4={'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1}, 
                conv_param_5={'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1}, conv_param_6={'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1}, 
                hidden_size=50, output_size=10):
        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])
        weight_init_scales = np.sqrt(2 / pre_node_nums) # He's init
        
        # weight initialize
        #------------------------------------------------------------------------------------------------------------------------------------------------
        self.params = {}
        pre_channel_num = input_dim[0]
        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):
            # 각 filter의 (FN, C, FH, FW)에 weight 갯수에 맞춘 scale값으로 초기화
            self.params['W'+str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])
            self.params['b'+str(idx+1)] = np.zeros(conv_param['filter_num']) # FN
            pre_channel_num = conv_param['filter_num']  # Conv 연산에서 FN은 output feature map의 C 위치로 가게 된다. 
        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)
        self.params['b7'] = np.zeros(hidden_size)
        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)
        self.params['b8'] = np.zeros(output_size)
        #------------------------------------------------------------------------------------------------------------------------------------------------
        
        # Layers
        #------------------------------------------------------------------------------------------------------------------------------------------------
        self.layers = []    # 기존 OrderedDict를 쓴 것과 달리 달리 단순 list를 활용. 즉 key 없이 클래스 객체를 차곡차곡 쌓았습니다.
        self.layers.append(Convolution(self.params['W1'], self.params['b1'], conv_param_1['stride'], conv_param_1['pad'])) # (W, b, stride, pad)
        self.layers.append(Relu())
        self.layers.append(Convolution(self.params['W2'], self.params['b2'], conv_param_2['stride'], conv_param_2['pad']))
        self.layers.append(Relu())
        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))
        self.layers.append(Convolution(self.params['W3'], self.params['b3'], conv_param_3['stride'], conv_param_3['pad']))
        self.layers.append(Relu())
        self.layers.append(Convolution(self.params['W4'], self.params['b4'], conv_param_4['stride'], conv_param_4['pad']))
        self.layers.append(Relu())
        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))
        self.layers.append(Convolution(self.params['W5'], self.params['b5'], conv_param_5['stride'], conv_param_5['pad']))
        self.layers.append(Relu())
        self.layers.append(Convolution(self.params['W6'], self.params['b6'], conv_param_6['stride'], conv_param_6['pad']))
        self.layers.append(Relu())
        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))
        self.layers.append(Affine(self.params['W7'], self.params['b7']))
        self.layers.append(Relu())
        self.layers.append(Dropout(0.5)) # Dropout Ratio
        self.layers.append(Affine(self.params['W8'], self.params['b8']))
        self.layers.append(Dropout(0.5))
        self.last_layer = SoftmaxWithLoss()
        #------------------------------------------------------------------------------------------------------------------------------------------------

    def predict(self, x, train_flg=False):
        # dropout의 경우 훈련시에는 노드 갯수를 줄이고, 추정시에는 weight 보정을 해줘야하므로 훈련 여부를 구분지어야 합니다. 이를 위한 조건문
        for layer in self.layers:
            if isinstance(layer, Dropout):
                x = layer.forward(x, train_flg)
            else:
                x = layer.forward(x)

        return x
    
    def loss(self, x, t):
        y = self.predict(x, train_flg=True) # Eross Entropy Loss 값까지 구하는건 훈련시에만 해당됩니다
        return self.last_layer.forward(y, t)
    
    def accuracy(self, x, t, batch_size=100):
        if t.ndim != 1 : t = np.argmax(t, axis=1) # one-hot인 t에 대해서 row idx를 return 하는 조건문
        acc = 0
        
        for i in range(int(x.shape[0] / batch_size)):   # train_num / batch_num
            tx = x[i*batch_size:(i+1)*batch_size]       # i - i+1 번째 batch에 대한 x 값들
            tt = t[i*batch_size:(i+1)*batch_size]       # i - i+1 번째 batch에 대한 t 값들
            y = self.predict(tx, train_flg=False)       # i - i+1 번째 batch에 대한 y 값들, 추정단계에 해당하므로 train_flg를 False로 둡니다
            y = np.argmax(y, axis=1)                    # row 기준 softmax idx 출력
            acc += np.sum(y == tt)                      # 같은 label인 경우만 합산
                                                        # 전체 집단에 대해 100개 단위로 참 집단 합계
        return acc / x.shape[0]                         # 전체 참에 대해 전체 값으로 나눔. 즉 정확도 계산


    def gradient(self, x, t):
        # forward / Cross Entropy Error Value 출력
        self.loss(x, t)
        
        # backward
        dout = 1
        dout = self.last_layer.backward(dout)
        
        tmp_layers = self.layers.copy() # 각 layer를 역순으로 뒤집어 모든 가중치를 list에 저장 
        tmp_layers.reverse()
        for layer in tmp_layers: # layer는 tmp_layers별 전체 가중치를 의미
            dout = layer.backward(dout)
        # 각 dout 값이 layer backward 함수에 전달되어 backpropagation이 차례차례 진행되며 . 이후 Class 내부 인자로 저장된 dW, db 등이 grads에 저장
        # 해당 값이 곧 손실함수에 미치는 영향력이기에 이를 줄이기 위한 학습이 optimization에 의해 이루어지게 됩니다.
        grads = {}
        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)): # Conv / Affine 연산만 weight가 존재하므로 그에 대해 개별 지정
            grads['W'+str(i+1)] = self.layers[layer_idx].dW
            grads['b'+str(i+1)] = self.layers[layer_idx].db

        return grads

    # Save & Load Weights
    #------------------------------------------------------------------------------------------------------------------------------------------------
    def save_params(self, file_name="DeepConvNet_params.pkl"):
        params = {}
        for key, val in self.params.items():
            params[key] = val
        with open(file_name, 'wb') as f:
            pickle.dump(params, f)

    def load_params(self, file_name="DeepConvNet_params.pkl"):
        with open(file_name, 'rb') as f:
            params = pickle.load(f)
        for key, val in params.items():
            self.params[key] = val

        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):
            self.layers[layer_idx].W = self.params['W' + str(i+1)]
            self.layers[layer_idx].b = self.params['b' + str(i+1)]
    #------------------------------------------------------------------------------------------------------------------------------------------------
```

이렇게 DeepConvNet 설계가 끝났습니다. 이제 훈련을 하고 가중치를 저장하도록 하겠습니다. 


```python
(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)

network = DeepConvNet()  
trainer = Trainer(network, x_train, t_train, x_test, t_test, epochs=20, mini_batch_size=100, optimizer='Adam', optimizer_param={'lr':0.001}, evaluate_sample_num_per_epoch=1000, verbose=False)
trainer.train()

# 매개변수 보관
network.save_params("DeepConvNet_params.pkl")
print("Saved Network Parameters!")
```

    DeepConvNet Learning: 100%|██████████| 12000/12000 [5:32:49<00:00,  1.66s/it]  


    Saved Network Parameters!



```python
plt.figure(figsize=(8, 6))
plt.plot(trainer.train_acc_list,label='Train_acc : '+str(trainer.train_acc_list[-1]))
plt.plot(trainer.test_acc_list,label='Test_acc : '+str(trainer.test_acc_list[-1]))
plt.title('DeepConvNet with MNIST')
plt.xlabel('epochs'), plt.ylabel('accuracy')
plt.xlim(0,20), plt.ylim(0,1)
plt.gca().set_xticklabels(['{:0.0f}'.format(x) for x in plt.gca().get_xticks()])
plt.gca().set_yticklabels(['{:0.0%}'.format(x) for x in plt.gca().get_yticks()])
plt.legend(loc='lower right', fontsize=15)
plt.show()
```

    /var/folders/b9/mp1fh7ds7wz6829fc1klmjqr0000gn/T/ipykernel_4354/903108085.py:7: UserWarning: FixedFormatter should only be used together with FixedLocator
      plt.gca().set_xticklabels(['{:0.0f}'.format(x) for x in plt.gca().get_xticks()])
    /var/folders/b9/mp1fh7ds7wz6829fc1klmjqr0000gn/T/ipykernel_4354/903108085.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator
      plt.gca().set_yticklabels(['{:0.0%}'.format(x) for x in plt.gca().get_yticks()])



    
![png](/assets/images/DLScratch_08/img_02.png)
    


7장의 Test_acc(98.5%)에 대해 추가로 1퍼센트 가량 성능이 증가했음을 확인할 수 있었습니다.

<b>_1.2. 정확도를 높이려면_</b>

저자는 16년 말 시점의 MNIST 인식 대회에서 SOTA(State of the Art) 기법들에 대한 내용을 언급하고 있습니다. 

>       Ensemble Learning
>       Learning Rate Decay
>       Data Augmentation

등의 기법이 MNIST dataset에 대해 추가적인 정확도 향상 효과가 있다고 언급하였습니다. 이 중 data augmentation은 rotate / shift / crop / flip 등의 기법이 있습니다.


```python
im_ratio = 0.8
img_02 = Image.open('./dataset/images/fig 8-4.png')
img_02 = img_02.resize((int(img_02.size[0]*im_ratio),int(img_02.size[1]*im_ratio)))
img_02
```




    
![png](/assets/images/DLScratch_08/img_03.png)
    



해당 방식으로 이미지를 강제로 늘리는 방법입니다. 해당 방법은 연구분야가 따로 있을 정도로 파고들면 내용이 방대하므로 추후 게시물에서 따로 다루겠습니다.

<b>_1.3. 깊게 하는 이유_</b>

Layer를 깊게 쌓는 이유는,

> <b>_층을 깊게 쌓으면 신경망의 매개변수를 줄이면서도 얕은 신경망 대비 같거나 더 높은 수준의 표현을 할 수 있다는 장점이 있기 때문입니다_</b>

다음의 이미지로 확인해보겠습니다.


```python
img_03 = imread('./dataset/images/fig 8-6.png')
img_04 = imread('./dataset/images/fig 8-5.png')

fig = plt.figure(figsize = (18,9),frameon=False)

gs = GridSpec(nrows=2,ncols=2,width_ratios=[1,1],height_ratios=[1,1])
ax_0 = fig.add_subplot(gs[0,:])
ax_0.set_xticks([]), ax_0.set_yticks([]), ax_0.set_axis_off()
ax_1 = fig.add_subplot(gs[1,:])
ax_1.set_xticks([]), ax_1.set_yticks([]), ax_1.set_axis_off()
ax_0.imshow(img_03)
ax_1.imshow(img_04)
```




    <matplotlib.image.AxesImage at 0x12d9bb6a0>




    
![png](/assets/images/DLScratch_08/img_04.png)
    


output feature map의 한 픽셀이 어떻게 계산되는지 생각해보면 다음과 같습니다. 

* 3-layer conv operation : (3X3) 2개의 filter 이용, 총 18개($2\cdot3^2$)의 매개변수 필요
* 1-layer conv operation : (5X5) filter 이용, 총 25개($5^2$)의 매개변수 필요

당장 층 하나를 더 추가하면 차이는 극명해집니다. 3-layer 영역을 예로 들겠습니다.

* 3-layer conv operation : (3X3) 3개의 filter 이용, 총 27개($3^3$)의 매개변수 필요
* 1-layer conv operation : (7X7) filter가 필요, 총 49개($7^2$)의 매개변수 필요

이와 같이 small filter를 deep network에 사용하면

*   <b>_Large receptive field_</b>를 big filter를 쓰지 않고 <b>적은 매개변수로 소화</b>할 수 있습니다. 이로써 <b>학습시간 단축</b>이라는 큰 장점을 갖게됩니다. 
*   ReLU 등의 activation function을 신경망 사이사이 끼워 넣음으로써 <b>네트워크의 비선형성을 추가</b>할 수 있게되고, AlexNet 사례에서 보듯 </b>표현력을 개선</b>할 수 있습니다. <b>복잡한 이미지도 인식</b>할 수 있게 되는 것입니다. 
*   각 계층 별로 이미지 요소를 분해할 수 있기 때문에 <b>복잡한 문제도 단계적으로 접근</b>할 수 있게 됩니다. 각 계층은 A라는 최종 이미지를 한 번에 인식할 필요 없이, A 이미지의 하위 a 요소만을 학습하는데 전념할 수 있게 되는 것입니다. 

이처럼 딥러닝은 복잡한 데이터를 해석하는데 상당한 장점을 갖추고 있습니다.

### 2. 딥러닝의 초기 역사

<b>_2.1. ImageNet_</b>

ImageNet은 1000개의 label을 갖고있는 100만 장 가량의 이미지 데이터 셋입니다. 이 중  ImageNet Classification Challenge(ILSVRC) 분야에서 대부분의 클래식 딥러닝 아키텍쳐들이 탄생하였을 정도로 인정받는 데이터 셋입니다. 정답 후보군의 오답률을 나타내는 top-5 error의 2010 초중반 시기를 나타내면 다음과 같습니다.


```python
im_ratio = 0.8
img_05 = Image.open('./dataset/images/fig 8-8.png')
img_05 = img_05.resize((int(img_05.size[0]*im_ratio),int(img_05.size[1]*im_ratio)))
img_05
```




    
![png](/assets/images/DLScratch_08/img_05.png)
    



여기서 2015년의 <b>_ResNet_</b>을 기점(top-5 error : 3.5%)으로 인간의 인식률을 이기게 되고 해당 특이점을 기준으로 딥러닝은 폭발적으로 연구, 발전이 이루어지게 됩니다. 위 그래프 중 최근 기준으로 세 가지 architecture를 간략히 다뤄보겠습니다.

<b>_2.2. VGG Net_</b>

LeNet이 첫 CNN 구조였고 AlexNet이 첫 딥러닝 모델이었다면, <b>VGG net</b>은 다양한 딥러닝 모델들의 <b>기준이 되었던</b> 딥러닝 모델입니다. 구조는 다음과 같습니다.


```python
im_ratio = 0.9
img_06 = Image.open('./dataset/images/fig 8-9.png')
img_06 = img_06.resize((int(img_06.size[0]*im_ratio),int(img_06.size[1]*im_ratio)))
img_06
```




    
![png](/assets/images/DLScratch_08/img_06.png)
    



ReLU와 같은 활성화함수 계층을 제외하고 총 16(또는 19) 계층으로 나타낸 모델로 (3 X 3)의 small filter를 연속적으로 활용하여 feature map을 천천히, 깊게 추출한 모델이라는 점이 특징입니다. 이후 중복된 conv layer 모듈마다 pooling 계층을 두어 feature map 크기를 절반으로 줄이길 반복하다 추후 fully connected layer를 통과시켜 결과를 출력합니다.

<b>_2.3. GoogLe Net_</b>

<b>GoogLeNet</b>은 계층 수도 매우 깊어지지만, <b>_Inception_</b> 구조를 처음으로 제안한 모델이란 점에서 의미가 있습니다. 구조는 다음과 같습니다.

> <span style='color:#808080'>_Szegedy, Christian et al. “Going deeper with convolutions.” 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014): 1-9._</span>


```python
im_ratio = 1
img_07 = Image.open('./dataset/DLfromScratch_08_06.png')
img_07 = img_07.resize((int(img_07.size[0]*im_ratio),int(img_07.size[1]*im_ratio)))
img_07
```




    
![png](/assets/images/DLScratch_08/img_07.png)
    



구조의 깊이 외에도 폭의 개념이 생겼다는 점이 특징입니다. 이 폭이 Inception 구조이며 다음과 같이 구성됩니다.


```python
im_ratio = 0.8
img_08 = Image.open('./dataset/images/fig 8-11.png')
img_08 = img_08.resize((int(img_08.size[0]*im_ratio),int(img_08.size[1]*im_ratio)))
img_08
```




    
![png](/assets/images/DLScratch_08/img_08.png)
    



다음과 같이 크기가 다른 filter & pooling을 한꺼번에 적용하여 결합하는 것이 inception 구조의 역할입니다. GoogLeNet의 또다른 특징으론 (1 X 1) filter를 사용한다는 점인데요, conv layer에 해당 filter를 사용하게 되면 계산량을 줄이고 깊은 네트워크를 가능케 합니다. 깊은 네트워크의 장점을 얻기 위한 일종의 트릭으로 볼 수 있습니다. @ 를 input feature map과 filter kernel의 convolution operation 이후 output feature map의 shape을 구하는 연산으로 칭하고 단순 예시를 들어보겠습니다. 

1 

        (N, 480, 14, 14) @ (48, 480, 5, 5) = (N, 48, 14, 14) (stride:2, zero-padding:2)
        필요 매개변수 : (480 * 5 * 5) * (48 * 14 * 14) = 112,896,000

2

        2.1. (N, 480, 14, 14) @ (16, 480, 1, 1) = (N, 16, 14, 14) (stride:1, zero-padding:0)
             필요 매개변수_1 : (480 * 1 * 1) * (16 * 14 * 14) = 1,505,280
        
        2.2. (N, 480, 14, 14) @ (48, 16, 5, 5) = (N, 48, 14, 14) (stride:2, zero-padding:2)
             필요 매개변수_2 : (16 * 5 * 5) * (48 * 14 * 14) = 3,763,200
        
        총 필요 매개변수 : 1,505,280 + 3,763,200 = 5,268,480

위와 같이 대략 20배 가량 차이가 나게 됩니다. 

<b>_2.4. ResNet_</b>

<b>ResNet</b>은 층을 더욱 깊게 하기 위해서 <b>_Skip Connection_</b> 구조를 도입한 것이 특징입니다. 구조는 다음과 같습니다. 

> <span style='color:#808080'>_He, Kaiming et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015): 770-778._</span>


```python
im_ratio = 1
img_09 = Image.open('./dataset/DLfromScratch_08_01.png')
img_09 = img_09.resize((int(img_09.size[0]*im_ratio),int(img_09.size[1]*im_ratio)))
img_09
```




    
![png](/assets/images/DLScratch_08/img_09.png)
    



해당 시기는 딥러닝 아키텍처의 layer 수가 비약적으로 증가하던 시기였고, <b>AlexNet</b>의 8-layer 보다 10배 이상 깊은 100-layer 이상의 architecture들이 연구되어 발표되었습니다. 하지만 어느 순간 깊이와 성능 간의 연결고리가 끊어지는 특이점이 왔는데, 이를 극복하고 더 깊은 연결을 가능케하며 성능도 그에 비례하여 증가시킬 수 있게 했던 구조가 언급드린 <b>_skip connection_</b>입니다. 구조는 다음과 같습니다.


```python
im_ratio = 0.8
img_10 = Image.open('./dataset/images/fig 8-1.png')
img_10 = img_10.resize((int(img_10.size[0]*im_ratio),int(img_10.size[1]*im_ratio)))
img_10
```




    
![png](/assets/images/DLScratch_08/img_10.png)
    



신경망이 깊어질 수록 많아진 layer 수 만큼 표현력이 증가하는 장점이 있지만, 그만큼 전달되는 gradient 값은 소실됩니다. 이는 output layer에서 멀어질 수록 그 수준이 커지게 되고 각종 weight의 영향력을 낮추어 모델 학습이 이루어지지 않도록 합니다. 이러한 <b>_Vanishing Gradient Problem_</b>이라고 합니다. 대부분 activation function의 미분과 관련이 있고, 이를 극복하기 위해 ReLU를 도입하였지만 upper 100 layer의 신경망에선 여전히 같은 문제가 대두됩니다. 

이를 해결하기 위해서 제안된 방법이 <b>forward / backward 연산을 건너뛰는 것</b>입니다. 이것이 skip connectioin의 핵심입니다. 소실될 여지가 있는 가중치를 건너뛴 layer의 출력 값에 더해줌으로써 vanishing gradient 문제를 극복하고 더욱 깊은 신경망을 구성할 수 있다는 것을 ResNet이 증명하였습니다. 해당 신경망의 구조는 150층 이상에서도 깊이와 성능의 비례함을 입증하였고, ImageNet 분류에서 사람의 인지력을 뛰어넘는 특이점을 보여주었습니다. 

이와같이 많은 데이터 셋과 심층 신경망으로 학습된 weight는 높은 가치를 지니는데요, He / Xavier로 weight initialization을 하는 것이 아닌, 다음과 같은 일반 이미지들로 훈련된 weight를 이용하여 신경망을 초기화를 하고 다른 도메인(데이터 셋이 적어 과적합 여지가 많은)에서 재학습(<b>fine tuning</b>)해도 좋은 성능을 발휘하는 것으로 알려져 있습니다. 이를  <b>_Transfer Learning_</b>이라고 합니다. <span style="color:#808080">_(물론 가중치 훈련에 쓰인 신경망과 다른 신경망에 적용하려면 여러 응용 과정이 필요합니다)_</span>

### 3. Faster

병렬처리의 발전이 딥러닝의 보편화를 이끌었다고 해도 과언이 아닌데요, 이번 절에선 관련 내용들을 살펴보겠습니다. 

<b>_3.1. 풀어야 할 숙제_</b>

딥러닝에서 어떤 과정이 처리 시간이 오래걸리는지 확인해보겠습니다. 아래의 그림은 AlexNet에서 걸리는 layer별 처리 시간입니다.

> <span style="color:#808080">Jia, A., & Yangqing, A. (2014). _Learning Semantic Image Representations at a Large Scale._ 2014 EECS Department University of California, Berkeley Technical Report No. UCB/EECS-2014-93.</span>


```python
im_ratio = .6
img_11 = Image.open('./dataset/DLfromScratch_08_02.png')
img_11 = img_11.resize((int(img_11.size[0]*im_ratio),int(img_11.size[1]*im_ratio)))
img_11
```




    
![png](/assets/images/DLScratch_08/img_11.png)
    



해당 논문에선 훈련시엔 forward / backward의 비중이 2.3 : 4.4 수준으로 backward의 연산량이 오래걸리고 추정시엔 90% 연산량이 각 layer 연산 계산에 쓰일정도로 빠른 연산량 처리가 딥러닝 훈련의 관건이됩니다.

<b>_3.2. GPU를 활용한 고속화_</b>

딥러닝 연산의 대부분은 대량의 행렬곱 연산입니다. 이는 CPU 보단 GPU에 잘 어울리는 연산 방식입니다. 사실 현재는 모든 최신 딥러닝 훈련이 GPU로만 이뤄진다고 볼 수 있을 정도인데요, Xeon gold 6140 CPU / NVIDIA T4 GPU 간 훈련시간을 비교해보겠습니다. <span style='color:#808080'>_(성능 차가 심한만큼 현재는 CPU와 GPU의 성능비교는 잘 이뤄지지 않습니다)_</span>


```python
im_ratio = 0.8
img_12 = Image.open('./dataset/DLfromScratch_08_03.png')
img_12 = img_12.resize((int(img_12.size[0]*im_ratio),int(img_12.size[1]*im_ratio)))
img_12
```




    
![png](/assets/images/DLScratch_08/img_12.png)
    



엄청난 성능차이가 있음을 확인할 수 있습니다. GPU 연산에는 상기 구현한 <b>_im2row 함수_</b> 와 같은 대용량 병렬처리에 특화된 <b>CUDA</b> 등의 개발환경도 존재하기 때문에 그 차이는 더욱 극명해지게 됩니다. 

<b>_3.3. 분산 학습_</b>

위의 GPU가 CPU보다 아무리 빠르다한들, 데이터 셋이 방대해지고 모델이 계층이 깊어지면 훈련시간은 한 달 가까이 소요되곤 합니다. 이런 부분을 극복하고자 이젠 multi-GPU를 통한 딥러닝 연산이 대세가 되었습니다. GPU 1대를 독립적으로 모델 훈련을 시켰을 때와 병렬로 GPU를 결합했을 때의 소요시간을 비교하면 다음과 같습니다. 


```python
im_ratio = 0.8
img_13 = Image.open('./dataset/images/fig 8-16.png')
img_13 = img_13.resize((int(img_13.size[0]*im_ratio),int(img_13.size[1]*im_ratio)))
img_13
```




    
![png](/assets/images/DLScratch_08/img_13.png)
    



GPU를 결합하는 개수가 증가할 수록 성능 증가폭이 줄어들긴 하지만, 작업의 효율을 56배나 끌어올릴 수 있다는 것은 분산학습의 의의를 보여줍니다. 해당 단계부턴 <b>_Tensorflow, Pytorch_</b> 같은 딥러닝 프레임워크 사용이 강제됩니다.

<b>_3.4. 연산 정밀도와 비트 줄이기_</b>

메모리 용량과 버스 대역폭 등도 딥러닝 고속화의 병목이 될 수 있습니다. 메모리 용량의 경우 batch number가 10인 경우와 100인 경우를 생각하면 이해하기 쉽습니다. 버스 대역폭의 경우 계산 오차를 희생하고 속도를 얻는다고 생각하면 됩니다. 다행인 것은 신경망이 Robust하기 때문에 계산 정밀도는 어느정도 오차를 허용합니다. 실수 표현 방식에는 64-bit / 32-bit 등이 있지만 딥러닝에선 16-bit 정밀도(half precision)를 사용합니다. 이는 사용자 입장에서 (스마트폰 탑재 시 빠른 연산이 필요) 중요하다고 볼 수 있습니다. 

### 4. Applictaion of Deep Learning

<b>_4.1. Object Detection_</b>

<b>_Object Detection_</b> 은 이미지 내 사물의 위치와 종류를 알아내는 기술입니다. 대표적인 모델로 R-CNN / YOLO 등이 있습니다. 여기선 R-CNN을 간단히 살펴보겠습니다.


```python
im_ratio = 0.8
img_14 = Image.open('./dataset/DLfromScratch_08_04.png')
img_14 = img_14.resize((int(img_14.size[0]*im_ratio),int(img_14.size[1]*im_ratio)))
img_14
```




    
![png](/assets/images/DLScratch_08/img_14.png)
    



특징되는 부분은 2번의 <b>_Region Proposal_</b> 입니다. 이미지의 영역을 표시할 <b>Bounding Box</b>를 일정 기준(해당 논문에선 <b>_Selective Search_</b> 란 CV 분야의 알고리즘 사용)을 통해 이미지 당 2K를 생성하고 각 bonding box별로 분류 네트워크를 진행하는 것입니다. 이보다 발전한 <b>_Fast R-CNN / Faster R-CNN_</b>등이 있으며 그 양상은 <b>_End to End_</b> 모델로 발전했다는 점입니다.

<b>_4.2. Segmentation_</b>

<b>_Segmentation_</b> 은 이미지를 bounding box로 표현하는 object detection에서 더 나아가 pixel 단위로 클래스를 masking 처리하는 것입니다. 다음과 같은 결과물을 출력하는 모델입니다.


```python
im_ratio = 0.8
img_15 = Image.open('./dataset/images/fig 8-19.png')
img_15 = img_15.resize((int(img_15.size[0]*im_ratio),int(img_15.size[1]*im_ratio)))
img_15
```




    
![png](/assets/images/DLScratch_08/img_15.png)
    



이를 구현하는 가장 직관적인 방법은 pixel 단위로 image classify를 수행하는 것입니다. 이는 pixel 수만큼 forward 연산이 필요한만큼 비효율적이고 지나치게 많은 시간이 필요하게 됩니다. 이를 해결하기 위한 기법으로 <b>_FCN(Fully Convolutional Network)_</b>이 고안되었습니다. 한 번의 forward로 모든 classifying을 수행하는 것으로 구조는 다음과 같습니다. 

> <span style="color:#808080">_Shelhamer, Evan et al. “Fully convolutional networks for semantic segmentation.” 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014): 3431-3440._</span>


```python
im_ratio = 0.5
img_15 = Image.open('./dataset/DLfromScratch_08_05.png')
img_15 = img_15.resize((int(img_15.size[0]*im_ratio),int(img_15.size[1]*im_ratio)))
img_15
```




    
![png](/assets/images/DLScratch_08/img_16.png)
    



일반적인 CNN 구조는 최종 layer를 전체 forward 계층 값이 flatten 된 형태인 Fully Connected Layer 형태로 만든 후 Affine 계층을 거쳐 차원을 축소합니다. 이후 Softmax Layer를 거쳐 해당 값을 추정치로 이용합니다. 반면 <b>fully convolutional network</b>는 input feature map과 동일한 shape에 그 값을 flatten 시킴으로서 공간 정보를 보존하고, 각 pixel별로 label을 masking 합니다. 해당 flatten 작업은 upsampling의 일종인 deconvolution 연산을 이용해 이중 선형 보간(bilinear interpolation)을 구현하여 이루어집니다. 자세한 내용은 논문을 참고하시길 바랍니다.

<b>_4.3. Image Caption_</b>

<b>_NLP(Natural Language Processing)_</b>와 <b>_RNN(Recurrent Neural Network)_</b> 그리고 CNN 모델을 함께 쓰는 <b>_Multimodal Processing_</b>을 통해 이미지를 설명하는 모델을 만들 수도 있습니다. 결과물은 다음과 같습니다. 

> <span style='color:#808080'>_Vinyals, Oriol et al. “Show and tell: A neural image caption generator.” 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014): 3156-3164._</b>


```python
im_ratio = 0.4
img_16 = Image.open('./dataset/DLfromScratch_08_07.png')
img_16 = img_16.resize((int(img_16.size[0]*im_ratio),int(img_16.size[1]*im_ratio)))
img_16
```




    
![png](/assets/images/DLScratch_08/img_17.png)
    



결과물이 놀랍습니다. 구조는 간략히 표현해서 다음과 같습니다. 


```python
im_ratio = 0.7
img_17 = Image.open('./dataset/DLfromScratch_08_08.png')
img_17 = img_17.resize((int(img_17.size[0]*im_ratio),int(img_17.size[1]*im_ratio)))
img_17
```




    
![png](/assets/images/DLScratch_08/img_18.png)
    



실제 구조는 훨씬 복잡하지만, CNN 구조에서 이미지의 특질을 추출하고 해당 특질을 weight init value 삼아 RNN 모델 상에서 language generation이 일어나게 됩니다. 상세한 내용은 논문을 참고하시길 바랍니다.

### 5. Future of Deep Learning

이번 장에선 '16년도 당시 최신 기법이었던 연구들의 종류와 그 결과물을 살펴보는 것으로 마무리하겠습니다. 각 모델들의 구조는 관련 논문을 찾아보시길 바랍니다.

<b>_5.1. Style Transfer_</b>

<b>_Style Transfer_</b> 는 현재 웹상에서도 구현 가능한 가벼운 모델이 구현되었지만, 특정 화풍을 컨텐츠 이미지에 적용하여 새로운 이미지를 생성하는 ('16년도엔 혁신적이었던) 연구를 말합니다. 결과물은 다음과 같습니다.

> <span style='color:#808080'>_Gatys, L.A., Ecker, A.S., & Bethge, M. (2015). A Neural Algorithm of Artistic Style. ArXiv, abs/1508.06576._</span>


```python
im_ratio = 0.5
img_18 = Image.open('./dataset/DLfromScratch_08_09.png')
img_18 = img_18.resize((int(img_18.size[0]*im_ratio),int(img_18.size[1]*im_ratio)))
img_18
```




    
![png](/assets/images/DLScratch_08/img_19.png)
    



<b>_5.2. Generative Adversarial Network_</b>

현재의 <b>_Diffusion_</b> 에 이르기까지 <b>_Generative Adversarial Network (GAN)_</b>는 등장부터 화려했던 알고리즘입니다. <b>Generator</b>와 <b>Discriminator</b> 두 개의 신경망을 이용해 정교한 생성자가 뛰어난 식별자를 이기도록 학습됩니다. 이에 탄생한 다양한 GAN 모델들은 여러 파생 GAN 모델의 탄생에 기여했습니다. 아래는 침실이 찍힌 대량의 이미지로 학습된 <b>DCGAN</b> 모델이 생성한 가상의 침실 이미지입니다.

> <span style='color:#808080'>_Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. CoRR, abs/1511.06434._</span>


```python
im_ratio = 0.5
img_19 = Image.open('./dataset/DLfromScratch_08_10.png')
img_19 = img_19.resize((int(img_19.size[0]*im_ratio),int(img_19.size[1]*im_ratio)))
img_19
```




    
![png](/assets/images/DLScratch_08/img_20.png)
    



<b>_5.3. Autonomous Driving_</b> : PASS

<b>_5.4. Deep Q-Network (Reinforcement Learning)_</b>

기존 label 여부에 따라 <b>_Suvervised / Unsupervised Learning_</b>으로 나누는 딥러닝의 종류 외에 <b>_Reinforcement Learning_</b> 이라는 연구분야가 있습니다. 

<b>Agent</b>가 <b>Environment</b>에 맞춰 행동을 선택하고, 이에 따라 environment가 유기적으로 변화합니다. 또한 그 변화수준에 따라 <b>Reward</b>를 얻게되고 이로써 agent의 행동 방향을 통제할 수 있게 됩니다. 이를 네트워크로 구성할 수 있는데요, 대표적으로 <b>_Deep Q-Network(DQN)_</b> 가 있습니다. DQN은 <b>Q-learning</b>이 딥러닝 형태로 발전한 것으로, neural network의 loss function 처럼 <b>action-value function</b>을 정의하고 <b>value(reward)</b>를 최대화하는 최적의 <b>action</b>을 결정하게 됩니다. 대표적인 예로 다음처럼 게임의 화면을 이미지화시켜 학습시키고, 이를 통해 최적의 루트로 게임을 최소한의 움직임으로 이기는 것입니다. 

> <span style='color:#808080'>_Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015)._</span>


```python
im_ratio = 0.5
img_20 = Image.open('./dataset/DLfromScratch_08_11.png')
img_20 = img_20.resize((int(img_20.size[0]*im_ratio),int(img_20.size[1]*im_ratio)))
img_20
```




    
![png](/assets/images/DLScratch_08/img_21.png)
    



Nature 논문에 실려 전세계에 강화학습을 알렸던 <b>_DeepMind_</b>의 해당 알고리즘은 추후 <b>_AlphaGO 부터 MuZero_</b>라는 대단한 업적을 남기게 되었습니다. 

>하나의 연구분야로 인정받는만큼 관련 내용은 방대합니다. 관심 있으신 분들은 <b>David Silvar</b>의 강의부터 시작하셔도 좋을 것 같습니다. 

### 6. Overview

* 수많은 문제에서 신경망을 더 깊게 하여 성능을 개선할 수 있다.
* 이미지 인식 기술 대회인 ILSVRC에서는 딥러닝 기반 기법이 상위권을 독점하고 있으며, 그 깊이도 더 깊어지는 추세다.
* 유명한 신경망으로는 VGG, GoogLeNet, ResNet이 있다.
* GPU와 분산 학습, 비트 정밀도 감소 등으로 딥러닝을 고속화할 수 있다.
* 딥러닝(신경망)은 사물 인식뿐 아니라 사물 검출과 분할에도 이용할 수 있다.
* 딥러닝의 응용 분야로는 사진의 캡션 생성, 이미지 생성, 강화학습 등이 있다. 최근에는 자율 주행에도 딥러닝을 접목하고 있어 기대된다.

> 약 한 달 걸렸던 <b>_밑바닥부터 시작하는 딥러닝 1권_</b> 에 대한 게시글을 마무리 짓습니다. 개인적으로 큰 도움이 되었습니다. 추후 동일 제목의 2권 게시글로 뵙겠습니다.

모든 저작권은 해당 제목의 책과 출판사에 있습니다.

---
