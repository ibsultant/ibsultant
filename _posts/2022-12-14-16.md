## 5. Error Backpropagation

#### _Contents_

---
1. 계산 그래프
    
    1.1. 계산 그래프로 풀다<br>
    1.2. 국소적 계산<br>
    1.3. 왜 계산 그래프로 푸는가?
    
2. 연쇄 법칙
    
    2.1. 계산 그래프의 역전파<br>
    2.2. 연쇄법칙이란?<br>
    2.3. 연쇄법칙과 계산 그래프

3. 역전파

    3.1. 덧셈 노드의 역전파<br>
    3.2. 곱셈 노드의 역전파<br>
    3.3. 사과 쇼핑의 예

4. 단순한 계층 구현하기

    4.1. 곱셈 계층<br>
    4.2. 덧셈 계층
    
5. 활성화 함수 계층 구현하기

    5.1. ReLU 계층<br>
    5.2. Sigmoid 계층

6. Affine / Softmax 계층 구현하기

    6.1. Affine 계층<br>
    6.2. 배치용 Affine 계층<br>
    6.3. Softmax-with-Loss 계층

7. 오차역전파법 구현하기

    7.1. 신경망 학습의 전체그림 <br>
    7.2. 오차역전파법을 적용한 신경망 구현하기<br>
    7.3. 오차역전파법으로 구한 기울기 검증하기<br>
    7.4. 오차역전파법을 사용한 학습 구현하기

8. 정리


---

이번 게시물은 5장의 Part 2 에 해당합니다. 이번 장에선 2-layer neural network를 error backpropagation을 이용하여 구현해보겠습니다.

### 7.오차역전파법 구현하기

<b>_7.1. 신경망 학습의 전체 그림_</b>

* Step_0 전제 : 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 합니다. 

* Step_1 - 미니배치 : 훈련 데이터 중 일부를 무작위로 가져옵니다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표입니다.

* Step_2 - 기울기 산출 : 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구합니다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시합니다. 여기서 error backpropagation이 적용되는 것이 수치미분이 적용됐던 4장과 가장 큰 차이점입니다. 

* Step_3 - 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신합니다. 

* Step_4 - 반복 : 1 ~ 3단계를 반복합니다.

<b>_7.2. 오차역전파법을 적용한 신경망 구현하기_</b>

여기에 등장하는 다양한 메소드 및 클래스를 정리해보았습니다.

|CLASS|Contents|
|:---|:---|
|OrderedDict|파이썬 collections 모듈에서 기본 제공하는 클래스로, 순서를 보장받는 dictionary 입니다|
|Relu|ReLU activation function 구현을 위한 클래스에 해당합니다|
|Affine|계산 그래프 상의 합, 곱 연산에 대한 forward/backward를 위한 클래스입니다 |
|SoftmaxWithLoss|softmax와 cross entropy error를 결합한 계산 그래프의 마지막 계층을 위한 클래스입니다|


|METHOD|Contents|
|:---|:---|
|softmax(x)|SoftmaxWithLoss 클래스에 필요한 메소드입니다|
|cross_entropy_error(y, t)|SoftmaxWithLoss 클래스에 필요한 메소드입니다|
|numerical_gradient(f, x)| 역전파로 구한 기울기를 검증하기 위한 수치미분 메소드입니다|

위의 것들을 종합하여 Error Backporpagation을 구현해보겠습니다. 최종 코드에 쓰인 순서대로 등장 코드를 적었습니다.


```python
from collections import OrderedDict
from matplotlib.image import imread
from mnist import load_mnist
from tqdm import tqdm
from PIL import Image


import sys, os
import pickle

import numpy as np
import matplotlib.pyplot as plt

class Relu:
    def __init__(self):
        self.mask = None            # mask는 T/F로 구성된 넘파이 배열로, 범위에 따른 T/F를 반환합니다.
    
    def forward(self, x):
        self.mask = (x <= 0)        # 0 이하면 True 반환, out value 0 반환
        out = x.copy()
        out[self.mask] = 0 

        return out   

    def backward(self, dout):       # dout 값에 곱해지는 값임을 잊지말 것
        dout[self.mask] = 0         # dout * 0
        dx = dout                   # dout * 1
        
        return dx

class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.original_x_shape = None
        self.dw = None
        self.db = None

    def forward(self, x):
        ###########################################################         
        # tensor 대응
        self.original_x_shape = x.shape # tensor 차원을 튜플형식으로 반환
        x = x.reshape(x.shape[0],-1)    # data input에 맞춰 array 변환
        ###########################################################
        self.x = x

        out = np.dot(self.x, self.W) + self.b
        
        return out

    def backward(self, dout):
        # Batch data shape을 맞추기위한 transpose T
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis = 0)

        dx = dx.reshape(*self.original_x_shape) 
        # 여기서 star(*)는 컨테이너 타입의 데이터를 unpacking하기 위한 목적이다. 텐서의 경우 가변인자가 되므로 사용한 것으로 추측.

        return dx

def softmax(x):
    # batch 데이터의 경우 데이터의 갯수와 flatten 된 입력값을 포함하면 (100, 784)와 같은 2차원이되므로 차원을 필수로 고려해야한다.
    # 784 내에서 최대값만 빼야하는데 np.max의 경우 스칼라 최댓값만 고려하므로 axis를 지정해야하고,
    # batch 개수만큼 최댓값을 빼줘야하는데 이를 위해선 transpose가 필요하다. 이를 위해 불필요해보이는 두 번의 transpose가 수행되는 것
    if x.ndim == 2:                 # 기존 X : (N, x) 가정
        x = x.T                     # new X : (x, N)
        x = x - np.max(x, axis=0)   # np.max(x, axis=0) : (1, N)
                                    # (x, N) - (1, N) //차 연산 수행가능, (x', N) 반환
        y = np.exp(x) / np.sum(np.exp(x), axis=0)   # 동일하게 적용, (x'', N) 반환
        return y.T                  # (N, x'') 반환

    x = x - np.max(x)   # exp(x + c')을 통해 resizing을 하여 overflow 문제 해결.
                        # 이때 c'값을 - max(x)를 취하여 지수함수의 발산문제 해결
    return np.exp(x) / np.sum(np.exp(x))

def cross_entropy(y, t):
    # 훈련데이터 : one-hot / 정답데이터 : only label 인 경우를 가정한 CEE 계산 코드
    if y.ndim == 1:                  #true label을 기준으로 차원 변환
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    
    if t.size == y.size:            # 훈련데이터 : one-hot / 정답데이터 : one-hot인 경우 필요한 변형, 정답인덱스만 추출.
        t = t.argmax(axis=1)

    delta = 1e-7                    # t = 1일 때 CEE 값인 log(1/y + delta)를 통해 overflow를 방지.
    batch_size = y.shape[0]
    cross_entropy_error = -np.sum(np.log(y[np.arange(batch_size), t])+ delta) / batch_size 
    # y[np.arange(batch_size), t] 정답 인덱스에 해당하는 y softmax 값을 추출하기 위한 코드. 역시 1과 유사할 수록 cross entropy error가 작아지는 구조다.
    
    return cross_entropy_error
    
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy(self.y, self.t)

        return self.loss
    
    def backward(self, dout = 1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        
        return dx

def numerical_gradient(f, x):
    h = 1e-4                  # 수치미분을 위한 작은 값 h
    grad = np.zeros_like(x)   # x와 shape이 같은 zero-matrix 변환
    
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    # 다차원 배열 x에 대한 객체로, 일반적 접근 방법인 중복 for문을 쓸 필요 없이 스칼라 값에 대한 접근을 가능케 한다.
    # multi_index : 요소 인덱스 값 반환
    # op_flags : operand(피연산자)에 대한 읽고 쓰는 권한 부여

    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + h
        f_1 = f(x)              # f(x+h)
        
        x[idx] = tmp_val - h 
        f_2 = f(x)              # f(x-h)
        grad[idx] = (f_1 - f_2) / (2 * h)
        
        x[idx] = tmp_val # 값 복원
        it.iternext()           # 다음 요소 접근
        
    return grad
```

아래 구현할 신경망의 그림은 다음과 같습니다. 코드와 함께 보시면 이해가 용이할 것입니다.


```python
img_1 = imread('./dataset/images/fig 3-17.png')
img_2 = imread('./dataset/images/fig 3-18.png')
img_3 = imread('./dataset/images/fig 3-19.png')
img_4 = imread('./dataset/images/fig 3-20.png')

plt.figure(figsize=(20, 15))
plt.subplot(2,2,1, title = 'STEP 1') ; plt.imshow(img_1)
plt.subplot(2,2,2, title = 'STEP 2') ; plt.imshow(img_2)
plt.subplot(2,2,3, title = 'STEP 3') ; plt.imshow(img_3)
plt.subplot(2,2,4, title = 'STEP 4') ; plt.imshow(img_4)
plt.show()
```


    
![png](output_4_0.png)
    



```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

        # 빈 계층에 layer 종류별로 클래스 인스턴스화
        self.layers = OrderedDict()
        self.layers['Affine_1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu'] = Relu()
        self.layers['Affine_2'] = Affine(self.params['W2'], self.params['b2'])
        self.lastLayer = SoftmaxWithLoss()  # 입력변수가 t도 포함되므로 따로 선언해야한다

    def predict(self, x):
        for layer in self.layers.values():  # 각 클래스 인스턴스에 맞는 forward 함수 실행
            x = layer.forward(x)             # 노드 순서별로 forward 진행
        
        return x                            # Affine_2 forward 결과값 반환

    def loss(self, x, t):
        y = self.predict(x)                 # Affine_2 forward 결과값 반환
        
        return self.lastLayer.forward(y, t) # softmax를 거친 cross entropy error 결과값 반환

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)            # one-hot index 출력
        if t.ndim != 1 : t = np.argmax(t, axis=1) # just label case도 one-hot 결과값에 맞춰주는 조건문

        accuracy = np.sum(y == t) / float(x.shape[0])
        
        return accuracy

    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)  # W에 CEE function을 저장

        # Cross Entropy Error function에 대한 편미분 값 저장
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads 

    def gradient(self, x, t):
        # Forward
        self.loss(x, t)

        # Backward
        dout = 1 # 최상류의 differential out
        dout = self.lastLayer.backward(dout)
        layers = list(self.layers.values()) # 반대로 묶어서
        layers.reverse()                    # predict 반복문을 backward로 진행하는 코드
        for layer in layers:
            dout = layer.backward(dout)

        grads = {}
        # 두 번째 곱 노드의 gradient
        grads['W2'] = self.layers['Affine_2'].dW
        grads['b2'] = self.layers['Affine_2'].db
        # 첫 번째 곱 노드의 gradient
        grads['W1'] = self.layers['Affine_1'].dW
        grads['b1'] = self.layers['Affine_1'].db
        
        return grads
```

위의 backpropagation (gradient method에 해당하는 부분)은 다음의 이미지를 참조하면 이해하기 편합니다.


```python
img_5 = Image.open('./dataset/images/fig a-5.png')
img_5
```




    
![png](output_7_0.png)
    



<b>_7.3. 오차역전파법으로 구한 기울기 검증하기_</b>

위의 코드에선 numerical_gradient 메소드를 통해 수치미분을 통해 loss function 최소화로 학습을 진행할 여지도 남겨두었는데요, 

정확히는 backpropagation 방법이 제대로 되었는지 확인하기 위한 <b>검증코드</b>로 보면 됩니다. 

이를 위한 코드를 아래 mnist.py 코드와 함께 구현하였습니다.


```python
img_6 = Image.open('./dataset/DLfromScratch_03_01.png')
img_6 = img_6.resize((int(img_6.width/2), int(img_6.height/2)))
img_6
```




    
![png](output_10_0.png)
    




```python
# mnist.py

url_base = 'http://yann.lecun.com/exdb/mnist/'
key_file = {
    'train_img' : 'train-images-idx3-ubyte.gz',
    'train_label' : 'train-labels-idx1-ubyte.gz',
    'test_img' : 't10k-labels-idx3-ubyte.gz',
    'test_label' : 't10k-labels-idx1-ubyte.gz'
}

dataset_dir = os.path.dirname(os.path.abspath(__file__)) #interactive shell에선 실행되지 않고 .py에서 실행된다.
save_file = dataset_dir + "/mnist.pkl" # 다양한 파이썬 자료형까지 저장을 지원하는 확장자

train_num = 60000
test_num = 10000
img_dim = (1, 28, 28)
img_size = 784

def _download(file_name):                       # private 함수
    file_path = dataset_dir +"/"+ file_name     # 입력받은 파일명을 기반으로 경로 설정/

    if os.path.exists(file_path):               # 경로 생성여부 체크
        return 
    
    print("Downloading " + file_name + "...")   
    urllib.request.urlretrieve(url_base + file_name, file_path) # 다운로드 수행
    print("Done")

def download_mnist():               #key_file에 있는 파일명순으로 다운로드 실행
    for v in key_file.values():
        _download(v)
                            #private func
def _load_label(file_name): #label file type 설명을 보면 unsigned byte로 0008 offset이라 표기되어 있음
    file_path = dataset_dir +'/'+ file_name

    print("Converting " + file_name + " to NumPy Array ...")
    with gzip.open(file_path, 'rb') as f:   # uint8 : unsigned int, 음이 아닌 0-255까지의 정수      
        labels = np.frombuffer(f.read(), np.uint8, offset=8) # gz 기반의 바이너리 파일이라 컨버팅이 필요.
    print("Done")

    return labels

def _load_img(file_name):
    file_path = dataset_dir +'/'+ file_name

    print("Converting " + file_name + " to NumPy Array ...")
    with gzip.open(file_path, 'rb') as f:   # img file description 설명의 pixel 시작부분 offset 참고.
        labels = np.frombuffer(f.read(), np.uint8, offset=16)  #gz 기반의 바이너리 파일이라 컨버팅이 필요.
    data = data.reshape(-1, img_size) # image 사이즈에 맞게 재구조화. 여기선 784개의 col을 갖도록 재구조.
    print("Done")

    return data

def _convert_numpy():
    # 각 압축파일을 dataset dictionary에 encoding하여 저장.
    dataset = {}
    dataset['train_img'] =  _load_img(key_file['train_img'])
    dataset['train_label'] = _load_label(key_file['train_label'])    
    dataset['test_img'] = _load_img(key_file['test_img'])
    dataset['test_label'] = _load_label(key_file['test_label'])

def init_mnist():
    download_mnist()            # 웹에서 MNIST 데이터 다운
    dataset = _convert_numpy()  # 다운받은 파일을 컨버팅하여 딕셔너리에 저장
    print("Creating pickle file ...")
    with open(save_file, 'wb') as f:
        pickle.dump(dataset, f, -1) # 여기서 -1은 protocol 인수로 최신 버전을 적용, 파일 저장에 문제 발생을 최소화한다.
    print("Done!")

def _change_one_hot_label(X):     
    T = np.zeros((X.size, 10))      # img 개수만큼 10개의 label 자리가 있는 배열 생성
    for idx, row in enumerate(T):
        row[X[idx]] = 1             # 실제 img index별 label 값만 1로 채워넣는 반복문 수행

    return T

def load_mnist(normalize=True, flatten=True, one_hot_label=False):
    """MNIST 데이터셋 읽기
    
    Parameters
    ----------
    normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.
    one_hot_label : 
        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.
        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.
    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다. 
    
    Returns
    -------
    (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)
    """

    if not os.path.exists(save_file):   # MNIST 인코딩 pickle 데이터 존재 여부 판단
        init_mnist()
        
    with open(save_file, 'rb') as f:    # 해당 데이터 파이썬로드
        dataset = pickle.load(f)
    
    if normalize:       # 데이터 변환 및 정규화
        for key in ('train_img', 'test_img'):
            dataset[key] = dataset[key].astype(np.float32)
            dataset[key] /= 255.0
            
    if one_hot_label:   # one-hot encoding 여부를 판단하고 수행. 이는 애매한 숫자 이미지의 경우 두 가지 레이블이 있을 수도 있다는 추론을 가능케한다.
        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])
        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])    
    
    if not flatten:     # flatten 하지 않을 경우, 1*28*28 img로 저장
         for key in ('train_img', 'test_img'):
            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)

    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) 

if __name__ == '__main__': # 모듈로 불러오면 실행되지 않는 구문.
    init_mnist()
```


```python
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

x_batch = x_train[:3]   # img 3개에 대해서만 backprop / numeric_grad 진행
t_batch = t_train[:3]

grad_numerical = network.numerical_gradient(x_batch, t_batch)
grad_backprop = network.gradient(x_batch, t_batch)

for key in grad_numerical.keys():
    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))
    print(key + " gap between numeric grad & backprop is " + " : " + str(diff))
```

    W1 gap between numeric grad & backprop is  : 2.6288674165640864e-13
    b1 gap between numeric grad & backprop is  : 8.545935389469434e-13
    W2 gap between numeric grad & backprop is  : 9.4011507693198e-13
    b2 gap between numeric grad & backprop is  : 1.1857181764218794e-10


세 이미지 내에서 진행된 각 gradient의 오차는 유의미하게 작습니다. 실제 학습을 진행하기전 해당 값이 0에 가까움을 확인함으로써 실수 없이 구현될 것임을 '어느정도' 예상할 수 있습니다.

이제 학습을 구현해보겠습니다.

<b>_7.4. 오차역전파법을 사용한 학습 구현하기_</b>


```python
(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)
network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)

# Hyperparameter
iters_num = 60000       # 100 epoch 진행
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

train_loss_list = []
# 정확도 추이
train_acc_list = []
test_acc_list = []
# 60000 / 1000 = 600, 즉 10 epoch에 해당.
iter_per_each = train_size / batch_size * 10


for i in tqdm(range(iters_num)):        # tqdm으로 반복문을 감싸면 반복횟수별 시간을 통해 loop 종료시간을 유추할 수 있습니다.
    # 미니배치 batch_size만큼 대한 샘플 무작위 추출
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    # 역전파를 통해 gradient 계산. 
    grad = network.gradient(x_batch, t_batch)
    
    # 매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    # 반복수에 따른 loss 추이를 위한 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    #매 600번마다 정확도 추이 계산 (1 epoch 해당))   
    if i % iter_per_each == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)

        print("train acc, test acc | "  + str(train_acc) + ", " + str(test_acc))
```

      0%|          | 73/60000 [00:00<07:30, 132.93it/s] 

    train acc, test acc | 0.10706666666666667, 0.1101


     10%|█         | 6080/60000 [00:09<02:24, 373.22it/s]

    train acc, test acc | 0.9691166666666666, 0.9629


     20%|██        | 12066/60000 [00:18<02:38, 302.28it/s]

    train acc, test acc | 0.9821333333333333, 0.971


     30%|███       | 18091/60000 [00:27<01:44, 402.82it/s]

    train acc, test acc | 0.9884, 0.9721


     40%|████      | 24140/60000 [00:36<01:34, 381.22it/s]

    train acc, test acc | 0.99185, 0.9739


     50%|█████     | 30071/60000 [00:44<01:32, 324.47it/s]

    train acc, test acc | 0.9942333333333333, 0.9744


     60%|██████    | 36094/60000 [00:53<01:03, 374.38it/s]

    train acc, test acc | 0.9966833333333334, 0.9731


     70%|███████   | 42133/60000 [01:02<00:47, 377.34it/s]

    train acc, test acc | 0.99815, 0.9734


     80%|████████  | 48126/60000 [01:12<00:35, 330.97it/s]

    train acc, test acc | 0.9983666666666666, 0.9751


     90%|█████████ | 54080/60000 [01:21<00:18, 328.70it/s]

    train acc, test acc | 0.9993833333333333, 0.9739


    100%|██████████| 60000/60000 [01:30<00:00, 661.43it/s]



```python
plt.xlabel('epochs')
plt.ylabel('accuracy')

plt.plot(train_acc_list, label='train_acc')
plt.plot(test_acc_list, '--', label = 'test_acc')
plt.legend()
```




    <matplotlib.legend.Legend at 0x112b81930>




    
![png](output_15_1.png)
    


위와 같이 epoch이 진행될 수록 훈련/시험 데이터 모두 정확도가 높아짐을 확인할 수 있습니다.

훈련/시험 데이터의 성능차 또한 적은것으로 미루어 보아  overfitting 없이 좋은 일반화 성능을 갖췄음을 확인할 수 있었습니다.

추가로 <b>_학습 및 검증에 걸린 소요시간이 대략 10000배정도 단축_</b> 되는 것을 확인할 수 있습니다. 

이로서 신경망의 각 계층 및 기울기 전파를 모듈화하여 구현하였으므로 자유롭게 원하는 신경망을 만들 수 있게 되었습니다.

### 8. 정리

* 계산그래프를 이용하면 계산 과정을 시각적으로 파악할 수 있다.
* 계산 그래프의 노드는 국소적 계산으로 구성된다. 국소적 계산을 조합해 전체 계산을 구성한다.
* 계산 그래프의 순전파는 통상의 계산을 수행한다. 한편, 계산 그래프의 역전파로는 각 노드의 미분을 구할 수 있다.
* 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있다. (오차역전파법)
* 수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 없는지 확인할 수 있다. (기울기 확인)

