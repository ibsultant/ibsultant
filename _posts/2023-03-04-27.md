---
layout: single
title:  "Essence of Linear Algebra_#05"
categories: 
 - Linear Algebra
classes: wide
use_math: true

---
  
## Essence of Linear Algebra_5

#### _Contents_

---
01. Vectors

02. Linear combinations, span, and basis vectors

03. Linear transformations and matrices

04. Matrix multiplication as composition

05. Three-dimensional linear transformations

06. The determinant

07. Inverse matrices, column space and null space

08. Nonsquare matrices as transformations between dimensions

09. Dot porducts and duality

10. Cross products

11. Cross products in the light of linear transformations

12. Cramer's rule, explained geometrically

13. Change of basis

14. Eigenvectors and eigenvalues

15. A quick trick for computing eigenvalues

16. Abstract vector spaces

---

<span style='color:#808080'>_모든 첨부 이미지/영상은 3Blue1Brown에 저작권이 있습니다_</span> 



### 14. Eigenvectors and eigenvalues


기존 장들을 시각화하는 연습을 충분히 했다는 것을 전제로, 충분히 직관적으로 이해할 수 있는 주제에 해당합니다. 예시를 들어보겠습니다. 대부분의 2차원 선형변환에서 vector들은 span이 line을 벗어납니다. 

<center><video src="https://user-images.githubusercontent.com/33462847/222880194-03542862-a273-4c68-991d-34aaceed4311.mp4" controls="controls" height="70%" width="70%"></video></center>

하지만 위와 같이 $$\begin{bmatrix}3&1 \\ 0&2\end{bmatrix}$$와 같은 케이스에서 $$\hat{i}, \begin{bmatrix}-1 \\ 1\end{bmatrix}$$ 상의 모든 vector는 span이 line 입니다. 특히 $\hat{i}$ 는 3배, $$\begin{bmatrix}-1 \\ 1\end{bmatrix}$$ 와 같이 span을 유지하는 특정 벡터는 2배 늘어납니다. 이와같이 <b>_Transformation 시 span을 유지하는 vector를 eigenvector 라고 부릅니다._</b> 이때 eigenvector는 모두 <b>_eigenvalue를 갖게되며 이는 transformation 후 eigenvector의 scaling factor를 말합니다._</b> (Eigenvalue 가 음수면 당연히 eigenvector가 flip 된다고 해석하면 됩니다)

3차원에서 이 의미를 생각해보면, eigenvector는 회전축을 의미하게 됩니다. 당연히 복잡한 행렬식보다 회전각과 eigenvector를 알고 있는게 훨씬 편합니다. 회전의 경우 only rotation이므로 eigenvalue = 1 입니다. 이처럼 특정 선형변환을 알기보단 eigenvalue / eigenvector 를 아는 것이 선형변환 중 일어나는 일에 대해 훨씬 직관적인 힌트를 줍니다. 이제 computation part를 살펴보겠습니다. 

$$
\boldsymbol A\overrightarrow v = \lambda \overrightarrow v
$$

각각 $\boldsymbol A$ 는 transformation matrix, $\overrightarrow v$ 는 eigenvector, $\lambda$ 는 eigenvalue에 해당됩니다. 해석하자면 matrix-vector multiplication $\boldsymbol A\overrightarrow v$ 가 eigenvector 에 $\lambda$ 만큼 scaling한 결과값과 같다는 의미입니다. 즉 행렬 A에 대해 위의 식을 참으로 만드는 eigenvalue $\lambda$, eigenvector $\overrightarrow v$를 구하는 것이 됩니다. 여기서 scaling factor $\lambda$ 를 행렬로 고쳐쓰면 identity matrix에 $\lambda$ 배. 즉, 

$$
\boldsymbol A\overrightarrow v = \lambda \overrightarrow v = \lambda I\overrightarrow v = \begin{bmatrix}\lambda & 0 & 0 \\ 0 & \lambda & 0 \\ 0 & 0 & \lambda \end{bmatrix}\overrightarrow v
$$

로도 표현할 수 있습니다. 기존식으로 돌아가 좌변을 넘겨 정리하면,

$$
(\boldsymbol A-\lambda I)\overrightarrow v=\overrightarrow 0
$$

이란 식을 얻고, $\overrightarrow v=\overrightarrow 0$ 인 결론을 얻어내는 것은 큰 의미가 없기 때문에 이를 제외한 matrix-vector muliplication 결과 값이 $\overrightarrow 0$ 이 되도록하는 $\overrightarrow v$를 찾으면 computational solution을 얻을 수 있습니다. 결국 공간을 squish 시키는 matrix를 찾아내는 것이 곧 solution을 얻는 방법이 되는데요, 이는 앞에서 배운 determinant를 활용하여 $det(\boldsymbol A-\lambda I)=0$ 이 되는 $\lambda$를 찾고 특정 vector 를 대입하여 squish 되는 다양한 eigenvector 들을 찾으면 온전히 solution을 풀게 됩니다. 하지만 rotation 과 같이 eigenvalue 가 없는 경우 $\lambda$ 는 허수를 갖게됩니다. 당연히 한 eigenvalue 에선 (span이 다른 eigenvector 들) or (동일 span을 갖는 다양한 vector들)을 찾을 수 있습니다. 

이제 basis로 구성되면 eigenvector와 eigenvalue가 직관적으로 정해지는 특이 케이스도 알아보겠습니다. 



![png](/assets/images/EofLA/img_06.png){:style="display:block; margin-left:auto; margin-right:auto"}
    



위와 같은 케이스는 $\hat i,\hat j$ 모두 eigenvector 이자 각 값이 곧 eigenvalue 가 됩니다. 이러한 $$\begin{bmatrix}a&0&0&0\\ 0&b&0&0\\ 0&0&c&0\\ 0&0&0&d\end{bmatrix}$$모양꼴의 <b>diagonal matrix</b>는 전부  <b>_각 basis 가 eigenvector 이고 그 값은 모두 eigenvalue 가 됩니다._</b> 또한 해당 vector는 해당 basis를 scaling만 하는 eigenvector 이므로 지수 행렬곱도 eigenvalue의 n 차 값만 구하면 되는 등 연산시 장점이 있습니다. 

위와 같은 vector는 특수한 케이스지만, transformation 시 eigenvector의 종류가 매우 많다면 eigenvector가 basis가 되도록 좌표계를 바꿔서 (13장의 change of basis) $A^{-1}MA$ 꼴의 행렬곱 형태로 바꿔 계산을 용이하게 할 수 있습니다. 예를 들어보겠습니다. 

$$
\begin{bmatrix}1&-1\\ 0&1\end{bmatrix}^{-I}\begin{bmatrix}3&-1\\ 0&2\end{bmatrix}\begin{bmatrix}1&-1\\ 0&1\end{bmatrix}
$$

위의 경우 eigenvector를 basis로 하는 좌표계를 이용해 구한 결과 값은 $$\begin{bmatrix}3&0\\ 0&2\end{bmatrix}$$ 로 diagonal matrix 형태로 결과 값이 나옵니다. 이제 이를 통해선 $$\begin{bmatrix}3&0\\ 0&2\end{bmatrix}^{100}$$ 과 같은 어려운 계산도 쉽게 할 수 있게됩니다. 이러한 계산을 다 한 이후에 역변환 과정만 거치면 됩니다. 이처럼 basis이기도 한 eigenvector 쌍을 <b>_eigenbasis_</b> 라고 부릅니다. 이를 통해 특정 행렬연산을 매우 쉽고 간단히 수행할 수 있게 되었습니다.

### 15. A quick trick for computing eigenvalues

이번 장에선 eigenvalue를 구하는데 필요한 세 가지 트릭들을 살펴보겠습니다.

(1) any trace of matrix, 즉 대각합과 eigenvalue의 연관성입니다. $$\Rightarrow \dfrac{1}{2}tr(\begin{bmatrix}a&b \\ c&d\end{bmatrix})=\dfrac{a+b}{2}=\dfrac{\lambda_1+\lambda_2}{2}=mean(a,b)=mean(\lambda_1+\lambda_2)$$

(2)$$det(\begin{bmatrix}a&b \\ c&d\end{bmatrix})=ad-bc=\lambda_1\lambda_2$$

$\because$  여기서 determinant는 선형변환 이후 공간의 면적 scaling을 나타내는데, 아래 영상처럼 $\lambda_1\lambda_2$ 가 변형된 공간에서 line으로 span 한다면 전체 면적이 $\lambda_1\lambda_2$ 만큼 증가함을 추론할 수 있습니다


<center><video src="https://user-images.githubusercontent.com/33462847/222880227-71223de8-52ce-4600-b800-b366153092de.mp4" controls="controls" height="70%" width="70%"></video></center>


(3) 예시를 통해 일반화 공식을 유도하겠습니다.

$$
let\quad \begin{bmatrix}8&4 \\ 2&6\end{bmatrix},\ mean(\lambda_1,\lambda_2)=7,\ det(\lambda_1, \lambda_2)=40
$$

이때 $\lambda_1\lambda_2$는 7을 기준으로 evenly spaced 이기 때문에 $\lambda_1\lambda_2$ 대신 $(7-d)(7+d)$ 로 표현할 수 있습니다. 이렇게되면 각 eigenvalue는,

$$
\begin{align}
\lambda_1\lambda_2\Rightarrow (7-d)(7+d)&=40\nonumber
\\
49-d^2&=40\nonumber
\\
d^2&=9\quad\nonumber
\\ (\because d>0)\ d&=3\nonumber 
\\ \nonumber
\\ \nonumber 
\lambda_1 =4,\ \lambda_2 &=10\nonumber
\end{align}
$$

로 계산을 할 수 있고 이를 일반화하여 determinant를 $p$, 2차 정사각행렬 대각합의 평균 $mean(a,b)$ 를 $m$이라 하면,

$$
\begin{align}
p&=(m+d)(m-d)=m^2-d^2\nonumber
\\
d^2&=m^2-p\nonumber
\\ \nonumber
\\ 
\because\lambda_1,\lambda_2&=m\pm\sqrt{m^2-p}\nonumber
\end{align}
$$

와 같이 유도할 수 있습니다. 이는 방정식에서 근의 공식을 유도한 과정과 같습니다. 단순히 determinant를 통해 eigenvalue를 구할 수도 있지만 결국 근의 공식으로 해를 구하는게 일반적으로 편해지듯 이도 유사하다고 볼 수 있습니다. 

### 16. Abstract vector spaces

<b>_Essence of Linear Algebra_</b> 의 마지막 챕터입니다. 이번 장에선 <b>_벡터란 무엇인가?_</b> 란 첫 번째 챕터의 질문으로 거슬러올라가며 논제를 시작하겠습니다. 일단 1/2/3차원은 몰라도 $$\begin{bmatrix} & \\ \\ \end{bmatrix}$$ 꼴이 n차원을 설명하기엔 편한 도구임은 부정하기 힘들 것입니다. 선형대수를 잘하는 사람들은 보통 주어진 좌표계와 상관없이 공간을 다룹니다. 다시말해 basis에 따라 좌표계를 바꿔가며 공간을 다룹니다. 이때 determinant는 변환 면적에 대한 scaling factor 이고 eigenvector 는 transformation 이후에도 span이 유지되는 vector를 의미하므로 coordinate system과는 관련이 없습니다. 질문을 구체화하여 만약 <b>_vector를 공간적인 본질 을 갖는 것_</b>으로 본다면?이란 가정을 해보고 공간에 대해 설명해보겠습니다.

<b>함수는 단지 다른 종류의 벡터에 해당합니다.</b> $f+g$를 살펴보면, 동일 좌표계 상에서 무수히 많은 좌표쌍을 더하는 것과 같습니다. 이는 $2\cdot f$의 경우도 동일한 범주로 취급할 수 있습니다. 벡터도 더하거나 실수배만 한다면 방금의 함수에다 기존 선형대수의 구조/문제 풀이 방법을 적용할 수 있습니다. 함수의 미분과 선형대수의 transformation 을 적용해보겠습니다. 

Linear transformation L 에 대해서 선형성은 
$$\begin{cases}
L(\overrightarrow v+\overrightarrow w)=L(\overrightarrow v)+L(\overrightarrow w)
\\
L(c\overrightarrow v)=cL(\overrightarrow v)
\end{cases}$$ 의 조건을 만족하면 정의할 수 있습니다. 이로 인해 

1. Linearly dependent한 basis의 linear combination 꼴로 벡터를 표현할 수 있고 
2. Matrix-vector multiplication을 가능케해서
3. Basis에 따라 선형변환을 완벽하게 표현할 수 있게 합니다. 

이를 다항함수의 한 미분 케이스에서 1번이 적용되는지 살펴보겠습니다.

$$
\begin{cases}
\dfrac{d}{dx}(x^3+x^2)=\dfrac{d}{dx}(x^3)+\dfrac{d}{dx}(x^2)
\\
\dfrac{d}{dx}(4x^3)=4\cdot \dfrac{d}{dx}(x^3)
\end{cases}
$$

이처럼 함수는 (선형성을 갖는다고 알려진)미분이라는 transformation을 함수의 선형결합형태로 표현할 수 있다는 것을 볼 수 있습니다. 이번엔 2번의 matrix-vector multiplication 여부를 판단하기 위해 all polynomial case를 고려해보겠습니다. 

표기법을 일정부분 약속해야하는데 $1\cdot x^2+3\cdot x+5\cdot 1$의 예시를 
$$
basis\ func
\begin{cases}
b_0(x)=1
\\
b_1(x)=x
\\
b_2(x)=x^2
\\
b_3(x)=x^3
\\
\vdots
\end{cases}\Rightarrow
\begin{bmatrix}
5\\ 3\\ 1\\ 0\\ \vdots
\end{bmatrix}
$$ 와 같이 표기하도록 약속하겠습니다. 

이제 함수를 basis 형태로 표현했으니 남은건 미분을 transformation 형태로 표현하는 것인데요, 이는 다항함수에선 직관적으로 간단히 표현할 수 있습니다. 

$$\dfrac{d}{dx}=\begin{bmatrix}0&1&0&0&\dots \\ 0&0&2&0&\dots \\ 0&0&0&3&\dots\\ \vdots&\vdots&\vdots&\vdots&\ddots \end{bmatrix}$$ 이와 같은 형태로 나태낼 수 있을 것입니다. 이를 $$\dfrac{d}{dx}(1\cdot x^3+5\cdot x^2+4\cdot x+5\cdot 1)$$의 경우에 대입을 시켜보면 $$\dfrac{d}{dx}=\begin{bmatrix}0&1&0&0&\dots \\ 0&0&2&0&\dots \\ 0&0&0&3&\dots\\ \vdots&\vdots&\vdots&\vdots&\ddots \end{bmatrix}\begin{bmatrix}5\\ 4\\ 5\\ 1\\ \vdots\end{bmatrix}$$ 형태로 쓸 수 있습니다. 이렇게 위의 2번 조건인 matrix-vector multiplication 형태로 표현할 수 있게되었습니다. 이것이 가능한 것은 앞서 언급했듯이 미분이 선형성을 만족하기 때문입니다. 

<b>_결국 matrix-vector multiplication과 미분_</b> $\dfrac{df}{dx}$ <b>_은 같은 종류에 해당합니다._</b>

이를 일반화하면,

<div align="center">

|Linear Algebra Concepts|Function Concepts|
|:---:|:---:|
|Linear Transformation|Linear Operation|
|Dot Product|Inner Product|
|Eigenvector|Eigenfunction|

</div>

이와 같이 대응시킬 수 있습니다. 다시 돌아가서 재질문을 해보겠습니다. Vector란 무엇인가?

수학엔 화살표 / 집합 / 행렬 / 함수 등 정의한게 무엇이던 합리적인 실수배와 덧셈을 하면 선형대수에서 쓰인 개념들을 적용할 수 있습니다. 이와 같은 vector-ish 한 것들을 한데모아 <b>_Vector Spaces_</b>라고 합니다. 그리고 모든 vector space를 고려하는게 아닌 해당 공간을 위해 실수배/덧셈에서 출발한 규칙을 정립한 것들을 <b>_Axiom(공리)_</b>라고 합니다. 현대 선형대수에선 어떤 방식으로 발견한 vector space 던 위의 이론을 사용할 땐 다음의 여덟 공리를 만족해야합니다. 



![png](/assets/images/EofLA/img_07.png){:style="display:block; margin-left:auto; margin-right:auto"}
    



이건 해당 이론을 새로운 벡터공간에 적용하기 위한 bridge로 볼 수 있습니다. 일종의 check-list 로 생각해도 좋습니다. 이렇게 axiom을 적용함으로써 다른 걸 고려하지 않고 linear algebra 이론을 적용할 수 있습니다. 첫 장에서 답이 될 수 있을지 모르겠지만 이렇듯 공리 관점에서만 보며 추상적으로 표현하는 것입니다. 구태여 arrow나 matrix로 표현할 필요는 없습니다. 그래서 linear 하다라는 표현을 grid가 '평행하고 균일한 간격이다'라고 한정지어 표현하는 대신 위의 공리로 설명하는 이유이기도 합니다. 

<b>_벡터는 무엇인가?에 대한 질문의 결론을 내자면 마치 '3은 무엇인가?'와 같이 무의미한 질문과 같습니다. 현대 수학은 vector의 형태를 신경쓰지 않습니다. 공리만 만족하면 무엇이던 vector가 될 수 있습니다._</b> 이처럼 형태로 표현하는 것이 아닌 vector space 속에 추상화시킵니다. 이 강의도 결국 직관을 위해 화살표 / 움직이는 좌표평면 등으로 보조설명을 하였으나 선형대수는 더 광범위하게 적용할 수 있는 학문이므로 추상적으로 표현하는 결론을 내며 마무리 짓고 있습니다.

> <b>_End of Lecture_</b>
